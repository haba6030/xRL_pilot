# Planning-Aware IRL/AIRL

**Modeling planning mechanisms as explicit factors in Inverse Reinforcement Learning for expertise and clinical trait prediction**

[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

---

## ğŸ¯ Research Goals

This project investigates whether **planning depth** can be explicitly modeled as an inferable parameter in IRL/AIRL, improving:

1. **Expertise discrimination**: Can planning depth distinguish experts from novices?
2. **Reward identifiability**: Does modeling planning improve IRL interpretability?
3. **Clinical prediction**: Can planning mechanisms explain clinical traits (e.g., anxiety)?
4. **Neural correlates**: Do planning parameters map to fMRI activity patterns?

Building on:
- **van Opheusden et al. (2023)**: Expertise increases planning depth in 4-in-a-row
https://www.nature.com/articles/s41586-023-06124-2 
- **Yao et al. (2024)**: Planning horizon as latent confounder in IRL
https://arxiv.org/abs/2409.18051 
- **Mhammedi et al. (2023)**: Multi-step inverse RL perspective
https://arxiv.org/abs/2304.05889 

---

## ğŸ“Š Dataset

**Source**: van Opheusden et al. (2023) 4-in-a-row behavioral dataset
- **Participants**: 40 humans
- **Trials**: 67,331 game moves
- **Conditions**: learning, time pressure, eye tracking, fMRI, generalization
- **Models**: 22 cognitive model variants (ablations, alternatives)

**Data files**: `opendata/` (CSV format)

---

## ğŸ”¬ Analysis Pipeline

### Phase 1: Behavioral Modeling âœ… (In Progress)

**Status**: Data exploration and baseline analysis complete

1. **Data reanalysis** (`data_reanalysis.py`)
   - Parameter distributions
   - Expertise classification (composite score)
   - Visualization suite

2. **Model comparison** (`model_comparison_analysis.py`)
   - Compare 22 model variants
   - Log-likelihood ranking
   - Participant-level preferences

3. **Immediate analysis** (`immediate_analysis.py`)
   - Planning depth vs expertise
   - Discrimination tests (AUC)
   - Response time correlations

**Key Finding**: Expert players show *shallower* planning depth than novices (p=0.01), suggesting efficient rather than deep planning.

### Phase 2: Planning-Aware AIRL ğŸš§ (71% Complete)

**Goal**: Implement Planning-Aware AIRL with discrete planning depth h âˆˆ {1,2,4,8}

**Main Approach**: **Option A (Pure NN)** â­ - Random ì´ˆê¸°í™”, ìˆœìˆ˜ AIRL í•™ìŠµ
**Baseline**: Option B (BC) - BFS â†’ BC â†’ AIRL (Steps A-E ì™„ë£Œ)

**Status**: Baseline ì™„ë£Œ (71%), Main experiments ì§„í–‰ ì˜ˆì •

#### âœ… Completed Steps (Option B - Baseline)

| Step | Description | File | Status |
|------|-------------|------|--------|
| A | h-specific í•™ìŠµ ë°ì´í„° ìƒì„± | `generate_training_data.py` | âœ… |
| B | Behavior Cloning (BC) | `train_bc.py` | âœ… |
| C | BCë¥¼ PPOë¡œ ë˜í•‘ | `create_ppo_generator.py` | âœ… |
| D | Depth-AGNOSTIC ë³´ìƒ ë„¤íŠ¸ì›Œí¬ | `create_reward_net.py` | âœ… |
| E | AIRL í•™ìŠµ | `train_airl.py` | âœ… |
| F | Multi-Depth ë¹„êµ | (next) | ğŸ”„ |
| G | í‰ê°€ ë° ë¶„ì„ | (planned) | ğŸ“‹ |

#### ğŸ”„ Next: Option A Main Experiments

- [ ] Option A í•™ìŠµ (h=1,2,4,8) - 50K-100K steps each
- [ ] Performance evaluation
- [ ] Option A vs B comparison

**í•µì‹¬ ì›ì¹™**: Planning depth hëŠ” **Policyì—ë§Œ** ì¡´ì¬, **Reward Network**ì—ëŠ” ì—†ìŒ

```python
# âœ… CORRECT
policy = DepthLimitedPolicy(h=h)              # h HERE
reward_net = create_reward_network(env)       # NO h!
observations.shape == (T+1, 89)               # NO h!
```

**Quick Start**:
```bash
cd fourinarow_airl
conda activate pedestrian_analysis
export KMP_DUPLICATE_LIB_OK=TRUE

# Run full pipeline
python3 generate_training_data.py --num_episodes 100
python3 train_bc.py --n_epochs 50
python3 create_ppo_generator.py
python3 train_airl.py --total_timesteps 50000
```

**ë¬¸ì„œ**: [PHASE2_PROGRESS.md](progress/PHASE2_PROGRESS.md), [AIRL_DESIGN.md](docs/AIRL_DESIGN.md)

### Phase 3: Clinical & Neural ğŸ”® (Planned)

**Goal**: Apply Planning-Aware AIRL to clinical traits and neural correlates

1. **Clinical modeling**
   - Clinical traits â†’ planning parameters
   - Explainable individual differences

2. **Neural correlates**
   - fMRI trial-wise regressors
   - Planning parameter mapping

---

## ğŸš€ Quick Start

### Installation after setting conda env

```bash
# Clone repository
git clone https://github.com/haba6030/xRL_pilot.git
cd xRL_pilot

# Install Python dependencies
pip install pandas numpy matplotlib seaborn scipy scikit-learn

# Optional: Jupyter for notebooks
pip install jupyter
```

### Run Analyses

```bash
# Data reanalysis
python data_reanalysis.py

# Model comparison
python model_comparison_analysis.py

# Immediate analysis (requires depth_by_session.txt)
python immediate_analysis.py

# View results
open analysis_*.png
```

### Explore Data

```python
import pandas as pd

# Load raw behavioral data
raw = pd.read_csv('opendata/raw_data.csv')
print(f"Trials: {len(raw)}, Participants: {raw['participant'].nunique()}")

# Load model fits
main_model = pd.read_csv('opendata/model_fits_main_model.csv')
print(main_model[['pruning threshold', 'lapse rate', 'log-likelihood']].describe())
```

---

## ğŸ“ Repository Structure

```
xRL_pilot/
â”œâ”€â”€ fourinarow_airl/          # Phase 2 implementation (Planning-Aware AIRL)
â”‚   â”œâ”€â”€ generate_training_data.py  # Step A
â”‚   â”œâ”€â”€ train_bc.py                # Step B
â”‚   â”œâ”€â”€ create_ppo_generator.py    # Step C
â”‚   â”œâ”€â”€ create_reward_net.py       # Step D
â”‚   â”œâ”€â”€ train_airl.py              # Step E
â”‚   â”œâ”€â”€ airl_utils.py              # Utilities
â”‚   â””â”€â”€ fourinarow_env.py          # Environment
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ training_trajectories/     # Step A outputs
â”‚   â””â”€â”€ expert_trajectories/       # Expert data
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ bc_policies/               # Step B outputs
â”‚   â”œâ”€â”€ ppo_generators/            # Step C outputs
â”‚   â””â”€â”€ airl_results/              # Step E outputs
â”œâ”€â”€ opendata/                  # Phase 1 experimental data (CSV)
â”‚   â”œâ”€â”€ raw_data.csv          # 67K trials
â”‚   â””â”€â”€ model_fits_*.csv      # 22 model variants
â”œâ”€â”€ papers/                    # Reference papers (PDF)
â”œâ”€â”€ xRL_pilot/                # van Opheusden (2023) codebase
â”‚   â”œâ”€â”€ Model code/           # C++ implementation
â”‚   â”‚   â”œâ”€â”€ bfs.cpp           # Best-first search + PV depth
â”‚   â”‚   â”œâ”€â”€ heuristic.cpp     # 17 feature weights
â”‚   â”‚   â””â”€â”€ matlab wrapper/   # Parameter fitting (BADS)
â”‚   â””â”€â”€ Analysis notebooks/   # Jupyter notebooks
â”œâ”€â”€ *.py                      # Phase 1 analysis scripts
â”œâ”€â”€ AIRL_DESIGN.md            # Phase 2 design document
â”œâ”€â”€ PHASE2_PROGRESS.md        # Phase 2 progress tracking
â”œâ”€â”€ IMPLEMENTATION_NOTES.md   # Technical implementation details
â”œâ”€â”€ CLAUDE.md                 # Full research plan
â””â”€â”€ README.md                 # This file
```

**Phase 2 ë¬¸ì„œ**:
- [AIRL_DESIGN.md](docs/AIRL_DESIGN.md) - Planning-Aware AIRL ì„¤ê³„
- [AIRL_COMPLETE_GUIDE.md](docs/AIRL_COMPLETE_GUIDE.md) - ì „ì²´ ì‹¤í–‰ ê°€ì´ë“œ â­
- [PHASE2_PROGRESS.md](progress/PHASE2_PROGRESS.md) - í˜„ì¬ ì§„í–‰ ìƒí™© (71% complete)
- [IMPLEMENTATION_NOTES.md](docs/IMPLEMENTATION_NOTES.md) - êµ¬í˜„ ê¸°ìˆ  ì°¸ê³ ì‚¬í•­

**Phase 1 ë¬¸ì„œ** (archived):
- [PROJECT_SUMMARY.md](archive/PROJECT_SUMMARY.md) - Phase 1 detailed documentation
- [FOLDER_STRUCTURE.md](archive/FOLDER_STRUCTURE.md) - Complete directory guide

---

## ğŸ“ˆ Current Results

### Expertise Classification

**Baseline (parameters only)**:
- AUC: **0.982**
- Accuracy: 96.7%
- Top features: log-likelihood (+1.76), pruning threshold (+1.46)

**With planning depth**:
- AUC: 0.987 (marginal improvement)
- **Finding**: Depth coefficient is *negative* (-0.59)
  - Deeper planning â†’ Novice direction
  - Supports "efficient planning" hypothesis

### Planning Depth Pattern

```
Expert:  6.23 Â± 1.30 steps
Novice:  7.29 Â± 0.55 steps
p = 0.011

Correlation with performance: r = -0.50 (p < 0.01)
â†’ Deeper planning associated with *worse* performance
```

**Interpretation**: Expertise reflects efficient pruning, not brute-force depth.

### Model Comparison

**Log-likelihood ranking** (higher = better):
1. MCTS: 2.00
2. No pruning: 2.00
3. Main model: 1.95
4. Fixed depth: 1.94

---

## ğŸ¤ Contributing

This is a research project. For collaboration inquiries:
- See [PROJECT_OVERVIEW.md](PROJECT_OVERVIEW.md) for research overview
- New contributors: Read documentation in order (README â†’ PROJECT_OVERVIEW â†’ AIRL_DESIGN)
- Questions: Open an issue on GitHub

---

## ğŸ“š References

1. **van Opheusden, B., et al. (2023)**. Expertise increases planning depth in human gameplay. *Nature*.
2. **Yao, W., et al. (2024)**. Planning horizon as a latent confounder in inverse reinforcement learning.
3. **Mhammedi, Z., et al. (2023)**. Reinforcement learning for multi-step inverse kinematics.

---

## ğŸ“„ License

MIT License (see LICENSE file)

---

## ğŸ”— Links

- **Original codebase**: [van Opheusden et al. (2023)](https://github.com/original-repo)
- **Project overview**: See [PROJECT_OVERVIEW.md](PROJECT_OVERVIEW.md)
- **Design document**: See [docs/AIRL_DESIGN.md](docs/AIRL_DESIGN.md)

---

**Last Updated**: 2025-12-26
**Current Phase**: Phase 2 - Planning-Aware AIRL (71% complete)
