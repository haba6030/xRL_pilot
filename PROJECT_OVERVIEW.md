# Planning-Aware AIRL: ì—°êµ¬ ê°œìš”

**Planning Depthë¥¼ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ì—¬ ì¸ê°„ ì˜ì‚¬ê²°ì •ì„ ì´í•´í•˜ê³  ì„¤ëª…í•˜ëŠ” ì—°êµ¬**

**Last Updated**: 2025-12-26
**Current Phase**: Phase 2 (71% complete)

---

## ğŸ“Œ í•µì‹¬ ì•„ì´ë””ì–´

### ì „í†µì ì¸ IRLì˜ ë¬¸ì œì 

**Inverse Reinforcement Learning (IRL)**ì€ í–‰ë™ ë°ì´í„°ë¡œë¶€í„° ë³´ìƒ í•¨ìˆ˜ë¥¼ ì¶”ë¡ í•©ë‹ˆë‹¤:

```
ê´€ì°°: ì‚¬ëŒë“¤ì˜ í–‰ë™ ë°ì´í„° (observations, actions)
ì§ˆë¬¸: "ì´ ì‚¬ëŒë“¤ì€ ì–´ë–¤ ë³´ìƒì„ ìµœì í™”í•˜ê³  ìˆëŠ”ê°€?"
ê²°ê³¼: ì¶”ë¡ ëœ ë³´ìƒ í•¨ìˆ˜ r(s, a)
```

**í•˜ì§€ë§Œ**, ì „í†µì  IRLì€ ì¤‘ìš”í•œ ê°€ì •ì„ í•©ë‹ˆë‹¤:
- âŒ ëª¨ë“  ì‚¬ëŒì´ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ê³„íší•œë‹¤
- âŒ Planning depthëŠ” ê³ ì •ë˜ì–´ ìˆê±°ë‚˜ ë¬´í•œí•˜ë‹¤
- âŒ í–‰ë™ ì°¨ì´ëŠ” ì˜¤ì§ ë³´ìƒ ì°¨ì´ì—ì„œë§Œ ì˜¨ë‹¤

### Planning-Aware AIRLì˜ í•´ê²°ì±…

ìš°ë¦¬ëŠ” **Planning Depth (h)**ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•©ë‹ˆë‹¤:

```
ê´€ì°°: ì‚¬ëŒë“¤ì˜ í–‰ë™ ë°ì´í„° (observations, actions)
ì§ˆë¬¸: "ì´ ì‚¬ëŒë“¤ì€ ì–¼ë§ˆë‚˜ ê¹Šì´ ìƒê°í•˜ë©°, ì–´ë–¤ ë³´ìƒì„ ìµœì í™”í•˜ëŠ”ê°€?"
ê²°ê³¼: Planning depth h + ë³´ìƒ í•¨ìˆ˜ r(s, a)
```

**í•µì‹¬ í†µì°°** (Yao et al., 2024):
> Planning horizonì€ IRLì—ì„œ latent confounder ì—­í• ì„ í•©ë‹ˆë‹¤.
> ì´ë¥¼ ë¬´ì‹œí•˜ë©´ **reward identifiability**ê°€ ê¹¨ì§‘ë‹ˆë‹¤.

---

## ğŸ¯ ì—°êµ¬ ëª©í‘œ

### Objective 1: Planning Depthë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì •ì˜

**Planning Depth (h)**: ì–¼ë§ˆë‚˜ ë§ì€ ë¯¸ë˜ ë‹¨ê³„ë¥¼ ë¯¸ë¦¬ ìƒê°í•˜ëŠ”ê°€?

```python
h = 1  # 1 step ahead  - ì¦‰ê°ì  ë°˜ì‘
h = 2  # 2 steps ahead - ê¸°ë³¸ ê³„íš
h = 4  # 4 steps ahead - ì¤‘ê¸‰ ê³„íš
h = 8  # 8 steps ahead - ì „ë¬¸ê°€ ìˆ˜ì¤€
```

**ë‹¤ë¥¸ íŒŒë¼ë¯¸í„°**:
- **Î² (inverse temperature)**: ì–¼ë§ˆë‚˜ ê²°ì •ì ìœ¼ë¡œ ì„ íƒí•˜ëŠ”ê°€?
- **lapse rate**: ë¬´ì‘ìœ„ë¡œ ì„ íƒí•  í™•ë¥ ì€ ì–¼ë§ˆë‚˜ ë˜ëŠ”ê°€?

### Objective 2: ì´ˆë³´ì vs ì „ë¬¸ê°€ êµ¬ë¶„

**ì—°êµ¬ ì§ˆë¬¸**: Planning depth hë¡œ ì „ë¬¸ê°€ë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ”ê°€?

**ê°€ì„¤**:
- **H1 (Brute-force)**: ì „ë¬¸ê°€ = ë” ê¹Šì€ planning (h â†‘)
- **H2 (Efficiency)**: ì „ë¬¸ê°€ = íš¨ìœ¨ì  pruning (h ë¹„ìŠ·í•˜ì§€ë§Œ ë” ë‚˜ì€ heuristics)

**Phase 1 ë°œê²¬** (van Opheusden et al., 2023 ë°ì´í„° ì¬ë¶„ì„):
- âœ… ì „ë¬¸ê°€ê°€ ì˜¤íˆë ¤ **ë” ì–•ì€** planning depthë¥¼ ë³´ì„ (p=0.01)
- âœ… Depthì™€ ì„±ëŠ¥ ê°„ **ë¶€ì  ìƒê´€ê´€ê³„** (r=-0.50)
- âœ… **Efficiency hypothesis ì§€ì§€**: ì „ë¬¸ê°€ = íš¨ìœ¨ì  pruning

### Objective 3: IRL ì„¤ëª…ë ¥ í–¥ìƒ

**í‘œì¤€ AIRL**:
```python
# ëª¨ë“  í–‰ë™ ì°¨ì´ë¥¼ rewardë¡œ ì„¤ëª…
reward_net = f(observation, action)  # ëª¨ë“  ê²ƒì„ rewardë¡œ!
```

**Planning-Aware AIRL**:
```python
# í–‰ë™ ì°¨ì´ë¥¼ planning + rewardë¡œ ë¶„í•´
policy = DepthLimitedPolicy(h=h)              # Planning mechanism
reward_net = f(observation, action)           # Reward (NO h!)
```

**ê¸°ëŒ€ íš¨ê³¼**:
- âœ… ë” ë‚˜ì€ reward identifiability
- âœ… ë” í•´ì„ ê°€ëŠ¥í•œ ê°œì¸ì°¨ ì„¤ëª…
- âœ… Out-of-distribution (OOD) generalization í–¥ìƒ

### Objective 4: ì„ìƒ íŠ¹ì„± ì„¤ëª…

**ì „í†µì  ì ‘ê·¼**:
```
ë¶ˆì•ˆ ì¥ì•  â†’ ë‹¤ë¥¸ ë³´ìƒ í•¨ìˆ˜ (ì˜ˆ: ìœ„í—˜ íšŒí”¼ â†‘)
```

**Planning-Aware ì ‘ê·¼**:
```
ë¶ˆì•ˆ ì¥ì•  â†’ Planning mechanism ì°¨ì´
  - ë” ì§§ì€ planning depth? (myopic)
  - ë” ë†’ì€ lapse rate? (distraction)
  - ë‹¤ë¥¸ feature weighting? (threat bias)
  + ë³´ìƒ í•¨ìˆ˜ ì°¨ì´
```

**ì´ì **: ë©”ì»¤ë‹ˆì¦˜ ê¸°ë°˜ ì„¤ëª… â†’ ë” ë‚˜ì€ intervention ê°€ëŠ¥ì„±

### Objective 5: ì‹ ê²½ ë©”ì»¤ë‹ˆì¦˜ ì—°ê²° (íƒìƒ‰ì )

**ë‘ ê°€ì§€ ì ‘ê·¼**:

1. **Model-based fMRI**:
```python
# Trial-wise regressors
value_t = Q(s_t, a_t)              # Value signal
uncertainty_t = H(Ï€(Â·|s_t))        # Uncertainty
conflict_t = max(Q) - second_max(Q) # Conflict
planning_proxy_t = f(h, depth_t)   # Planning proxy
```

2. **Individual differences**:
```python
# Subject-level parameters â†’ brain activity
h_subject â†’ dmPFC activity?
Î²_subject â†’ striatum activity?
lapse_subject â†’ attention network?
```

---

## ğŸ§ª ë°©ë²•ë¡ : 4-in-a-Row Game

### ì™œ 4-in-a-Rowì¸ê°€?

**ì¥ì **:
- âœ… ì¶©ë¶„íˆ ë³µì¡ (planning í•„ìš”)
- âœ… ê³„ì‚° ê°€ëŠ¥ (h=1~8 ì‹¤í˜„ ê°€ëŠ¥)
- âœ… ì˜ ì •ì˜ëœ heuristics (van Opheusden et al., 2023)
- âœ… í’ë¶€í•œ ë°ì´í„° (67,331 trials, 40 participants)

**ê²Œì„ ì„¤ëª…**:
- 6Ã—6 ë³´ë“œ
- 2ëª…ì˜ í”Œë ˆì´ì–´ (Black/White)
- ëª©í‘œ: 4ê°œë¥¼ ì—°ì†ìœ¼ë¡œ ë†“ê¸°
- í–‰ë™ ê³µê°„: 36 positions (0-35)

### Model Components

#### 1. Board State

```python
# 89-dimensional observation
board_state = {
    'board': 72,      # 6Ã—6Ã—2 (black/white bitboards)
    'features': 17,   # heuristic features
}
# âš ï¸ NO h information in observations!
```

#### 2. Heuristic Evaluation

**17 features** (van Opheusden et al., 2023):
- Center control
- Connected/unconnected 2-in-a-row
- 3-in-a-row
- 4-in-a-row (win)
- Orientation variants (horizontal, vertical, diagonal)

```python
def heuristic_value(state, weights):
    features = extract_features(state)  # 17-dim
    return dot(weights, features)
```

#### 3. Depth-Limited Search

**Best-First Search (BFS)** with fixed depth h:

```python
def depth_limited_search(state, h, weights):
    """
    Search up to depth h
    Returns Q-values for all legal actions
    """
    frontier = PriorityQueue()
    Q = {}

    for action in legal_actions(state):
        next_state = transition(state, action)
        value = heuristic_value(next_state, weights)
        frontier.push(next_state, depth=1, root_action=action)
        Q[action] = value

    while frontier and depth < h:
        node = frontier.pop()
        # Expand and update Q-values
        # ...

    return Q
```

#### 4. Policy

**Softmax policy** with temperature Î² and lapse rate:

```python
def policy(state, h, beta, lapse, weights):
    Q = depth_limited_search(state, h, weights)

    # Softmax
    pi_soft = softmax(beta * Q)

    # Lapse (random choice)
    pi_uniform = uniform(len(Q))
    pi = (1 - lapse) * pi_soft + lapse * pi_uniform

    return pi
```

---

## ğŸ—ï¸ Planning-Aware AIRL êµ¬ì¡°

### í•µì‹¬ ì›ì¹™ â­

```python
# âœ… CRITICAL PRINCIPLE
# Planning depth hëŠ” POLICYì—ë§Œ ì¡´ì¬
# Reward networkëŠ” DEPTH-AGNOSTIC

# âœ… CORRECT
policy = DepthLimitedPolicy(h=h)              # h HERE!
reward_net = create_reward_network(env)       # NO h!
observations.shape == (T+1, 89)               # NO h!
```

**ì™œ?**

1. **Theoretical**: RewardëŠ” í™˜ê²½ì˜ ì†ì„±, Planningì€ agentì˜ ì†ì„±
2. **Identifiability**: hì™€ rewardë¥¼ ë¶„ë¦¬í•´ì•¼ ê°ê° ì¶”ë¡  ê°€ëŠ¥
3. **Generalization**: ê°™ì€ rewardë¡œ ë‹¤ë¥¸ h policy ìƒì„± ê°€ëŠ¥

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   AIRL Training                          â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Generator       â”‚         â”‚  Discriminator     â”‚   â”‚
â”‚  â”‚  (h-specific)    â”‚         â”‚  (h-AGNOSTIC)      â”‚   â”‚
â”‚  â”‚                  â”‚         â”‚                    â”‚   â”‚
â”‚  â”‚  Policy Ï€_h      â”‚â”€â”€â”€â”€â”€â”€â”€â”€>â”‚  Reward Net r_Ï†    â”‚   â”‚
â”‚  â”‚  (PPO)           â”‚         â”‚  (NO h param!)     â”‚   â”‚
â”‚  â”‚                  â”‚         â”‚                    â”‚   â”‚
â”‚  â”‚  Input: s (89)   â”‚         â”‚  Input: (s,a,s')   â”‚   â”‚
â”‚  â”‚  Output: a       â”‚         â”‚  Output: reward    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚          â†‘                              â†‘               â”‚
â”‚          â”‚                              â”‚               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ h-specific       â”‚         â”‚ Expert trajectories â”‚  â”‚
â”‚  â”‚ training data    â”‚         â”‚ (NO h labels!)      â”‚  â”‚
â”‚  â”‚ (h=1,2,4,8)      â”‚         â”‚ (s, a, s', done)    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Two Approaches: Option A vs Option B

**ì´ í”„ë¡œì íŠ¸ëŠ” Option A (Pure NN)ë¥¼ main approachë¡œ ì§„í–‰í•©ë‹ˆë‹¤** â­

| | Option A (Main) â­ | Option B (Baseline) |
|---|---|---|
| **Generator ì´ˆê¸°í™”** | Random weights | BC from BFS |
| **í•™ìŠµ ë°©ì‹** | ìˆœìˆ˜ AIRL | BC â†’ AIRL fine-tune |
| **Timesteps** | 50K-100K | 10K |
| **ì¥ì ** | ìˆœìˆ˜í•œ planning depth íš¨ê³¼ ì¸¡ì • | ë¹ ë¥´ê³  ì•ˆì •ì  |
| **í˜„ì¬ ìƒíƒœ** | ì½”ë“œ ì¤€ë¹„ ì™„ë£Œ â†’ ì‹¤í—˜ ì˜ˆì • | Steps A-E ì™„ë£Œ (71%) |
| **ìš©ë„** | Main experiments | Baseline/ë¹„êµêµ° |

**ìƒì„¸ ì„¤ëª…**: [AIRL_COMPLETE_GUIDE.md](docs/AIRL_COMPLETE_GUIDE.md) ì°¸ì¡°

### Implementation Pipeline (Option B - Baseline)

**Phase 2 êµ¬í˜„** (í˜„ì¬ 71% ì™„ë£Œ - Baseline í™•ë³´):

#### Step A: h-specific Training Data ìƒì„±

```python
# fourinarow_airl/generate_training_data.py

for h in [1, 2, 4, 8]:
    policy = DepthLimitedPolicy(h=h)

    trajectories = []
    for episode in range(num_episodes):
        traj = play_episode(env, policy)
        trajectories.append({
            'observations': traj.obs,   # (T+1, 89) - NO h!
            'actions': traj.acts,       # (T,)
            'h': h,                     # metadata only
        })

    save(trajectories, f'trajectories_h{h}.pkl')
```

**âœ… Checkpoint 1**: Observations are 89-dim (NO h)
**âœ… Checkpoint 2**: Actions in range [0, 35]
**âœ… Checkpoint 3**: 'h' is metadata only

#### Step B: Behavior Cloning (BC)

```python
# fourinarow_airl/train_bc.py

for h in [1, 2, 4, 8]:
    # Load h-specific trajectories
    trajectories = load(f'trajectories_h{h}.pkl')

    # Convert to imitation format (WITHOUT h!)
    imitation_trajs = convert_to_imitation_format(trajectories)

    # Train BC policy (depth-agnostic neural network)
    bc_trainer = BC(
        observation_space=Box(89,),   # NO h!
        action_space=Discrete(36),
        demonstrations=imitation_trajs,
    )
    bc_trainer.train(n_epochs=50)

    save(bc_trainer, f'bc_trainer_h{h}.pkl')
```

**âœ… Checkpoint 3**: Convert WITHOUT h
**âœ… Checkpoint 4**: BC policy has NO depth-related attributes

#### Step C: BCë¥¼ PPOë¡œ ë˜í•‘

```python
# fourinarow_airl/create_ppo_generator.py

for h in [1, 2, 4, 8]:
    # Load BC policy
    bc_trainer = load(f'bc_trainer_h{h}.pkl')

    # Create PPO with BC initialization
    ppo_model = PPO(
        'MlpPolicy',
        env,
        learning_rate=3e-4,
    )

    # Copy BC weights to PPO
    ppo_model.policy.load_state_dict(
        bc_trainer.policy.state_dict()
    )

    save(ppo_model, f'ppo_generator_h{h}.zip')
```

**âœ… Checkpoint 5**: PPO uses BC policy (depth-agnostic)

#### Step D: Depth-AGNOSTIC Reward Network

```python
# fourinarow_airl/create_reward_net.py

def create_reward_network(env):
    """
    âš ï¸ CRITICAL: NO h parameter!
    """
    reward_net = BasicRewardNet(
        observation_space=env.observation_space,  # Box(89,)
        action_space=env.action_space,            # Discrete(36)
        hid_sizes=[64, 64],
    )
    return reward_net  # NO h anywhere!
```

**âœ… Checkpoint 6a**: NO h in function signature
**âœ… Checkpoint 6b**: NO h in reward network
**âœ… Checkpoint 6c**: No depth-related attributes
**âœ… Checkpoint 6d**: Forward pass verified

**Technical Note**: `BasicRewardNet` requires two-stage processing:

```python
# Preprocess (handles action one-hot encoding)
state_th, action_th, next_state_th, done_th = reward_net.preprocess(
    obs_tensor,       # (batch, 89) FloatTensor
    action_tensor,    # (batch,) LongTensor - indices!
    next_obs_tensor,  # (batch, 89) FloatTensor
    done_tensor       # (batch,) BoolTensor
)

# Forward pass
reward = reward_net(state_th, action_th, next_state_th, done_th)
```

#### Step E: AIRL Training

```python
# fourinarow_airl/train_airl.py

for h in [1, 2, 4, 8]:
    # Load h-specific generator (BC â†’ PPO)
    gen_algo = PPO.load(f'ppo_generator_h{h}.zip', env=env)

    # Create depth-AGNOSTIC reward network
    reward_net = create_reward_network(env)  # NO h!

    # Load expert trajectories (NO h labels!)
    expert_trajectories = load('expert_trajectories.pkl')

    # AIRL trainer
    trainer = airl.AIRL(
        demonstrations=expert_trajectories,  # NO h labels
        gen_algo=gen_algo,                   # h-specific
        reward_net=reward_net,               # h-AGNOSTIC!
        allow_variable_horizon=True,         # 4-in-a-row games vary
    )

    trainer.train(total_timesteps=50000)

    save(trainer, f'airl_results_h{h}/')
```

**âœ… Checkpoint 7a**: Expert trajectories have NO h labels
**âœ… Checkpoint 7b**: Generator learned from h-specific policy
**âœ… Checkpoint 7c**: Discriminator has NO h parameter

#### Step F: Multi-Depth Comparison (ğŸ”„ ì§„í–‰ ì¤‘)

**ëª©í‘œ**: ì–´ë–¤ hê°€ expert behaviorë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ”ê°€?

```python
results = {}
for h in [1, 2, 4, 8]:
    trainer = load(f'airl_results_h{h}/')

    # Evaluation metrics
    results[h] = {
        'disc_acc': trainer.disc_acc,
        'disc_acc_expert': trainer.disc_acc_expert,
        'disc_acc_gen': trainer.disc_acc_gen,
        'imitation_quality': evaluate_imitation(trainer),
        'kl_divergence': compute_kl(trainer, expert_policy),
    }

# Best h = most balanced discriminator
best_h = argmin(abs(results[h]['disc_acc'] - 0.5) for h in [1,2,4,8])
```

**í‰ê°€ ê¸°ì¤€**:
- **Discriminator accuracy â‰ˆ 0.5** (generator fools discriminator)
- **Trajectory similarity** (Euclidean distance, DTW)
- **Action distribution KL** (behavioral realism)

#### Step G: í‰ê°€ ë° ë¶„ì„ (ğŸ“‹ ê³„íš)

**ë¶„ì„**:
1. Best h ì‹ë³„
2. Learned reward ì‹œê°í™”
3. Policy comparison (h=1 vs h=8)
4. Generalization test (OOD states)

---

## ğŸ“Š AIRL Training Metrics ì´í•´í•˜ê¸°

### Discriminator Metrics

**3ê°€ì§€ ì£¼ìš” ì§€í‘œ**:
```python
disc_acc         # Overall discriminator accuracy
disc_acc_expert  # Accuracy on expert data (should be ~0.5)
disc_acc_gen     # Accuracy on generated data (should be ~0.5)
```

### Training Progression

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Training Stage 1: Discriminator Too Strong (Undertrained)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ disc_acc = 0.5            # Overall looks OK...             â”‚
â”‚ disc_acc_expert = 1.0     # BUT: Discriminator too strong!  â”‚
â”‚ disc_acc_gen = 0.0        # Generator can't fool it         â”‚
â”‚                                                             â”‚
â”‚ Interpretation: Need more training!                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Training Stage 2: Balanced (Well-Trained) âœ…                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ disc_acc â‰ˆ 0.5            # Overall balanced                â”‚
â”‚ disc_acc_expert â‰ˆ 0.5     # Generator fools discriminator!  â”‚
â”‚ disc_acc_gen â‰ˆ 0.5        # Good imitation quality          â”‚
â”‚                                                             â”‚
â”‚ Interpretation: AIRL converged! âœ…                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Training Stage 3: Generator Too Strong (Overtrained)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ disc_acc = 0.5            # Overall balanced                â”‚
â”‚ disc_acc_expert = 0.0     # Generator dominates             â”‚
â”‚ disc_acc_gen = 1.0        # Discriminator too weak          â”‚
â”‚                                                             â”‚
â”‚ Interpretation: Possible mode collapse                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**âš ï¸ Common Mistake**:
- âŒ "disc_acc_expert=1.0 means good!" (NO!)
- âœ… "All metrics â‰ˆ 0.5 means good!" (YES!)

---

## ğŸ”¬ ê¸°ëŒ€ ê²°ê³¼ ë° Contributions

### Expected Results

**RQ1**: Planning depthë¡œ expertise êµ¬ë¶„ ê°€ëŠ¥í•œê°€?
- **ì˜ˆìƒ**: Moderate correlation (r=0.3-0.5)
- **Phase 1 ë°œê²¬**: Negative correlation (expertise = shallower depth)

**RQ2**: Planning-Aware AIRLì´ standard AIRLë³´ë‹¤ ë‚˜ì€ê°€?
- **ì˜ˆìƒ**: âœ… ë” ë‚˜ì€ reward identifiability
- **ì˜ˆìƒ**: âœ… ë” ë‚˜ì€ OOD generalization
- **ì˜ˆìƒ**: âœ… ë” í•´ì„ ê°€ëŠ¥í•œ individual differences

**RQ3**: ìµœì  hëŠ” ë¬´ì—‡ì¸ê°€?
- **ì˜ˆìƒ**: h=2~4 (intermediate planning)
- **ê²€ì¦ ë°©ë²•**: Discriminator balance, imitation quality

### Contributions

**1. Theoretical**:
- Planningì„ latent confounderë¡œ ëª…ì‹œì  ëª¨ë¸ë§
- Reward identifiability í–¥ìƒ ë°©ë²• ì œì‹œ

**2. Methodological**:
- Planning-Aware AIRL framework êµ¬í˜„
- Multi-depth comparison protocol

**3. Empirical**:
- 4-in-a-row ë°ì´í„°ì—ì„œ ê²€ì¦
- Expertise-planning relationship ê·œëª…

**4. Clinical**:
- Planning mechanism ê¸°ë°˜ individual differences ì„¤ëª…
- Future: ì„ìƒ íŠ¹ì„± ì˜ˆì¸¡ ê°€ëŠ¥ì„±

---

## ğŸ“š ì£¼ìš” ì°¸ê³ ë¬¸í—Œ

### Core Papers

1. **van Opheusden, B., Acerbi, L., & Ma, W. J. (2023)**. "Expertise increases planning depth in human gameplay". *Nature*, 618, 1000-1005.
   - https://www.nature.com/articles/s41586-023-06124-2
   - **ê¸°ì—¬**: Planning depthì™€ expertise ê´€ê³„, 4-in-a-row ë°ì´í„°/ëª¨ë¸

2. **Yao, W., Chen, B., & Dragan, A. D. (2024)**. "Planning horizon as a latent confounder in inverse reinforcement learning". *arXiv preprint arXiv:2409.18051*.
   - https://arxiv.org/abs/2409.18051
   - **ê¸°ì—¬**: Planning horizonì´ IRLì—ì„œ confounder ì—­í•  ì¦ëª…

3. **Mhammedi, Z., Helou, D., & Gretton, A. (2023)**. "Reinforcement learning for multi-step inverse kinematics". *arXiv preprint arXiv:2304.05889*.
   - https://arxiv.org/abs/2304.05889
   - **ê¸°ì—¬**: Multi-step factorë¥¼ explicití•˜ê²Œ ëª¨ë¸ë§

4. **Fu, J., Luo, K., & Levine, S. (2018)**. "Learning robust rewards with adversarial inverse reinforcement learning". *ICLR 2018*.
   - **ê¸°ì—¬**: AIRL ì•Œê³ ë¦¬ì¦˜ (MaxEnt IRL + GAN)

### Related Work

- **Ng, A. Y., & Russell, S. J. (2000)**. "Algorithms for inverse reinforcement learning". *ICML 2000*.
- **Ziebart, B. D., et al. (2008)**. "Maximum entropy inverse reinforcement learning". *AAAI 2008*.
- **Ho, J., & Ermon, S. (2016)**. "Generative adversarial imitation learning". *NeurIPS 2016*.

---

## ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ

| ì¹´í…Œê³ ë¦¬ | ë„êµ¬/ë¼ì´ë¸ŒëŸ¬ë¦¬ | ë²„ì „ | ìš©ë„ |
|----------|----------------|------|------|
| **ì–¸ì–´** | Python | 3.9.7 | ë©”ì¸ êµ¬í˜„ |
| **í™˜ê²½** | Conda | - | íŒ¨í‚¤ì§€ ê´€ë¦¬ (pedestrian_analysis) |
| **RL** | stable-baselines3 | latest | PPO êµ¬í˜„ |
| **IRL** | imitation | 1.0.1 | BC, AIRL êµ¬í˜„ |
| **DL** | PyTorch | latest | ì‹ ê²½ë§ í•™ìŠµ |
| **Data** | NumPy, Pandas | latest | ë°ì´í„° ì²˜ë¦¬ |
| **Viz** | Matplotlib, Seaborn | latest | ì‹œê°í™” |
| **Game** | Custom dm_env | - | 4-in-a-row í™˜ê²½ |

### í™˜ê²½ ì„¤ì •

```bash
# Conda í™˜ê²½
conda activate pedestrian_analysis

# OpenMP ì¶©ëŒ í•´ê²° (macOS)
export KMP_DUPLICATE_LIB_OK=TRUE

# Working directory
cd /Users/jinilkim/Library/CloudStorage/OneDrive-Personal/Projects/xRL_pilot/fourinarow_airl
```

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘ (Lab Members)

### 1. í™˜ê²½ ì„¤ì •

```bash
# Repository clone
git clone [repository-url]
cd xRL_pilot

# Conda í™˜ê²½ í™œì„±í™”
conda activate pedestrian_analysis

# OpenMP workaround (macOS only)
export KMP_DUPLICATE_LIB_OK=TRUE
```

### 2. ë¬¸ì„œ ì½ê¸° ìˆœì„œ

**ì‹œì‘**:
1. [README.md](README.md) - í”„ë¡œì íŠ¸ ì†Œê°œ
2. [PROJECT_OVERVIEW.md](PROJECT_OVERVIEW.md) (ì´ íŒŒì¼) - ì—°êµ¬ ë°°ê²½

**Phase 2 ì´í•´**:
3. [AIRL_DESIGN.md](docs/AIRL_DESIGN.md) - ì„¤ê³„ ë¬¸ì„œ
4. [AIRL_COMPLETE_GUIDE.md](docs/AIRL_COMPLETE_GUIDE.md) - ì‹¤í–‰ ê°€ì´ë“œ â­
5. [PHASE2_PROGRESS.md](progress/PHASE2_PROGRESS.md) - í˜„ì¬ ì§„í–‰ ìƒí™©

**êµ¬í˜„ ì„¸ë¶€ì‚¬í•­**:
6. [IMPLEMENTATION_NOTES.md](docs/IMPLEMENTATION_NOTES.md) - ê¸°ìˆ  ì°¸ê³ ì‚¬í•­
7. [IMPLEMENTATION_SUMMARY.md](docs/IMPLEMENTATION_SUMMARY.md) - êµ¬í˜„ ìš”ì•½

### 3. ì½”ë“œ ì‹¤í–‰

```bash
cd fourinarow_airl

# Step A: Training data ìƒì„±
python3 generate_training_data.py --num_episodes 100

# Step B: BC í•™ìŠµ
python3 train_bc.py --n_epochs 50

# Step C: PPO generator ìƒì„±
python3 create_ppo_generator.py

# Step D: Reward network í…ŒìŠ¤íŠ¸
python3 create_reward_net.py --test

# Step E: AIRL í•™ìŠµ
python3 train_airl.py --total_timesteps 50000
```

### 4. ê²°ê³¼ í™•ì¸

```bash
# Training trajectories
ls data/training_trajectories/

# BC policies
ls models/bc_policies/

# PPO generators
ls models/ppo_generators/

# AIRL results
ls models/airl_results/
```

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„ (Step F)

**í˜„ì¬ ìƒíƒœ**: Steps A-E ì™„ë£Œ (71%)

**ë‹¤ìŒ ì‘ì—…**: Multi-Depth Comparison

```bash
# Train AIRL for all depths with sufficient timesteps
python3 train_airl.py --total_timesteps 100000
```

**ë¶„ì„ ê³„íš**:
1. Compare discriminator metrics across h âˆˆ {1, 2, 4, 8}
2. Evaluate imitation quality
3. Identify best h for expert behavior
4. Visualize learned rewards

**ì—°êµ¬ ì§ˆë¬¸**: "ì–´ë–¤ planning depthê°€ expert behaviorë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ”ê°€?"

---

## ğŸ¤ ì—°êµ¬ì§„ ë° ê¸°ì—¬

**ì†Œì†**: [Lab Name]
**PI**: [PI Name]
**ì—°êµ¬ì›**: [Researcher Names]

**ê¸°ì—¬ ë°©ë²•**:
- ì´ìŠˆ ì œê¸°: GitHub Issues
- ì½”ë“œ ë¦¬ë·°: Pull Requests
- ë¬¸ì„œ ê°œì„ : [DOCUMENTATION_QUALITY_REVIEW.md](progress/DOCUMENTATION_QUALITY_REVIEW.md) ì°¸ì¡°

---

## ğŸ“§ ë¬¸ì˜

- **ì´ë©”ì¼**: [contact email]
- **GitHub**: [repository URL]
- **Lab Website**: [lab website]

---

**Last Updated**: 2025-12-26
**Document Version**: 1.0
**Status**: Phase 2 (71% complete)
