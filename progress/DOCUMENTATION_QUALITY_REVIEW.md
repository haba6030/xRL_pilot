# ë¬¸ì„œ í’ˆì§ˆ ê²€í†  ë° ê°œì„  ë°©ì•ˆ

**ê²€í† ì¼**: 2025-12-25
**ëª©ì **: ì—°êµ¬ì‹¤ êµ¬ì„±ì›ë“¤ì´ í”„ë¡œì íŠ¸ë¥¼ ì´í•´í•˜ê³  ë…¼ì˜í•˜ê¸° ìœ„í•œ ë¬¸ì„œ ì •ë¦¬
**ëŒ€ìƒ**: GitHub/Notion ê³µìœ ìš© ë¬¸ì„œ

---

## ğŸ“‹ í˜„ì¬ ë¬¸ì„œ í˜„í™©

### ì´ 19ê°œ ë¬¸ì„œ ì¡´ì¬

**ë¬¸ì œì **:
- âŒ ë„ˆë¬´ ë§ì€ ë¬¸ì„œ (19ê°œ) â†’ ì–´ë””ì„œ ì‹œì‘í•´ì•¼ í• ì§€ ë¶ˆëª…í™•
- âŒ ì¤‘ë³µëœ ë‚´ìš© (AIRL_COMPLETE_GUIDE vs GYMNASIUM_AND_AIRL_GUIDE)
- âŒ ì¼ë¶€ outdated ë‚´ìš© (OPTION_A_VS_B, OPTION_DIFFERENCE_SIMPLE)
- âŒ ëª…í™•í•œ ë¬¸ì„œ ê³„ì¸µ êµ¬ì¡° ë¶€ì¬
- âŒ ì‹ ê·œ ë©¤ë²„ë¥¼ ìœ„í•œ ì‹œì‘ì  ë¶ˆëª…í™•

---

## âœ… ê¶Œì¥ ë¬¸ì„œ êµ¬ì¡° (3-Tier)

### ğŸ“Œ Tier 1: ì‹œì‘ ë¬¸ì„œ (í•„ìˆ˜ ì½ê¸°)

**ëª©ì **: í”„ë¡œì íŠ¸ ì „ì²´ ê°œìš” íŒŒì•… (15-20ë¶„)

```
1. README.md                    # í”„ë¡œì íŠ¸ ì†Œê°œ, Quick Start
   â”œâ”€ ì—°êµ¬ ëª©ì  ë° ë°°ê²½
   â”œâ”€ í•µì‹¬ ì•„ì´ë””ì–´ (Planning-Aware IRL)
   â”œâ”€ ì£¼ìš” ê²°ê³¼ (í˜„ì¬ê¹Œì§€)
   â””â”€ ë‹¤ìŒ ë‹¨ê³„ë¡œ ê°€ëŠ” ë§í¬

2. PROJECT_OVERVIEW.md (ì‹ ê·œ ìƒì„± ê¶Œì¥)
   â”œâ”€ ì—°êµ¬ ì§ˆë¬¸ (Research Questions)
   â”œâ”€ ì ‘ê·¼ ë°©ë²• (Methodology Overview)
   â”œâ”€ ë°ì´í„° (4-in-a-row expert data)
   â”œâ”€ ì£¼ìš” ë°œê²¬ (Key Findings)
   â””â”€ í˜„ì¬ ìƒíƒœ (Current Status)
```

### ğŸ“š Tier 2: í•µì‹¬ ë¬¸ì„œ (ê¹Šì´ ì´í•´)

**ëª©ì **: êµ¬í˜„ ë° ì´ë¡  ì´í•´ (1-2ì‹œê°„)

```
3. PLANNING_DEPTH_PRINCIPLES.md    # í•µì‹¬ ì›ì¹™ (ì´ë¡ )
   â””â”€ hëŠ” POLICYì—ë§Œ ì¡´ì¬, rewardëŠ” depth-agnostic

4. IMPLEMENTATION_SUMMARY.md        # êµ¬í˜„ ìš”ì•½ (Steps A-E)
   â”œâ”€ Step A: Training data generation
   â”œâ”€ Step B: Behavior Cloning
   â”œâ”€ Step C: PPO generator
   â”œâ”€ Step D: Reward network
   â””â”€ Step E: AIRL training

5. AIRL_DESIGN.md                   # ì„¤ê³„ ë¬¸ì„œ
   â”œâ”€ Environment êµ¬ì¡°
   â”œâ”€ State/Action representation
   â”œâ”€ AIRL ì ìš© ì „ëµ
   â””â”€ í•™ìŠµ ì ˆì°¨ (ì‹¤ì œ êµ¬í˜„ ë°˜ì˜)

6. IMPLEMENTATION_NOTES.md          # ê¸°ìˆ  ì„¸ë¶€ì‚¬í•­
   â”œâ”€ Library ë²„ì „ ë° ì„¤ì •
   â”œâ”€ API ì‚¬ìš©ë²• (imitation, SB3)
   â”œâ”€ ì£¼ìš” ì´ìŠˆ ë° í•´ê²°ì±…
   â””â”€ Training metrics í•´ì„
```

### ğŸ”§ Tier 3: ì°¸ê³  ë¬¸ì„œ (í•„ìš”ì‹œ)

**ëª©ì **: íŠ¹ì • ì£¼ì œ ê¹Šì´ íŒŒê¸°

```
7. PHASE2_PROGRESS.md               # ì§„í–‰ ìƒí™© (ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸)
8. GYMNASIUM_AND_AIRL_GUIDE.md      # Gymnasium í™˜ê²½ ì´í•´
9. DEPTH_INTEGRATION_DETAILED.md    # Depth integration ìƒì„¸
10. PHASE2_VALIDATION_CHECKLIST.md  # ê²€ì¦ í”„ë¡œí† ì½œ
```

### ğŸ—‘ï¸ ì‚­ì œ/í†µí•© ê¶Œì¥

```
âŒ AIRL_COMPLETE_GUIDE.md          â†’ AIRL_DESIGN.mdì™€ ì¤‘ë³µ
âŒ OPTION_A_VS_B.md                 â†’ Outdated (BC approach ì„ íƒë¨)
âŒ OPTION_DIFFERENCE_SIMPLE.md      â†’ Outdated
âŒ IMPLEMENTATION_STATUS.md         â†’ IMPLEMENTATION_SUMMARY.mdì™€ ì¤‘ë³µ
âŒ RESPONSE_TO_FEEDBACK.md          â†’ í•„ìš”ì‹œ ë³„ë„ í´ë”ë¡œ ì´ë™
âŒ RESEARCH_DISCUSSION.md           â†’ í•„ìš”ì‹œ ë³„ë„ í´ë”ë¡œ ì´ë™
âŒ DEPTH_VARIABLE_VERIFICATION.md   â†’ ê²€ì¦ ì™„ë£Œ, ì•„ì¹´ì´ë¸Œ
âŒ FOLDER_STRUCTURE.md              â†’ READMEì— í†µí•©
```

---

## ğŸ“‚ ê¶Œì¥ í´ë” êµ¬ì¡°

```
xRL_pilot/
â”œâ”€â”€ README.md                          # í”„ë¡œì íŠ¸ ì§„ì…ì 
â”œâ”€â”€ PROJECT_OVERVIEW.md                # ì—°êµ¬ ê°œìš” (ì‹ ê·œ ìƒì„±)
â”‚
â”œâ”€â”€ docs/                              # í•µì‹¬ ë¬¸ì„œ
â”‚   â”œâ”€â”€ 1_PRINCIPLES.md               # í•µì‹¬ ì›ì¹™ (â† PLANNING_DEPTH_PRINCIPLES.md)
â”‚   â”œâ”€â”€ 2_DESIGN.md                   # ì„¤ê³„ (â† AIRL_DESIGN.md)
â”‚   â”œâ”€â”€ 3_IMPLEMENTATION.md           # êµ¬í˜„ (â† IMPLEMENTATION_SUMMARY.md)
â”‚   â””â”€â”€ 4_TECHNICAL_NOTES.md          # ê¸°ìˆ  (â† IMPLEMENTATION_NOTES.md)
â”‚
â”œâ”€â”€ progress/                          # ì§„í–‰ ìƒí™©
â”‚   â”œâ”€â”€ CURRENT_STATUS.md             # í˜„ì¬ ìƒíƒœ (â† PHASE2_PROGRESS.md)
â”‚   â””â”€â”€ VALIDATION_CHECKLIST.md       # ê²€ì¦ (â† PHASE2_VALIDATION_CHECKLIST.md)
â”‚
â”œâ”€â”€ guides/                            # ìƒì„¸ ê°€ì´ë“œ
â”‚   â”œâ”€â”€ gymnasium_guide.md            # (â† GYMNASIUM_AND_AIRL_GUIDE.md)
â”‚   â””â”€â”€ depth_integration.md          # (â† DEPTH_INTEGRATION_DETAILED.md)
â”‚
â”œâ”€â”€ archive/                           # ì•„ì¹´ì´ë¸Œ
â”‚   â”œâ”€â”€ design_options/               # ì„¤ê³„ ì˜µì…˜ ë…¼ì˜
â”‚   â”‚   â”œâ”€â”€ option_a_vs_b.md
â”‚   â”‚   â””â”€â”€ option_difference.md
â”‚   â””â”€â”€ old_discussions/              # ê³¼ê±° ë…¼ì˜
â”‚       â”œâ”€â”€ research_discussion.md
â”‚       â””â”€â”€ response_to_feedback.md
â”‚
â”œâ”€â”€ fourinarow_airl/                   # ì½”ë“œ
â”‚   â”œâ”€â”€ README.md                     # ì½”ë“œ ì‚¬ìš©ë²•
â”‚   â”œâ”€â”€ generate_training_data.py
â”‚   â”œâ”€â”€ train_bc.py
â”‚   â”œâ”€â”€ create_ppo_generator.py
â”‚   â”œâ”€â”€ create_reward_net.py
â”‚   â””â”€â”€ train_airl.py
â”‚
â””â”€â”€ CLAUDE.md                          # Claude ì§€ì‹œì‚¬í•­ (ìœ ì§€)
```

---

## ğŸ“ ê° ë¬¸ì„œë³„ ê°œì„  ì‚¬í•­

### 1. README.md (ì‹ ê·œ ì‘ì„± í•„ìš”)

**í˜„ì¬ ë¬¸ì œ**: ê¸°ì¡´ READMEê°€ ì—†ê±°ë‚˜ outdated

**ê¶Œì¥ êµ¬ì¡°**:
```markdown
# Planning-Aware AIRL for 4-in-a-Row

## ğŸ¯ ì—°êµ¬ ëª©ì 
Planning depthë¥¼ explicití•˜ê²Œ ëª¨ë¸ë§í•˜ì—¬ IRLì˜ reward identifiability í–¥ìƒ

## ğŸ”‘ í•µì‹¬ ì•„ì´ë””ì–´
- Planning depth hëŠ” **POLICYì—ë§Œ** ì¡´ì¬
- Reward networkëŠ” **ì™„ì „íˆ depth-agnostic**
- hë³„ í•™ìŠµ í›„ ë¹„êµ â†’ ìµœì  h ì¶”ì •

## ğŸ“Š í˜„ì¬ ìƒíƒœ
- âœ… Steps A-E ì™„ë£Œ (71%)
- ğŸ“ Step F ì¤€ë¹„ ì¤‘ (Multi-depth comparison)
- 8/8 validation checkpoints passed

## ğŸš€ Quick Start
1. Environment setup
2. Run test: `python3 train_airl.py --test`
3. Full training: `python3 train_airl.py --total_timesteps 50000`

## ğŸ“š ë¬¸ì„œ
- [ì—°êµ¬ ê°œìš”](PROJECT_OVERVIEW.md) - ì‹œì‘ì€ ì—¬ê¸°ì„œ
- [í•µì‹¬ ì›ì¹™](docs/1_PRINCIPLES.md) - ì´ë¡ ì  ë°°ê²½
- [êµ¬í˜„ ìš”ì•½](docs/3_IMPLEMENTATION.md) - ì½”ë“œ ì´í•´

## ğŸ‘¥ Team
- PI: [ì´ë¦„]
- Contributors: [ì´ë¦„ë“¤]

## ğŸ“„ References
- van Opheusden et al. (2023) - 4-in-a-row expertise
- Yao et al. (2024) - Planning horizon in IRL
```

---

### 2. PROJECT_OVERVIEW.md (ì‹ ê·œ ìƒì„±)

**ëª©ì **: ì—°êµ¬ ì „ì²´ ê·¸ë¦¼ ì œê³µ

```markdown
# ì—°êµ¬ ê°œìš”: Planning-Aware AIRL

## 1. ì—°êµ¬ ì§ˆë¬¸

**Q1**: Planning depthê°€ ë‹¤ë¥¸ ì‚¬ëŒë“¤ì˜ í–‰ë™ì„ ê°™ì€ reward functionìœ¼ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆëŠ”ê°€?

**Q2**: Planning depthë¥¼ explicití•˜ê²Œ ëª¨ë¸ë§í•˜ë©´ IRLì˜ reward identifiabilityê°€ í–¥ìƒë˜ëŠ”ê°€?

**Q3**: 4-in-a-row expertë“¤ì˜ ìµœì  planning depthëŠ”?

## 2. ë°ì´í„°

- **ì¶œì²˜**: van Opheusden et al. (2023)
- **ë‚´ìš©**: 40ëª… ì°¸ê°€ì, 67K trials
- **ê²Œì„**: 4-in-a-row (6Ã—6 board)
- **íŠ¹ì§•**: Expert vs Novice, Elo rating ìˆìŒ

## 3. ë°©ë²•ë¡ 

### 3.1 ê¸°ì¡´ ì ‘ê·¼ (van Opheusden)
- BFS + heuristic weights
- Planning depth hëŠ” ê³ ì • parameter

### 3.2 ìš°ë¦¬ ì ‘ê·¼ (Planning-Aware IRL)
- hë³„ë¡œ **ë³„ë„ generator** í•™ìŠµ (h âˆˆ {1,2,4,8})
- **ê³µí†µ reward network** (depth-agnostic)
- AIRLë¡œ adversarial learning

### 3.3 í•µì‹¬ ì›ì¹™
**hëŠ” POLICYì—ë§Œ ì¡´ì¬, RewardëŠ” depth-agnostic**

| Component | h ì¡´ì¬? |
|-----------|---------|
| DepthLimitedPolicy | âœ… YES |
| Observations | âŒ NO |
| Reward Network | âŒ NO |

## 4. êµ¬í˜„ (Steps A-E)

```
A. Generate h-specific training data
   â””â”€ DepthLimitedPolicy(h) â†’ trajectories

B. Behavior Cloning
   â””â”€ Neural network mimics h-specific behavior

C. Wrap BC with PPO
   â””â”€ BC policy â†’ PPO generator

D. Create reward network
   â””â”€ Depth-agnostic discriminator

E. AIRL training
   â””â”€ h-specific generator + depth-agnostic reward
```

## 5. ì£¼ìš” ê²°ê³¼ (í˜„ì¬ê¹Œì§€)

- âœ… ëª¨ë“  validation checkpoints í†µê³¼ (8/8)
- âœ… AIRL training ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ
- ğŸ“Š Multi-depth comparison ì§„í–‰ ì¤‘

## 6. ë‹¤ìŒ ë‹¨ê³„

1. All depths training (h=1,2,4,8)
2. Discriminator metrics ë¹„êµ
3. Best h ì„ íƒ
4. Expert behavior ë¶„ì„

## 7. ì°¸ê³  ë¬¸í—Œ

- van Opheusden et al. (2023). Expertise increases planning depth in human gameplay. *Nature*
- Yao et al. (2024). Planning in Inverse Reinforcement Learning
```

---

### 3. AIRL_DESIGN.md ê°œì„ 

**í˜„ì¬ ë¬¸ì œ**: ì¼ë¶€ ì´ˆê¸° ì„¤ê³„ì™€ ì‹¤ì œ êµ¬í˜„ì´ í˜¼ì¬

**ê°œì„  ë°©ì•ˆ**:
```markdown
# AIRL ì„¤ê³„ ë¬¸ì„œ

## âš ï¸ ì£¼ì˜
- ì´ ë¬¸ì„œëŠ” **ì´ˆê¸° ì„¤ê³„**ë¥¼ í¬í•¨í•©ë‹ˆë‹¤
- **ì‹¤ì œ êµ¬í˜„**ì€ ì¼ë¶€ ë‹¤ë¦…ë‹ˆë‹¤ (Section C ì°¸ì¡°)
- ìµœì‹  êµ¬í˜„ì€ IMPLEMENTATION_SUMMARY.md ì°¸ì¡°

## ëª©ì°¨
1. Pedestrian í”„ë¡œì íŠ¸ ë¶„ì„ (ì°¸ê³ ìš©)
2. 4-in-a-row ì ìš© ê°€ëŠ¥ì„±
3. ì œì•ˆ ì„¤ê³„
4. Planning-Aware AIRL (ì´ë¡ )
5. êµ¬í˜„ ë¡œë“œë§µ
6. ì˜ˆìƒ ê²°ê³¼
7. ë¯¸í•´ê²° ì´ìŠˆ
8. ìµœì¢… íŒë‹¨
9. Implementation Notes â†’ IMPLEMENTATION_NOTES.md ì°¸ì¡°

## Section C: í•™ìŠµ ì ˆì°¨ (Actual Implementation)

**ì£¼ìš” ë³€ê²½ì‚¬í•­**:
| ì„¤ê³„ | ì‹¤ì œ êµ¬í˜„ |
|------|-----------|
| BFS Distillation | BC (Behavior Cloning) |
| h-conditioned reward | Completely depth-agnostic |

[ì‹¤ì œ ì½”ë“œ ì˜ˆì‹œ...]
```

---

### 4. IMPLEMENTATION_SUMMARY.md ê°œì„ 

**í˜„ì¬**: ì¢‹ìŒ, ì•½ê°„ì˜ êµ¬ì¡° ê°œì„ ë§Œ í•„ìš”

**ê°œì„ **:
```markdown
# êµ¬í˜„ ìš”ì•½

**Status**: 5/7 steps complete (71%)
**Checkpoints**: 8/8 passed âœ…

## ğŸ“– ë¹ ë¥¸ ì´í•´

### ì „ì²´ Pipeline (í•œëˆˆì—)

```
Expert Data (h=?)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  For each h âˆˆ {1, 2, 4, 8}      â”‚
â”‚                                   â”‚
â”‚  Step A: DepthLimitedPolicy(h)   â”‚
â”‚     â†“                             â”‚
â”‚  Step B: BC â†’ Neural Policy      â”‚
â”‚     â†“                             â”‚
â”‚  Step C: BC â†’ PPO Generator      â”‚
â”‚                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Step D: Depth-AGNOSTIC Reward Net
    â†“
Step E: AIRL Training
    â”œâ”€ Generator (h-specific)
    â””â”€ Discriminator (depth-agnostic)
    â†“
Step F: Compare h values
    â†“
Step G: Best h selection
```

### í•µì‹¬ ì›ì¹™ (ë‹¤ì‹œ ê°•ì¡°)

âœ… hëŠ” POLICYì—ë§Œ
âŒ hëŠ” REWARDì— ì—†ìŒ

[ë‚˜ë¨¸ì§€ ê¸°ì¡´ ë‚´ìš©...]
```

---

### 5. IMPLEMENTATION_NOTES.md ê°œì„ 

**í˜„ì¬**: ë§¤ìš° ì¢‹ìŒ, ì†Œí­ ê°œì„ 

**ì¶”ê°€ ê¶Œì¥**:
```markdown
## ëª©ì°¨
1. [Environment Setup](#1-environment-setup)
2. [BasicRewardNet Usage](#2-basicrewardnet-usage)
3. [imitation 1.0.1 API](#3-imitation-101-api)
4. [Architecture Matching](#4-architecture-matching)
5. [Data Formats](#5-data-formats)
6. [AIRL Metrics](#6-airl-training-metrics) â­ ì¤‘ìš”!
7. [Common Issues](#9-common-issues-and-solutions)
8. [Training Tips](#10-training-recommendations)

## 6. AIRL Training Metrics â­

### Discriminator Metrics í•´ì„ (ë§¤ìš° ì¤‘ìš”!)

**âŒ ì˜ëª»ëœ í•´ì„**:
```
disc_acc_expert = 1.0  â†’ "ì™„ë²½í•˜ê²Œ í•™ìŠµë¨!"
disc_acc_gen = 0.0     â†’ "ì™„ë²½í•˜ê²Œ í•™ìŠµë¨!"
```

**âœ… ì˜¬ë°”ë¥¸ í•´ì„**:
```python
# Well-trained (ëª©í‘œ)
disc_acc_expert â‰ˆ 0.5  # Generatorê°€ discriminatorë¥¼ ì†ì„
disc_acc_gen â‰ˆ 0.5     # ì¢‹ì€ imitation
disc_acc â‰ˆ 0.5         # Balanced

# Undertrained (í˜„ì¬)
disc_acc_expert = 1.0  # Discriminatorê°€ ë„ˆë¬´ ê°•í•¨
disc_acc_gen = 0.0     # Generatorê°€ ì•½í•¨
â†’ total_timesteps ì¦ê°€ í•„ìš”!
```

### ì‹œê°í™”ë¡œ ì´í•´í•˜ê¸°

```
Training Progress:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Early     disc_acc_expert: 1.0 â”â”â”â”â”â”â”â”â”â”
(Bad)     disc_acc_gen:    0.0

Mid       disc_acc_expert: 0.7 â”â”â”â”â”â”â”
          disc_acc_gen:    0.3 â”â”â”

Good      disc_acc_expert: 0.5 â”â”â”â”â”
(Target)  disc_acc_gen:    0.5 â”â”â”â”â”
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```
```

---

### 6. PLANNING_DEPTH_PRINCIPLES.md

**í˜„ì¬**: ì´ë¡ ì ìœ¼ë¡œ ì˜ ì •ë¦¬ë¨

**ì¶”ê°€ ê¶Œì¥**: ì‹œê°ì  ì˜ˆì‹œ
```markdown
## í•µì‹¬ ì›ì¹™ (Visual)

### âŒ ì˜ëª»ëœ ì ‘ê·¼ (hë¥¼ rewardì— ë„£ìŒ)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Discriminator (Reward Network) â”‚
â”‚                                  â”‚
â”‚  Input: (state, action, h)      â”‚  â† h í¬í•¨ (ì˜ëª»ë¨!)
â”‚  Output: reward                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### âœ… ì˜¬ë°”ë¥¸ ì ‘ê·¼ (hëŠ” policyì—ë§Œ)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Generator (Policy)              â”‚
â”‚                                  â”‚
â”‚  DepthLimitedPolicy(h=2)         â”‚  â† hëŠ” ì—¬ê¸°ë§Œ!
â”‚  Input: state (89-dim)           â”‚
â”‚  Output: action                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Discriminator (Reward Network) â”‚
â”‚                                  â”‚
â”‚  Input: (state, action)          â”‚  â† h ì—†ìŒ!
â”‚  Output: reward                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ì™œ ì´ë ‡ê²Œ í•´ì•¼ í•˜ëŠ”ê°€?

**ì´ìœ  1: Reward Identifiability**
- hë¥¼ rewardì— ë„£ìœ¼ë©´ confounding
- ê°™ì€ behaviorë¥¼ ë‹¤ë¥¸ (h, reward) ì¡°í•©ìœ¼ë¡œ ì„¤ëª… ê°€ëŠ¥

**ì´ìœ  2: Generalization**
- Depth-agnostic rewardëŠ” ëª¨ë“  hì— ì ìš© ê°€ëŠ¥
- Transfer learning ìš©ì´

**ì´ìœ  3: Interpretability**
- RewardëŠ” "ë¬´ì—‡ì´ ì¢‹ì€ê°€" (what)
- Planning depthëŠ” "ì–´ë–»ê²Œ ìƒê°í•˜ëŠ”ê°€" (how)
```

---

## ğŸ¨ GitHub/Notion ìµœì í™”

### GitHubìš© ê°œì„ ì‚¬í•­

1. **READMEì— Badges ì¶”ê°€**
```markdown
![Progress](https://img.shields.io/badge/Progress-71%25-green)
![Checkpoints](https://img.shields.io/badge/Checkpoints-8%2F8-success)
![Status](https://img.shields.io/badge/Status-Active-blue)
```

2. **Wiki í™œìš©**
- Main docs â†’ Wikië¡œ ì´ë™
- READMEëŠ” ê°„ê²°í•˜ê²Œ ìœ ì§€

3. **Issues/Projects í™œìš©**
- Step F, Gë¥¼ Issuesë¡œ tracking
- Project boardë¡œ ì§„í–‰ìƒí™© ì‹œê°í™”

### Notionìš© ê°œì„ ì‚¬í•­

1. **Database êµ¬ì¡°**
```
ğŸ“Š Progress Tracker
â”œâ”€ Steps (A-G)
â”‚  â”œâ”€ Status (Done/In Progress/Pending)
â”‚  â”œâ”€ Files
â”‚  â””â”€ Checkpoints
â”œâ”€ Validation Checkpoints (1-8)
â””â”€ Metrics (disc_acc ë“±)
```

2. **Toggle Blocks í™œìš©**
```
â–¶ Step A: Generate Training Data
  â””â”€ [ìƒì„¸ ë‚´ìš©...]

â–¶ Step B: Behavior Cloning
  â””â”€ [ìƒì„¸ ë‚´ìš©...]
```

3. **Callout Boxes**
```
ğŸ’¡ í•µì‹¬ ì›ì¹™
hëŠ” POLICYì—ë§Œ ì¡´ì¬!

âš ï¸ ì£¼ì˜ì‚¬í•­
disc_acc = 0.5ê°€ ëª©í‘œ!

âœ… ì™„ë£Œ
All checkpoints passed
```

---

## ğŸš€ ì‹¤í–‰ ê¶Œì¥ì‚¬í•­

### Phase 1: ë¬¸ì„œ ì •ë¦¬ (ìš°ì„ ìˆœìœ„)

**ì¦‰ì‹œ ì‹¤í–‰**:
1. âœ… README.md ìƒˆë¡œ ì‘ì„±
2. âœ… PROJECT_OVERVIEW.md ìƒì„±
3. âœ… í´ë” êµ¬ì¡° ì •ë¦¬ (`docs/`, `progress/`, `archive/`)
4. âœ… ì¤‘ë³µ/outdated ë¬¸ì„œ ì‚­ì œ

**ë‹¤ìŒ ì£¼**:
5. ê° í•µì‹¬ ë¬¸ì„œì— ëª©ì°¨ ë° ë‚´ë¶€ ë§í¬ ì¶”ê°€
6. ì‹œê°ì  ë‹¤ì´ì–´ê·¸ë¨ ì¶”ê°€ (ì›ì¹™, pipeline)
7. GitHub Wiki ì„¤ì •

### Phase 2: ì ‘ê·¼ì„± í–¥ìƒ

1. **ì‹ ê·œ ë©¤ë²„ Onboarding ê°€ì´ë“œ** ì‘ì„±
   - 30ë¶„ Quick Start
   - 1ì‹œê°„ Deep Dive
   - ì²« ì‹¤í—˜ ì‹¤í–‰í•˜ê¸°

2. **FAQ ì„¹ì…˜** ì¶”ê°€
   - "hë¥¼ rewardì— ë„£ìœ¼ë©´ ì•ˆ ë˜ëŠ” ì´ìœ ?"
   - "disc_acc = 0.5ê°€ ì™œ ì¢‹ì€ê°€?"
   - "BC vs BFS Distillation?"

3. **Troubleshooting Guide**
   - OpenMP ì—ëŸ¬
   - Variable horizon ì—ëŸ¬
   - Tensor dimension ì—ëŸ¬

---

## ğŸ“Š ë¬¸ì„œ í’ˆì§ˆ ì²´í¬ë¦¬ìŠ¤íŠ¸

### í•„ìˆ˜ ìš”ì†Œ

ê° ë¬¸ì„œëŠ” ë‹¤ìŒì„ í¬í•¨í•´ì•¼ í•¨:

- [ ] **ëª…í™•í•œ ì œëª© ë° ëª©ì ** (ì²« 3ì¤„ì—)
- [ ] **ìµœì¢… ì—…ë°ì´íŠ¸ ë‚ ì§œ**
- [ ] **ê´€ë ¨ ë¬¸ì„œ ë§í¬** (See also)
- [ ] **ì˜ˆì œ ì½”ë“œ** (í•´ë‹¹ ì‹œ)
- [ ] **ì‹œê°ì  ìš”ì†Œ** (ë‹¤ì´ì–´ê·¸ë¨, í‘œ)
- [ ] **í˜„ì¬ ìƒíƒœ í‘œì‹œ** (âœ…/ğŸ”„/âŒ)

### ê°€ë…ì„± ì²´í¬

- [ ] ì œëª© ê³„ì¸µ êµ¬ì¡° (H1 â†’ H2 â†’ H3)
- [ ] ì½”ë“œ ë¸”ë¡ì— ì–¸ì–´ ëª…ì‹œ
- [ ] ì¤‘ìš” ë‚´ìš©ì€ Callout/Bold
- [ ] ë„ˆë¬´ ê¸´ ë¬¸ë‹¨ ë¶„ë¦¬ (3-5ì¤„)
- [ ] Technical term í•œ/ì˜ ë³‘ê¸°

---

## ğŸ’¡ ìµœì¢… ê¶Œì¥ì‚¬í•­

### ìš°ì„ ìˆœìœ„ 1 (ì´ë²ˆ ì£¼ ë‚´)

1. **README.md ì‘ì„±** - í”„ë¡œì íŠ¸ ì§„ì…ì 
2. **PROJECT_OVERVIEW.md ìƒì„±** - ì—°êµ¬ ì „ì²´ ê·¸ë¦¼
3. **í´ë” ì •ë¦¬** - docs/, progress/, archive/
4. **ì¤‘ë³µ ë¬¸ì„œ ì‚­ì œ** - 8ê°œ ë¬¸ì„œ ì œê±°

### ìš°ì„ ìˆœìœ„ 2 (ë‹¤ìŒ ì£¼)

5. AIRL_DESIGN.mdì— "ì£¼ì˜" ì„¹ì…˜ ì¶”ê°€
6. IMPLEMENTATION_NOTES.mdì— ì‹œê°í™” ì¶”ê°€
7. ê° ë¬¸ì„œì— ëª©ì°¨ ì¶”ê°€
8. GitHub Wiki ì„¤ì •

### ìš°ì„ ìˆœìœ„ 3 (ì—¬ìœ  ìˆì„ ë•Œ)

9. Notion database êµ¬ì¶•
10. Onboarding guide ì‘ì„±
11. FAQ ì„¹ì…˜ ì‘ì„±
12. Video tutorial ì œì‘ (optional)

---

**ì‘ì„±ì**: Claude
**ê²€í†  í•„ìš”**: ì—°êµ¬ì‹¤ PI ë° ì£¼ìš” ë©¤ë²„
**ë‹¤ìŒ ê²€í† **: Step F ì™„ë£Œ í›„
