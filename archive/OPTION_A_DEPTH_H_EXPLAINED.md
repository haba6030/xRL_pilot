# Option Aì—ì„œ depth hëŠ” ì–´ë–»ê²Œ ì‚¬ìš©ë˜ë‚˜?

## TL;DR (í•µì‹¬ ë‹µë³€)

**Option Aì—ì„œ depth hëŠ” ì£¼ë¡œ "ì‹¤í—˜ ì„¤ê³„ ë³€ìˆ˜"ì…ë‹ˆë‹¤.**

```python
# hëŠ” ë„¤íŠ¸ì›Œí¬ì— ë“¤ì–´ê°€ì§€ ì•ŠìŒ!
# ë‹¨ì§€ ì–´ë–¤ expert dataë¥¼ ì‚¬ìš©í• ì§€ ê²°ì •

for h in [1, 2, 4, 8]:
    # 1. Expert data: BFS(h)ë¡œ ìƒì„±í•œ ë°ì´í„° ì„ íƒ
    expert_data = load_expert_trajectories_from_BFS(h=h)

    # 2. Generator: ëœë¤ ì´ˆê¸°í™” (h ì‚¬ìš© ì•ˆ í•¨!)
    gen = PPO("MlpPolicy", env)  # NO h parameter!

    # 3. Reward network: depth-agnostic (h ì‚¬ìš© ì•ˆ í•¨!)
    reward = BasicRewardNet(obs_space, action_space)  # NO h!

    # 4. AIRL í•™ìŠµ
    train_airl(expert_data, gen, reward)

    # 5. ì €ì¥: hëŠ” íŒŒì¼ëª…ì—ë§Œ ì‚¬ìš©
    save(gen, f"airl_pure_generator_h{h}.zip")
```

**í•µì‹¬**: hëŠ” **ì–´ë–¤ expertë¥¼ ëª¨ë°©í• ì§€** ê²°ì •í•˜ëŠ” ë ˆì´ë¸”ì¼ ë¿, ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ì™€ëŠ” ë¬´ê´€!

---

## ìƒì„¸ ì„¤ëª…

### ì‹œë‚˜ë¦¬ì˜¤ 1: ê° hë§ˆë‹¤ ë‹¤ë¥¸ Expert ì‚¬ìš©

**ê°€ì¥ ì¼ë°˜ì ì¸ ì‚¬ìš©ë²•**

```python
# Synthetic expert data ìƒì„±
for h in [1, 2, 4, 8]:
    # BFS(h)ë¡œ "expert" ìƒì„±
    expert_trajs = generate_depth_limited_trajectories(h=h, num_episodes=100)

    # ì´ expertë¥¼ ëª¨ë°©í•˜ëŠ” pure NN policy í•™ìŠµ
    train_airl_pure_nn(h=h, expert_trajs=expert_trajs)
```

**hì˜ ì—­í• **:
- BFS(h=1)ë¡œ ìƒì„±í•œ expertì™€ BFS(h=8)ë¡œ ìƒì„±í•œ expertëŠ” **í–‰ë™ì´ ë‹¤ë¦„**
- Option AëŠ” ê°ê°ì˜ expertë¥¼ ëª¨ë°©í•˜ë ¤ê³  ì‹œë„
- ë„¤íŠ¸ì›Œí¬ëŠ” hë¥¼ ëª¨ë¥´ì§€ë§Œ, **í•™ìŠµ ë°ì´í„°ê°€ ë‹¤ë¥´ë¯€ë¡œ** ê²°ê³¼ê°€ ë‹¬ë¼ì§

**ì‹¤í—˜ ì§ˆë¬¸**:
- "Pure NNì´ h=1 expert vs h=8 expertë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆë‚˜?"
- "ê°™ì€ ì•„í‚¤í…ì²˜ë¡œ ë‹¤ë¥¸ planning depth expertë¥¼ ëª¨ë°©í•  ìˆ˜ ìˆë‚˜?"

---

### ì‹œë‚˜ë¦¬ì˜¤ 2: ê°™ì€ Expert, ë‹¤ë¥¸ í•´ì„

**ê³ ê¸‰ ì‚¬ìš©ë²•** (ì´ë¡ ì )

```python
# Human expert data (depth ì•Œ ìˆ˜ ì—†ìŒ)
human_expert_trajs = load_expert_trajectories('opendata/raw_data.csv')

for h in [1, 2, 4, 8]:
    # ê°€ì„¤: "ì´ human expertê°€ depth hë¡œ í”Œë ˆì´í–ˆë‹¤"
    # Option A: ê°™ì€ ë°ì´í„°ë¡œ í•™ìŠµ, hëŠ” ë‹¨ì§€ ì‹¤í—˜ ID
    train_airl_pure_nn(
        h=h,  # Experiment ID
        expert_trajs=human_expert_trajs  # Same data!
    )
```

**hì˜ ì—­í• **:
- ê° hëŠ” **ë…ë¦½ì ì¸ ì‹¤í—˜**ì„ ì˜ë¯¸
- ê°™ì€ expertë¥¼ ëª¨ë°©í•˜ì§€ë§Œ, **ë‹¤ë¥¸ ëœë¤ seed**ë¡œ ì´ˆê¸°í™”
- ì—¬ëŸ¬ ë²ˆ í•™ìŠµí•´ì„œ **ë¶„ì‚°(variance)** ì¸¡ì •

**ì‹¤í—˜ ì§ˆë¬¸**:
- "ê°™ì€ expertë¥¼ ëª¨ë°©í•  ë•Œ ì´ˆê¸°í™”ì— ë”°ë¼ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ policyê°€ ë‚˜ì˜¤ë‚˜?"
- "Pure NNì˜ ì•ˆì •ì„±ì€?"

---

## Option A vs Option B ë¹„êµ (h ì‚¬ìš©)

### Option A: hëŠ” "ì–´ë–¤ expertë¥¼ ëª¨ë°©í• ì§€" ë ˆì´ë¸”

```python
# Option A êµ¬ì¡°
for h in [1, 2, 4, 8]:
    # hì— ë”°ë¼ ë‹¤ë¥¸ expert data
    expert_data = generate_BFS_data(h=h)  # â† h ì‚¬ìš©!

    # Generator: h ëª¨ë¦„
    gen = PPO("MlpPolicy", env)  # NO h

    # Reward: h ëª¨ë¦„
    reward = BasicRewardNet(...)  # NO h

    # í•™ìŠµ
    AIRL(expert_data, gen, reward)

    # ê²°ê³¼: hë³„ë¡œ ë‹¤ë¥¸ policy (ë‹¤ë¥¸ expert ë•Œë¬¸)
```

### Option B: hëŠ” "BFS ì´ˆê¸°í™” + ì–´ë–¤ expert" ë ˆì´ë¸”

```python
# Option B êµ¬ì¡°
for h in [1, 2, 4, 8]:
    # 1. hì— ë”°ë¼ BFS(h) ë°ì´í„° ìƒì„±
    bfs_data = generate_BFS_data(h=h)  # â† h ì‚¬ìš©!

    # 2. BCë¡œ BFS(h) ëª¨ë°©
    bc_policy = BC(bfs_data)  # â† hì˜ ì˜í–¥ ë°›ìŒ

    # 3. PPOë¡œ ë˜í•‘ (BC policy ìƒì†)
    gen = PPO.from_bc(bc_policy)  # â† ê°„ì ‘ì ìœ¼ë¡œ h ì˜í–¥

    # 4. Reward: h ëª¨ë¦„
    reward = BasicRewardNet(...)  # NO h

    # 5. Expert data (ë³´í†µ BFS(h)ì™€ ê°™ìŒ)
    expert_data = bfs_data

    # 6. AIRL fine-tuning
    AIRL(expert_data, gen, reward)

    # ê²°ê³¼: hë³„ë¡œ ë‹¤ë¥¸ policy (BFS ì´ˆê¸°í™” + ë‹¤ë¥¸ expert)
```

---

## êµ¬ì²´ì  ì˜ˆì‹œ

### ì˜ˆì‹œ 1: h=2 vs h=8 expert ëª¨ë°© (Option A)

```python
# h=2 experiment
expert_h2 = generate_BFS_trajectories(h=2, episodes=100)
# â†’ ì–•ì€ ê³„íš, ë‹¨ê¸°ì  ì´ìµ ì¶”êµ¬, ë¹ ë¥¸ ê²°ì •

gen_h2 = PPO("MlpPolicy", env)  # Random init
reward_h2 = BasicRewardNet(...)

train_airl(expert_h2, gen_h2, reward_h2)
# ê²°ê³¼: gen_h2ëŠ” ì–•ì€ ê³„íš í–‰ë™ í•™ìŠµ

# h=8 experiment
expert_h8 = generate_BFS_trajectories(h=8, episodes=100)
# â†’ ê¹Šì€ ê³„íš, ì¥ê¸°ì  ì „ëµ, ëŠë¦° ê²°ì •

gen_h8 = PPO("MlpPolicy", env)  # Random init (ë‹¤ë¥¸ seed)
reward_h8 = BasicRewardNet(...)  # Fresh instance

train_airl(expert_h8, gen_h8, reward_h8)
# ê²°ê³¼: gen_h8ëŠ” ê¹Šì€ ê³„íš í–‰ë™ í•™ìŠµ
```

**ì°¨ì´ì **:
- `gen_h2`ì™€ `gen_h8`ëŠ” **ê°™ì€ ì•„í‚¤í…ì²˜**
- í•˜ì§€ë§Œ **ë‹¤ë¥¸ expert data**ë¡œ í•™ìŠµ
- ë„¤íŠ¸ì›Œí¬ëŠ” hë¥¼ ëª¨ë¥´ì§€ë§Œ, **í–‰ë™ íŒ¨í„´ì´ ë‹¬ë¼ì§**

---

### ì˜ˆì‹œ 2: ì‹¤ì œ ì½”ë“œ íë¦„

```python
# fourinarow_airl/train_airl_pure_nn.pyì˜ ì‹¤ì œ ì‚¬ìš©

# ì‚¬ìš©ìê°€ h=4ë¥¼ ì„ íƒ
h = 4

# Step 1: Expert data (h=4 BFSë¡œ ìƒì„±)
from generate_training_data import generate_depth_limited_trajectories

expert_trajs = generate_depth_limited_trajectories(
    h=4,  # â† h ì‚¬ìš©! (expert ìƒì„±)
    num_episodes=100
)
# ì´ ë°ì´í„°ëŠ” "depth 4ë¡œ ê³„íší•œ í–‰ë™"

# Step 2: Generator (h ëª¨ë¦„!)
from create_ppo_generator_pure_nn import create_pure_ppo_generator

gen_algo, venv = create_pure_ppo_generator(
    env=env,
    h=4,  # â† Naming only! (ë„¤íŠ¸ì›Œí¬ëŠ” h ëª¨ë¦„)
    learning_rate=3e-4
)
# gen_algoëŠ” ëœë¤ ì´ˆê¸°í™”, h ì •ë³´ ì—†ìŒ

# Step 3: Reward (h ëª¨ë¦„!)
from create_reward_net import create_reward_network

reward_net = create_reward_network(env)  # NO h parameter!

# Step 4: AIRL í•™ìŠµ
train_airl(
    h=4,              # â† Metadata/logging
    expert_trajs,     # â† h=4 BFS data
    gen_algo,         # â† h ëª¨ë¦„
    reward_net        # â† h ëª¨ë¦„
)

# Step 5: ì €ì¥
gen_algo.save('models/airl_pure_nn_results/airl_pure_generator_h4.zip')
#                                                                  ^^
#                                                              hëŠ” íŒŒì¼ëª…ì—ë§Œ!
```

---

## hì˜ ì„¸ ê°€ì§€ ì˜ë¯¸ ì •ë¦¬

### 1. **Expert ìƒì„± ì‹œ ì‚¬ìš©** (Option A & B ê³µí†µ)

```python
# hëŠ” BFS ì•Œê³ ë¦¬ì¦˜ì˜ ì‹¤ì œ íŒŒë¼ë¯¸í„°
expert_data = generate_BFS_trajectories(h=4)
# BFSê°€ depth=4ê¹Œì§€ íƒìƒ‰ â†’ íŠ¹ì • í–‰ë™ íŒ¨í„´ ìƒì„±
```

### 2. **ì‹¤í—˜ ì¡°ì§í™”** (Option A & B ê³µí†µ)

```python
# hëŠ” ì‹¤í—˜ ë²„ì „ ê´€ë¦¬
experiments = {
    'h1': train_airl(h=1, expert_h1, ...),
    'h2': train_airl(h=2, expert_h2, ...),
    'h4': train_airl(h=4, expert_h4, ...),
    'h8': train_airl(h=8, expert_h8, ...),
}

# ë‚˜ì¤‘ì— ë¹„êµ
compare(experiments['h1'], experiments['h8'])
```

### 3. **íŒŒì¼ naming** (Option A & B ê³µí†µ)

```python
# hëŠ” íŒŒì¼ ì €ì¥/ë¡œë“œ ì‹œ êµ¬ë¶„ì
models/
â”œâ”€â”€ airl_pure_nn_results/
â”‚   â”œâ”€â”€ airl_pure_generator_h1.zip  # h=1 ì‹¤í—˜ ê²°ê³¼
â”‚   â”œâ”€â”€ airl_pure_generator_h2.zip  # h=2 ì‹¤í—˜ ê²°ê³¼
â”‚   â”œâ”€â”€ airl_pure_generator_h4.zip  # h=4 ì‹¤í—˜ ê²°ê³¼
â”‚   â””â”€â”€ airl_pure_generator_h8.zip  # h=8 ì‹¤í—˜ ê²°ê³¼
```

---

## ì™œ ê° hë§ˆë‹¤ ë”°ë¡œ í•™ìŠµí•˜ë‚˜?

### ì—°êµ¬ ì§ˆë¬¸

**"Planning depthê°€ í–‰ë™ì— ì–´ë–¤ ì˜í–¥ì„ ì£¼ëŠ”ê°€?"**

```python
# ê°€ì„¤
# h=1 expert: ê·¼ì‹œì•ˆì  (myopic) - ì¦‰ê°ì  ì´ë“ë§Œ ê³ ë ¤
# h=8 expert: ì „ëµì  (strategic) - ì¥ê¸°ì  ê²°ê³¼ ê³ ë ¤

# ì‹¤í—˜
# 1. ê° hë³„ë¡œ expert ìƒì„±
expert_h1 = BFS(h=1).play_games(100)
expert_h8 = BFS(h=8).play_games(100)

# 2. Pure NNì´ ê°ê° ëª¨ë°© ê°€ëŠ¥í•œê°€?
policy_h1 = train_airl_pure(expert_h1)  # NNì´ h=1 í–‰ë™ í•™ìŠµ
policy_h8 = train_airl_pure(expert_h8)  # NNì´ h=8 í–‰ë™ í•™ìŠµ

# 3. ê²°ê³¼ ë¹„êµ
# - policy_h1ê³¼ policy_h8ê°€ ë‹¤ë¥¸ í–‰ë™ì„ í•˜ë‚˜?
# - ì–´ë–¤ ì°¨ì´ê°€ ìˆë‚˜? (win rate, action distribution, etc.)
# - Pure NNì´ planning depthë¥¼ "ê°„ì ‘ì ìœ¼ë¡œ" í•™ìŠµí–ˆë‚˜?
```

### ì˜ˆìƒ ê²°ê³¼

```python
# Policy h1 í–‰ë™ íŒ¨í„´
policy_h1.play_game():
    # ì§§ì€ trajectory (ë¹¨ë¦¬ ëë‚¨)
    # ê³µê²©ì  (ì¦‰ê°ì  ìŠ¹ë¦¬ ì¶”êµ¬)
    # ìˆ˜ë¹„ ì•½í•¨ (ë¯¸ë˜ ìœ„í˜‘ ë¬´ì‹œ)

# Policy h8 í–‰ë™ íŒ¨í„´
policy_h8.play_game():
    # ê¸´ trajectory (ì‹ ì¤‘í•˜ê²Œ í”Œë ˆì´)
    # ì „ëµì  (í•¨ì • ì„¤ì¹˜)
    # ìˆ˜ë¹„ ê°•í•¨ (ì¥ê¸° ì „ëµ)
```

**í•µì‹¬ ì¸ì‚¬ì´íŠ¸**:
- ë„¤íŠ¸ì›Œí¬ëŠ” hë¥¼ ëª¨ë¦„
- í•˜ì§€ë§Œ **ë‹¤ë¥¸ expertì˜ í–‰ë™ íŒ¨í„´**ì„ í•™ìŠµ
- ê²°ê³¼ì ìœ¼ë¡œ **ê°„ì ‘ì ìœ¼ë¡œ planning depthë¥¼ í‘œí˜„**

---

## Option Aì—ì„œ hë¥¼ ì‚¬ìš©í•˜ëŠ” ì‹¤ì œ ì›Œí¬í”Œë¡œìš°

### ì›Œí¬í”Œë¡œìš° 1: ëª¨ë“  h ì‹¤í—˜

```bash
# h=1 ì‹¤í—˜
python3 fourinarow_airl/train_airl_pure_nn.py \
    --h 1 \
    --expert_data synthetic \  # BFS(h=1) ìƒì„±
    --total_timesteps 50000

# h=2 ì‹¤í—˜
python3 fourinarow_airl/train_airl_pure_nn.py \
    --h 2 \
    --expert_data synthetic \  # BFS(h=2) ìƒì„±
    --total_timesteps 50000

# h=4 ì‹¤í—˜
python3 fourinarow_airl/train_airl_pure_nn.py \
    --h 4 \
    --expert_data synthetic \  # BFS(h=4) ìƒì„±
    --total_timesteps 50000

# h=8 ì‹¤í—˜
python3 fourinarow_airl/train_airl_pure_nn.py \
    --h 8 \
    --expert_data synthetic \  # BFS(h=8) ìƒì„±
    --total_timesteps 50000
```

### ì›Œí¬í”Œë¡œìš° 2: Human expertì— ëŒ€í•œ ê°€ì„¤ ê²€ì¦

```bash
# ê°™ì€ human expert dataë¡œ ì—¬ëŸ¬ ì‹¤í—˜
# (ê° hëŠ” ë…ë¦½ì  ì‹¤í—˜ ID)

for h in 1 2 4 8; do
    python3 fourinarow_airl/train_airl_pure_nn.py \
        --h $h \
        --expert_data opendata/raw_data.csv \  # Same data!
        --total_timesteps 50000 \
        --seed $((42 + h))  # Different random seed
done
```

---

## í•µì‹¬ ìš”ì•½

### Option Aì—ì„œ depth hëŠ”:

1. âœ… **Expert ë°ì´í„° ìƒì„± ì‹œ ì‚¬ìš©**
   - `BFS(h=4)`ë¡œ trajectory ìƒì„±
   - ê° hë§ˆë‹¤ ë‹¤ë¥¸ í–‰ë™ íŒ¨í„´

2. âœ… **ì‹¤í—˜ ì¡°ì§í™”**
   - ê° hëŠ” ë…ë¦½ì ì¸ AIRL ì‹¤í—˜
   - ë‚˜ì¤‘ì— ë¹„êµ ë¶„ì„

3. âœ… **íŒŒì¼ naming**
   - `airl_pure_generator_h4.zip`
   - êµ¬ë¶„ ë° ì¶”ì  ìš©ì´

4. âŒ **ë„¤íŠ¸ì›Œí¬ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš© ì•ˆ í•¨**
   - Generator (MLP): NO h
   - Reward network: NO h
   - Observation: 89-dim (NO h)

### í•µì‹¬ ì›ì¹™

> **"hëŠ” ì–´ë–¤ expertë¥¼ ëª¨ë°©í• ì§€ ê²°ì •í•˜ëŠ” ë ˆì´ë¸”ì´ì§€, ë„¤íŠ¸ì›Œí¬ê°€ ë³´ëŠ” ì •ë³´ê°€ ì•„ë‹ˆë‹¤."**

```python
# ì´ë ‡ê²Œ ì´í•´í•˜ë©´ ë¨
Option A:
    Expert(h=4) â†’ [Pure NN learns] â†’ Policy(mimics h=4 behavior)
                   â†‘
                   hëŠ” ì—¬ê¸°ì—ë§Œ!
                   ë„¤íŠ¸ì›Œí¬ëŠ” h ëª¨ë¦„!
```

---

## ì‹¤í—˜ ì„¤ê³„ íŒ

### ì¢‹ì€ ì‹¤í—˜ ì„¤ê³„

```python
# ê° hë§ˆë‹¤ ì¶©ë¶„í•œ expert data
for h in [1, 2, 4, 8]:
    expert_data[h] = generate_BFS_trajectories(h=h, episodes=200)

    # Pure NN í•™ìŠµ
    policy[h] = train_airl_pure(expert_data[h], timesteps=100000)

# ë¹„êµ ë¶„ì„
for h1, h2 in [(1,2), (2,4), (4,8)]:
    compare_policies(policy[h1], policy[h2])
    # â†’ "h ì¦ê°€ì— ë”°ë¼ í–‰ë™ì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ê°€?"
```

### ë‚˜ìœ ì‹¤í—˜ ì„¤ê³„

```python
# âŒ ì˜ëª»ëœ ì˜ˆ: hë¥¼ ë„¤íŠ¸ì›Œí¬ì— ë„£ìœ¼ë ¤ê³  ì‹œë„
policy = train_airl_pure(
    expert_data,
    network_with_h_input=True  # WRONG!
)
# ì´ëŸ¬ë©´ rewardê°€ h-dependentê°€ ë˜ì–´ ì´ë¡ ì  ë¬¸ì œ ë°œìƒ
```

---

ì´ì œ ëª…í™•í•˜ì‹ ê°€ìš”? Option Aì—ì„œ hëŠ” **ì‹¤í—˜ ì„¤ê³„ ë³€ìˆ˜**ì´ì§€, **ë„¤íŠ¸ì›Œí¬ íŒŒë¼ë¯¸í„°ê°€ ì•„ë‹™ë‹ˆë‹¤**! ğŸ¯
