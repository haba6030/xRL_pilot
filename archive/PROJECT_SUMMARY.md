# Planning-Aware IRL/AIRL ì—°êµ¬ í”„ë¡œì íŠ¸ ìš”ì•½

**í”„ë¡œì íŠ¸ëª…**: Planning-Aware IRL/AIRL for Expertise, Clinical Traits, and Neural Links

**ì‹œì‘ì¼**: 2024-12-17
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2024-12-17

**íŒ€ì› ì˜¨ë³´ë”© ë° í”„ë¡œì íŠ¸ íŒ”ë¡œìš°ì—…ì„ ìœ„í•œ ì¢…í•© ë¬¸ì„œ**

---

## ğŸ“‹ ëª©ì°¨

1. [ì—°êµ¬ ëª©ì  ë° ë™ê¸°](#ì—°êµ¬-ëª©ì -ë°-ë™ê¸°)
2. [í•µì‹¬ ê°€ì • ë° ê°€ì„¤](#í•µì‹¬-ê°€ì •-ë°-ê°€ì„¤)
3. [ì—°êµ¬ ëŒ€ìƒ ë° ì§€í‘œ](#ì—°êµ¬-ëŒ€ìƒ-ë°-ì§€í‘œ)
4. [ì—°êµ¬ ë°©ë²•ë¡ ](#ì—°êµ¬-ë°©ë²•ë¡ )
5. [ì§„í–‰ í˜„í™©](#ì§„í–‰-í˜„í™©)
6. [ì£¼ìš” ë°œê²¬](#ì£¼ìš”-ë°œê²¬)
7. [ë‹¤ìŒ ë‹¨ê³„](#ë‹¤ìŒ-ë‹¨ê³„)
8. [ì°¸ê³  ìë£Œ](#ì°¸ê³ -ìë£Œ)

---

## ğŸ¯ ì—°êµ¬ ëª©ì  ë° ë™ê¸°

### ì—°êµ¬ ì§ˆë¬¸

**Q1**: Planning depthê°€ expertiseì™€ clinical traitsë¥¼ ì„¤ëª…í•˜ëŠ” ë…ë¦½ì ì¸ ìš”ì¸ì¸ê°€?

**Q2**: Planningì„ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ë©´ IRL/AIRLì˜ reward identifiabilityê°€ ê°œì„ ë˜ëŠ”ê°€?

**Q3**: Planning parametersê°€ neural mechanismsì™€ ì—°ê²°ë˜ëŠ”ê°€?

### ë™ê¸°

ê¸°ì¡´ IRL/AIRL ì—°êµ¬ëŠ” **reward functionë§Œ ì¶”ë¡ **í•˜ì§€ë§Œ, ì‹¤ì œ ì¸ê°„ í–‰ë™ì€:
- **Planning mechanisms**ì— ì˜í•´ í˜•ì„±ë¨ (ì–¼ë§ˆë‚˜ ê¹Šì´ íƒìƒ‰í•˜ëŠ”ê°€)
- **Individual differences**ê°€ í¼ (ì „ë¬¸ê°€ vs ì´ˆë³´ì)
- **Cognitive constraints**ë¥¼ ë°›ìŒ (ì‹œê°„, ì‘ì—… ê¸°ì–µ ë“±)

### ì´ë¡ ì  ë°°ê²½

1. **van Opheusden et al. (2023)**:
   - Expertise â†” deeper planning (ì „ë¬¸ê°€ëŠ” ë” ê¹Šê²Œ ê³„íš)
   - 4-in-a-row ê²Œì„ì—ì„œ ê²€ì¦

2. **Yao et al. (2024)**:
   - Planning horizonì€ latent confounder
   - ì´ë¥¼ ë¬´ì‹œí•˜ë©´ reward identifiability ê¹¨ì§

3. **Mhammedi et al. (2023)**:
   - Multi-step inverse ê´€ì 
   - Planningì„ explicit multi-step factorë¡œ ëª¨ë¸ë§

### ë³¸ ì—°êµ¬ì˜ ì°¨ë³„ì 

**ê¸°ì¡´**: Planning depthë¥¼ implicití•˜ê²Œ ê°€ì •
**ë³¸ ì—°êµ¬**: Planningì„ **explicit, inferable, manipulable mechanism**ìœ¼ë¡œ ë‹¤ë£¸

â†’ Expertise/clinical variability ì„¤ëª… + IRL interpretability ê°œì„ 

---

## ğŸ’¡ í•µì‹¬ ê°€ì • ë° ê°€ì„¤

### ê°€ì •

**A1. Planning as discrete parameter**
- Planning depth h âˆˆ {1, 2, 3, 4, 5}ë¡œ ì´ì‚°í™” ê°€ëŠ¥
- ê° ê°œì¸ì€ ì„ í˜¸í•˜ëŠ” planning depth ì¡´ì¬

**A2. Planning-reward separability**
- Reward functionê³¼ planning mechanismì€ ë…ë¦½ì 
- ë™ì¼í•œ rewardì— ëŒ€í•´ ë‹¤ë¥¸ planning depth ì‚¬ìš© ê°€ëŠ¥

**A3. Identifiability**
- Planning depth, inverse temperature Î², lapse rateëŠ” ì‹ë³„ ê°€ëŠ¥
- ì ì ˆí•œ regularizationê³¼ ë°ì´í„°ë¡œ ë¶„ë¦¬ ê°€ëŠ¥

### ê°€ì„¤

**H1. Expertise discrimination** (Primary)
- Planning depth hê°€ expertise (novice vs expert)ë¥¼ ìœ ì˜ë¯¸í•˜ê²Œ êµ¬ë³„
- ì˜ˆìƒ: Expert â†’ higher h

**H2. Incremental value over baseline**
- Planning-aware modelì´ baseline (parameter-only)ë³´ë‹¤ ì˜ˆì¸¡ë ¥ ë†’ìŒ
- ì¸¡ì •: AUC, log-likelihood improvement

**H3. Planning quality matters**
- Planningì˜ "ê¹Šì´"ë³´ë‹¤ "íš¨ìœ¨ì„±"ì´ ì¤‘ìš”
- Pruning thresholdì™€ depthì˜ ìƒí˜¸ì‘ìš©

**H4. Clinical relevance** (Exploratory)
- Anxiety/disorder severity â†” planning parameters
- Planning depthê°€ clinical traits ì„¤ëª…ì— ê¸°ì—¬

**H5. Neural correlates** (Exploratory)
- Planning parameters â†” fMRI activity (e.g., PFC, striatum)
- Trial-wise regressorsë¡œ neural signatures ë°œê²¬

---

## ğŸ“Š ì—°êµ¬ ëŒ€ìƒ ë° ì§€í‘œ

### ë°ì´í„°

**ì¶œì²˜**: van Opheusden et al. (2023) 4-in-a-row dataset

**ê·œëª¨**:
- **ì°¸ê°€ì**: 40ëª… (human-vs-human)
- **Trials**: 67,331 trials
- **ì‹¤í—˜ ì¡°ê±´**: learning, time pressure, eye tracking, generalization, fMRI

**Cross-validation**: 5-fold (ì°¸ê°€ìë³„)

### ì£¼ìš” ë³€ìˆ˜

#### ë…ë¦½ ë³€ìˆ˜ (Planning parameters)
- **h**: Planning depth (1-5 steps)
- **Î²**: Inverse temperature (choice stochasticity)
- **lapse**: Random choice probability
- **Pruning threshold**: Search tree pruning criterion
- **Feature drop rate**: Feature omission rate

#### ì¢…ì† ë³€ìˆ˜ (Behavioral outcomes)
- **Choice accuracy**: Log-likelihood of observed actions
- **Response time**: Decision latency
- **Win rate**: Game outcome (Elo rating)

#### ë¶„ë¥˜ ì§€í‘œ (Expertise)
- **Composite score**:
  ```
  z(log-likelihood) + z(pruning threshold) - z(lapse rate)
  ```
- **Binary label**: Median split â†’ Expert (1) vs Novice (0)

#### ì„±ëŠ¥ ì§€í‘œ
- **Model comparison**: AIC, BIC, log-likelihood
- **Discrimination**: AUC-ROC, accuracy, confusion matrix
- **Correlation**: Pearson r, Spearman Ï

### í‰ê°€ ê¸°ì¤€

**Baseline**: Parameters only (pruning, lapse, log-likelihood)
**Target**: + Planning depth h

**Success criteria**:
- Î”LL (log-likelihood increase) > 0.05 bits/trial
- Î”AUC > 0.05
- p < 0.05 for h coefficient in logistic regression

---

## ğŸ”¬ ì—°êµ¬ ë°©ë²•ë¡ 

### Phase 1: Behavioral Modeling (ì§„í–‰ ì¤‘)

#### Step 1.1: Data exploration âœ…
- Raw data: 67K trials, 40 participants
- Model fits: 22 variants (main, ablations, alternatives)
- Response time distributions

#### Step 1.2: Expertise classification âœ…
- Composite score from log-likelihood, pruning, lapse
- Binary label: 20 Expert, 20 Novice

#### Step 1.3: Planning depth analysis âš ï¸ (ë¶ˆí™•ì‹¤)
- `depth_by_session.txt` ì‚¬ìš© (30ëª… Ã— 5 sessions)
- **ë¬¸ì œ**: ì •í™•íˆ PV depthì¸ì§€ ë¶ˆëª…í™•
- **ë°œê²¬**: Expert < Novice (ì—­ì„¤ì  ê²°ê³¼, p=0.01)

#### Step 1.4: Discrimination test âœ…
- Logistic regression: parameters â†’ expertise
- **Baseline AUC**: 0.982 (ê±°ì˜ ì™„ë²½)
- **With PV depth**: 0.987 (ë¯¸ì„¸ ê°œì„ )

#### Step 1.5: Model comparison âœ…
- MCTS > No pruning > Main model > Fixed-depth
- **í•´ì„**: Stochastic stoppingì´ ì œí•œì ì¼ ìˆ˜ ìˆìŒ

### Phase 2: Planning-Aware Modeling (ì˜ˆì •)

#### Step 2.1: Fixed-h model implementation
```cpp
// Modify BFS to fix depth at h âˆˆ {1,2,3,4,5}
class heuristic_fixed_h : public heuristic {
    int fixed_depth;
    zet makemove_bfs_fixed_h(board, bool);
};
```

#### Step 2.2: Parameter fitting
- MATLAB BADS optimizer
- For each participant i, each h:
  - Optimize (Î², lapse) given h
  - Compute log-likelihood
  - Select best h via AIC/BIC

#### Step 2.3: Model selection
- Compare h=1,2,3,4,5 per participant
- Aggregate: optimal h distribution
- Test: h ~ expertise

### Phase 3: AIRL Extension (ì˜ˆì •)

#### Planning-aware AIRL algorithm
```python
for h in H:
    initialize reward_network r_Ï†
    initialize planner_policy Ï€_Î¸(s|h)  # constrained by h

    repeat:
        # Discriminator: real vs fake trajectories
        Ï† â† update_discriminator(expert_trajs, rollouts(Ï€_Î¸))

        # Generator: match expert under inferred reward
        Î¸ â† update_planner(Ï€_Î¸, reward=r_Ï†)

    score â† evaluate(r_Ï†, Ï€_Î¸)

return best (h, r_Ï†, Ï€_Î¸)
```

#### Evaluation
- Likelihood / imitation score
- OOD generalization (new boards)
- Turing test realism

### Phase 4: Clinical & Neural (íƒìƒ‰ì )

- **Clinical**: Anxiety â†’ planning parameters â†’ behavior
- **fMRI**: Trial-wise regressors (value, uncertainty, planning proxy)
- **Individual differences**: Parameters â†” ROI activity

---

## ğŸ“ˆ ì§„í–‰ í˜„í™©

### âœ… ì™„ë£Œëœ ì‘ì—…

#### 1. í™˜ê²½ ì„¤ì • (2024-12-17)
- [x] GitHub ì €ì¥ì†Œ clone (`xRL_pilot/`)
- [x] í´ë” êµ¬ì¡° íŒŒì•… (`FOLDER_STRUCTURE.md`)
- [x] CLAUDE.md ì—…ë°ì´íŠ¸ (ì½”ë“œë² ì´ìŠ¤ êµ¬ì¡° ì¶”ê°€)

#### 2. ë°ì´í„° ì¬ë¶„ì„ (2024-12-17)
- [x] `data_reanalysis.py`: ê¸°ë³¸ í†µê³„, íŒŒë¼ë¯¸í„° ë¶„í¬
  - 40ëª…, 67K trials
  - Expertise ë³µí•© ì§€í‘œ ìƒì„± (z-score ê¸°ë°˜)
  - ì‹œê°í™” 7ê°œ ìƒì„±

- [x] `model_comparison_analysis.py`: ëª¨ë¸ ë³€í˜• ë¹„êµ
  - 8ê°œ ëª¨ë¸ log-likelihood ë¹„êµ
  - MCTS (2.00) > Main (1.95) > Fixed-depth (1.94)
  - ì°¸ê°€ìë³„ ëª¨ë¸ ì„ í˜¸ë„ ë¶„ì„

- [x] `immediate_analysis.py`: ì¦‰ì‹œ ë¶„ì„
  - PV depth vs expertise: **Expert < Novice** (p=0.01) âš ï¸
  - Discrimination: AUC 0.982 (baseline), 0.987 (with depth)
  - RT correlations: depth â†‘ â†’ RT â†‘, LL â†“

#### 3. ê²€ì¦ ì‘ì—… (2024-12-17)
- [x] `verify_depth_variable.py`: depth ë³€ìˆ˜ ê²€ì¦
  - Raw vs -2 corrected: ë°©í–¥ ë™ì¼
  - depth_by_session.txtì˜ ì •ì²´ ë¶ˆëª…í™•
  - ìƒê´€ê´€ê³„ëŠ” ì´ë¡ ì ìœ¼ë¡œ íƒ€ë‹¹

### ğŸš§ ì§„í–‰ ì¤‘ì¸ ì‘ì—…

#### 1. Planning depth ì •ì²´ í™•ì¸
- [ ] `compute_planning_depth` ë°”ì´ë„ˆë¦¬ ì‹¤í–‰
- [ ] ì›ë³¸ Peak ë°ì´í„° ì°¾ê¸° (`splits/` ë””ë ‰í† ë¦¬)
- [ ] PV depth ì¬ê³„ì‚° ë° ë¹„êµ

#### 2. ì°¸ê°€ì ë§¤ì¹­
- [ ] opendata 40ëª… â†” depth 30ëª… ëŒ€ì‘ í™•ì¸
- [ ] Learning notebook 150ëª…ê³¼ì˜ ê´€ê³„ íŒŒì•…

### ğŸ“ ëŒ€ê¸° ì¤‘ì¸ ì‘ì—…

#### Phase 2: Fixed-h modeling
- [ ] C++ ì½”ë“œ ìˆ˜ì • (`heuristic_fixed_h` í´ë˜ìŠ¤)
- [ ] MATLAB í”¼íŒ… íŒŒì´í”„ë¼ì¸ ìˆ˜ì •
- [ ] ê° hë³„ log-likelihood ê³„ì‚°
- [ ] Optimal h distribution ë¶„ì„

#### Phase 3: AIRL
- [ ] Python wrapper í™˜ê²½ êµ¬ì¶• (`dm_env`)
- [ ] 4-in-a-row í™˜ê²½ ë˜í•‘
- [ ] AIRL discriminator êµ¬í˜„ (PyTorch)
- [ ] Planning-constrained policy êµ¬í˜„

#### Phase 4: Clinical/fMRI
- [ ] fMRI ë°ì´í„° í™•ì¸ ë° ì „ì²˜ë¦¬
- [ ] Trial-wise regressors ìƒì„±
- [ ] GLM analysis

---

## ğŸ” ì£¼ìš” ë°œê²¬

### 1. ì—­ì„¤ì  Planning Depth íŒ¨í„´ âš ï¸

**ì˜ˆìƒ**: Expert â†’ deeper planning (van Opheusden, 2023)

**ì‹¤ì œ**:
```
Expert:  PV depth = 6.23 Â± 1.30
Novice:  PV depth = 7.29 Â± 0.55
p = 0.011 (ìœ ì˜ë¯¸)
```

**í•´ì„**:
- âŒ Expertise â‰  simply "deeper planning"
- âœ… Expertise = **efficient planning** (ì ì€ depthë¡œ ì¢‹ì€ ê²°ê³¼)
- NoviceëŠ” ë¹„íš¨ìœ¨ì ìœ¼ë¡œ ê¹Šê²Œ íƒìƒ‰í•˜ì§€ë§Œ ì„±ëŠ¥ ë‚˜ì¨

**ì¦ê±°**:
- Depth â†‘ â†’ Log-likelihood â†“ (r = -0.50, p < 0.01)
- Depth â†‘ â†’ Response time â†‘ (r = +0.36, p < 0.05)
- Expert: ë†’ì€ pruning threshold (íš¨ìœ¨ì  ê°€ì§€ì¹˜ê¸°)

### 2. ê±°ì˜ ì™„ë²½í•œ Baseline Discrimination

**ê¸°ì¡´ íŒŒë¼ë¯¸í„°ë§Œìœ¼ë¡œë„ expertise êµ¬ë³„ ê±°ì˜ ì™„ë²½**:
- AUC = 0.982 (parameters only)
- AUC = 0.987 (+ PV depth)
- Î” = 0.005 (ë¯¸ë¯¸í•œ ê°œì„ )

**Feature importance**:
1. Log-likelihood (+1.76) â† ê°€ì¥ ì¤‘ìš”
2. Pruning threshold (+1.46)
3. PV depth (-0.59) â† ìŒìˆ˜! (ê¹Šì„ìˆ˜ë¡ Novice)
4. Lapse rate (-0.37)

**í•¨ì˜**:
- PV depth ì¶”ê°€í•´ë„ ì„±ëŠ¥ ê°œì„  ë¯¸ë¯¸
- **í•˜ì§€ë§Œ**: depthì˜ coefficient ë°©í–¥ì´ ì¤‘ìš”í•œ ì •ë³´ ì œê³µ
- Planning depthë¥¼ ë‹¨ë…ìœ¼ë¡œê°€ ì•„ë‹ˆë¼ **interaction term**ìœ¼ë¡œ ëª¨ë¸ë§ í•„ìš”

### 3. ëª¨ë¸ ë³€í˜• ë¹„êµ

**Log-likelihood ìˆœìœ„** (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ):
1. MCTS (2.00)
2. No pruning (2.00)
3. No feature drop (2.00)
4. Main model (1.95)
5. Fixed depth (1.94)

**í•´ì„**:
- Stochastic stopping (gamma parameter)ì´ ë„ˆë¬´ ì œí•œì 
- Feature droppingì´ ì˜¤íˆë ¤ ì„±ëŠ¥ ì €í•˜ ìœ ë°œ
- Fixed depthì˜ ì„±ëŠ¥ ì €í•˜: ìœ ì—°ì„± ë¶€ì¡±

### 4. Response Time íŒ¨í„´

**ì£¼ìš” ìƒê´€ê´€ê³„**:
- RT â†” Median RT: r = +0.78 (ë‹¹ì—°)
- RT â†” PV depth: r = +0.36 (ê¹Šê²Œ íƒìƒ‰ = ëŠë¦¼)
- RT â†” Log-likelihood: r = -0.19 (ì¢‹ì€ ì„±ëŠ¥ = ë¹ ë¦„)

**Planning depthì™€ ë‹¤ë¥¸ ë³€ìˆ˜**:
- Depth â†” Log-likelihood: r = -0.50 âš ï¸ (ê¹Šê²Œ íƒìƒ‰ = ë‚˜ìœ ì„±ëŠ¥)
- Depth â†” Center weight: r = +0.55
- Depth â†” RT: r = +0.36

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

### ì¦‰ì‹œ ì‹¤í–‰ (1-2ì£¼)

#### 1. Planning Depth ê²€ì¦ (ìµœìš°ì„ )
```bash
# C++ ë°”ì´ë„ˆë¦¬ ì‹¤í–‰
cd "xRL_pilot/Model code"
./compute_planning_depth --data ../data_hvh.txt --output pv_depth_test.txt

# Pythonìœ¼ë¡œ ë¹„êµ
python compare_depth_files.py
```

**ëª©í‘œ**: depth_by_session.txtê°€ ì •í™•íˆ ë¬´ì—‡ì¸ì§€ í™•ì¸

#### 2. Learning Trajectory ë¶„ì„
```python
# ë™ì¼ ì°¸ê°€ìì˜ ì´ˆê¸° vs í›„ê¸° ê²Œì„ ë¹„êµ
for participant in participants:
    early_games = games[0:20]
    late_games = games[80:100]

    compare_pv_depth(early, late)
    # ì˜ˆìƒ: U-shaped? ì´ˆê¸°â†‘ â†’ ì¤‘ê¸°â†‘ â†’ í›„ê¸°â†“ (íš¨ìœ¨í™”)
```

**ëª©í‘œ**: Experienceì— ë”°ë¥¸ planning depth ë³€í™” ì¶”ì 

#### 3. Pruning Efficiency Metric
```python
efficiency = log_likelihood / pv_depth
# "ì ì€ íƒìƒ‰ìœ¼ë¡œ ì¢‹ì€ ê²°ê³¼" = ë†’ì€ íš¨ìœ¨ì„±
```

**ê°€ì„¤**: Expertì˜ efficiency > Novice

### ë‹¨ê¸° ëª©í‘œ (1ê°œì›”)

#### 1. Fixed-h Model êµ¬í˜„
- [ ] `heuristic_fixed_h.cpp` ì‘ì„±
- [ ] Compile ë° í…ŒìŠ¤íŠ¸
- [ ] MATLAB wrapper ìˆ˜ì •

#### 2. Parameter Fitting
- [ ] ê° ì°¸ê°€ìë³„ h âˆˆ {1,2,3,4,5} í”¼íŒ…
- [ ] Optimal h distribution
- [ ] h ~ expertise ê²€ì •

#### 3. Interaction Term Modeling
```python
# Planning depth Ã— pruning quality
model = LogisticRegression()
X = pd.DataFrame({
    'h': planning_depth,
    'pruning': pruning_threshold,
    'h_x_pruning': planning_depth * pruning_threshold  # interaction
})
model.fit(X, expertise_label)
```

### ì¤‘ê¸° ëª©í‘œ (2-3ê°œì›”)

#### 1. Planning-Aware AIRL
- [ ] Python wrapper í™˜ê²½ êµ¬ì¶•
- [ ] AIRL baseline êµ¬í˜„
- [ ] Planning-constrained policy
- [ ] Toy problem ê²€ì¦

#### 2. Parameter Recovery Simulation
```python
# Ground truth hë¡œ synthetic data ìƒì„±
for h_true in [1,2,3,4,5]:
    simulate_trajectories(h=h_true, ...)
    h_recovered = fit_model(trajectories)
    recovery_rate = (h_recovered == h_true).mean()
```

**ëª©í‘œ**: Identifiability ê²€ì¦

### ì¥ê¸° ëª©í‘œ (6ê°œì›”+)

#### 1. Clinical Extension
- [ ] Clinical trait ë°ì´í„° ìˆ˜ì§‘ ì„¤ê³„
- [ ] Anxiety/disorder severity ì¸¡ì •
- [ ] Planning parameters â†” clinical traits

#### 2. fMRI Analysis
- [ ] Trial-wise regressors (value, uncertainty, planning)
- [ ] GLM analysis
- [ ] ROI-based correlations

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ë…¼ë¬¸

1. **van Opheusden, B., et al. (2023)**. "Expertise increases planning depth in human gameplay." *Nature*.
   - ìœ„ì¹˜: `papers/` (ì°¾ê¸°)
   - í•µì‹¬: Expertise â†” deeper planning, BFS with heuristic evaluation

2. **Yao, W., et al. (2024)**. "Planning horizon as a latent confounder in IRL."
   - ìœ„ì¹˜: `papers/Yao(2024)_IRLandPlanning.pdf`
   - í•µì‹¬: Planning horizon â†’ reward identifiability

3. **Mhammedi, Z., et al. (2023)**. "RL for multi-step inverse kinematics."
   - ìœ„ì¹˜: `papers/Mhammedi(2023)_RLmultiInvKinematics.pdf`
   - í•µì‹¬: Multi-step inverse perspective

### ì½”ë“œë² ì´ìŠ¤

- **GitHub**: https://github.com/haba6030/xRL_pilot
- **í•µì‹¬ íŒŒì¼**:
  - `Model code/bfs.cpp`: BFS + `get_depth_of_pv()`
  - `Model code/heuristic.cpp`: 17 feature weights
  - `Model code/matlab wrapper/fit_model.m`: BADS fitting
  - `Analysis notebooks/learning.ipynb`: PV depth ë¶„ì„

### ë‚´ë¶€ ë¬¸ì„œ

- **CLAUDE.md**: ì—°êµ¬ ê³„íš + ì½”ë“œ êµ¬ì¡°
- **FOLDER_STRUCTURE.md**: ì „ì²´ í´ë” êµ¬ì¡°
- **ì´ ë¬¸ì„œ**: í”„ë¡œì íŠ¸ ì¢…í•© ìš”ì•½

---

## ğŸ¤ íŒ€ì› ì˜¨ë³´ë”© ê°€ì´ë“œ

### ì‹ ê·œ íŒ€ì›ì„ ìœ„í•œ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### Day 1: í™˜ê²½ ì„¤ì •
- [ ] GitHub ì €ì¥ì†Œ clone
- [ ] Python í™˜ê²½ ì„¤ì • (`pandas`, `numpy`, `matplotlib`, `scikit-learn`, `scipy`)
- [ ] Jupyter notebook ì‹¤í–‰ í™•ì¸
- [ ] ë¬¸ì„œ ì½ê¸°: README.md â†’ FOLDER_STRUCTURE.md â†’ ì´ ë¬¸ì„œ

#### Day 2-3: ë°ì´í„° íƒìƒ‰
- [ ] `opendata/` íŒŒì¼ë“¤ í™•ì¸
- [ ] `data_reanalysis.py` ì‹¤í–‰ ë° ì¶œë ¥ í™•ì¸
- [ ] `immediate_analysis.py` ì‹¤í–‰ ë° ê²°ê³¼ í•´ì„
- [ ] ìƒì„±ëœ PNG íŒŒì¼ë“¤ í™•ì¸

#### Day 4-5: ì½”ë“œ ì´í•´
- [ ] `xRL_pilot/Model code/` C++ ì½”ë“œ í›‘ì–´ë³´ê¸°
- [ ] `bfs.cpp`ì˜ `get_depth_of_pv()` ì´í•´
- [ ] `heuristic.h` íŒŒë¼ë¯¸í„° êµ¬ì¡° íŒŒì•…
- [ ] `Analysis notebooks/learning.ipynb` ì‹¤í–‰ (ê°€ëŠ¥í•˜ë‹¤ë©´)

#### Week 2: ì‹¤ìŠµ
- [ ] ìƒˆë¡œìš´ ë¶„ì„ ì•„ì´ë””ì–´ êµ¬í˜„
- [ ] ê¸°ì¡´ ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì • ë° í™•ì¥
- [ ] ì²« íŒ€ ë¯¸íŒ…ì—ì„œ ì§ˆë¬¸ ë° ë…¼ì˜

### ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ (FAQ)

**Q1: depth_by_session.txtê°€ ì •í™•íˆ ë¬´ì—‡ì¸ê°€ìš”?**
- A: í˜„ì¬ ë¶ˆëª…í™•í•©ë‹ˆë‹¤. 30ëª… ì°¸ê°€ìì˜ planning depth ê´€ë ¨ ì§€í‘œì´ì§€ë§Œ, ì •í™•íˆ PV depthì¸ì§€ ë‹¤ë¥¸ metricì¸ì§€ í™•ì¸ ì¤‘ì…ë‹ˆë‹¤.

**Q2: ì™œ Expertê°€ Noviceë³´ë‹¤ planning depthê°€ ë‚®ë‚˜ìš”?**
- A: ì˜ˆìƒê³¼ ë°˜ëŒ€ë˜ëŠ” ê²°ê³¼ì…ë‹ˆë‹¤. í•´ì„ì€ "ExpertëŠ” íš¨ìœ¨ì ìœ¼ë¡œ ì–•ê²Œ íƒìƒ‰"ì…ë‹ˆë‹¤. Pruningì„ ì˜í•´ì„œ ë¶ˆí•„ìš”í•œ ê¹Šì€ íƒìƒ‰ì„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

**Q3: Baseline AUCê°€ ì´ë¯¸ 0.98ì¸ë° ê°œì„  ì—¬ì§€ê°€ ìˆë‚˜ìš”?**
- A: ì§ì ‘ì ì¸ discrimination ì„±ëŠ¥ ê°œì„ ì€ ì–´ë µì§€ë§Œ, planning depthì˜ **í•´ì„ ê°€ëŠ¥ì„±**ê³¼ **interaction effect** ë¶„ì„ì— ì˜ì˜ê°€ ìˆìŠµë‹ˆë‹¤. ë˜í•œ AIRLì—ì„œ reward identifiability ê°œì„ ì´ ëª©í‘œì…ë‹ˆë‹¤.

**Q4: C++ ì½”ë“œë¥¼ ìˆ˜ì •í•´ì•¼ í•˜ë‚˜ìš”?**
- A: Phase 2ì—ì„œ `heuristic_fixed_h` í´ë˜ìŠ¤ë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤. C++ ê²½í—˜ì´ ìˆë‹¤ë©´ ë„ì›€ì´ ë˜ì§€ë§Œ, ê¸°ì¡´ ì½”ë“œ íŒ¨í„´ì„ ë”°ë¼í•˜ë©´ ë©ë‹ˆë‹¤.

**Q5: MATLABì´ í•„ìš”í•œê°€ìš”?**
- A: Parameter fittingì— MATLAB + BADS optimizerê°€ ì‚¬ìš©ë˜ì§€ë§Œ, Pythonìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥í•©ë‹ˆë‹¤ (scipy.optimize, GPyOpt ë“±).

---

## ğŸ“§ ì—°ë½ì²˜ ë° í˜‘ì—…

**í”„ë¡œì íŠ¸ ë¦¬ë“œ**: [ì´ë¦„]
**GitHub**: https://github.com/haba6030/xRL_pilot
**ë¬¸ì„œ ì—…ë°ì´íŠ¸**: ì£¼ìš” ë°œê²¬ì´ë‚˜ ë°©ë²• ë³€ê²½ ì‹œ ì´ ë¬¸ì„œë¥¼ ì—…ë°ì´íŠ¸í•´ì£¼ì„¸ìš”.

**ë²„ì „ ê´€ë¦¬**:
- ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‚ ì§œë³„ë¡œ ë°±ì—… (`analysis_YYYYMMDD.py`)
- ì£¼ìš” ê²°ê³¼ëŠ” `results/` ë””ë ‰í† ë¦¬ì— ì €ì¥
- Git commit ë©”ì‹œì§€ì— ì§„í–‰ ìƒí™© ëª…ì‹œ

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2024-12-17
**ë‹¤ìŒ ë¦¬ë·° ì˜ˆì •**: Phase 2 ì‹œì‘ ì‹œ (Fixed-h model êµ¬í˜„ í›„)
