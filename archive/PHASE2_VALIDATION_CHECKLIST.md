# Phase 2 Implementation: Validation Checklist

## ëª©í‘œ

AIRL íŒŒì´í”„ë¼ì¸ êµ¬í˜„ ì‹œ **ì›ì¹™ ìœ„ë°˜** ë° **ì˜ë„í•˜ì§€ ì•Šì€ í•™ìŠµ**ì„ ë°©ì§€í•©ë‹ˆë‹¤.

---

## Critical Principle (ì ˆëŒ€ ìœ„ë°˜ ê¸ˆì§€)

> **Planning depth hëŠ” POLICYì—ë§Œ ì¡´ì¬. Reward networkëŠ” depth-agnostic.**

ì´ ì›ì¹™ì´ ì§€ì¼œì§€ì§€ ì•Šìœ¼ë©´:
- âŒ Reward-planning disentanglement ë¶•ê´´
- âŒ ì—°êµ¬ ì§ˆë¬¸ ë¬´ì˜ë¯¸í™”
- âŒ ì´ë¡ ì  íƒ€ë‹¹ì„± ìƒì‹¤

---

# Step 1: Trajectory Conversion

## ğŸ“‹ íŒŒì¼
`fourinarow_airl/airl_utils.py` - `convert_to_imitation_format()`

## âš ï¸ ì ì¬ì  ìœ„í—˜

### ìœ„í—˜ 1: ì •ë³´ ì†ì‹¤
**ë¬¸ì œ**: Trajectory ë³€í™˜ ê³¼ì •ì—ì„œ ì¤‘ìš” ì •ë³´ ì†ì‹¤
**ê²€ì¦**:
```python
# ë³€í™˜ ì „í›„ ë¹„êµ
assert np.array_equal(original.observations, converted.obs)
assert np.array_equal(original.actions, converted.acts)
```
**êµ¬í˜„**: âœ… `airl_utils.py:357-367`ì—ì„œ ê²€ì¦

### ìœ„í—˜ 2: Depth ì •ë³´ ìœ ì¶œ
**ë¬¸ì œ**: Observationì— depth ì •ë³´ê°€ í¬í•¨ë  ê°€ëŠ¥ì„±
**ê²€ì¦**:
```python
# Observationì€ 89-dimë§Œ í¬í•¨
# 0-35: Black pieces (board state)
# 36-71: White pieces (board state)
# 72-88: Van Opheusden features
# NO depth information!
```
**êµ¬í˜„**: âœ… `airl_utils.py:111-120`ì—ì„œ ëª…ì‹œì  í™•ì¸

### ìœ„í—˜ 3: Data type ë¶ˆì¼ì¹˜
**ë¬¸ì œ**: imitation libraryê°€ ìš”êµ¬í•˜ëŠ” dtypeê³¼ ë¶ˆì¼ì¹˜
**ê²€ì¦**:
```python
observations = observations.astype(np.float32)  # imitation requirement
actions = actions.astype(np.int64)
```
**êµ¬í˜„**: âœ… `airl_utils.py:63-70`

## âœ… Validation Checkpoints

- [x] **Checkpoint 1**: Observation shape (T+1, 89) ë³´ì¡´
- [x] **Checkpoint 2**: Action shape (T,) ë³´ì¡´
- [x] **Checkpoint 3**: ì •ë³´ ì†ì‹¤ ì—†ìŒ (array_equal ê²€ì¦)
- [x] **Checkpoint 4**: Terminal flag ì˜¬ë°”ë¦„
- [x] **Checkpoint 5**: Action range [0, 35] ê²€ì¦
- [x] **Checkpoint 6**: NO depth information in observations

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
cd fourinarow_airl
python3 airl_utils.py
```

**ì˜ˆìƒ ì¶œë ¥**:
```
Trajectory Conversion Validation Report
âœ“ Converted N trajectories
âœ“ Observation shape: (T+1, 89)
âœ“ NO planning depth h in observations
```

---

# Step 2: Reward Network Setup

## ğŸ“‹ íŒŒì¼
`test_reward_net.py` (ì‘ì„± ì˜ˆì •)

## âš ï¸ ì ì¬ì  ìœ„í—˜

### ìœ„í—˜ 1: Depth parameterê°€ reward networkì— ì¶”ê°€
**ë¬¸ì œ**: ì‹¤ìˆ˜ë¡œ hë¥¼ reward network inputì— í¬í•¨
**ê¸ˆì§€ íŒ¨í„´**:
```python
# âŒ WRONG
class RewardNet(nn.Module):
    def __init__(self, obs_dim, action_dim, h):  # â† h parameter!
        self.h_embedding = nn.Embedding(5, 8)

    def forward(self, state, action, next_state, h):  # â† h in forward!
        h_emb = self.h_embedding(h)
        x = torch.cat([state, action, next_state, h_emb])
        return self.mlp(x)
```

**ì˜¬ë°”ë¥¸ íŒ¨í„´**:
```python
# âœ… CORRECT
from imitation.rewards.reward_nets import BasicRewardNet

reward_net = BasicRewardNet(
    observation_space=env.observation_space,  # Box(89,)
    action_space=env.action_space,            # Discrete(36)
    hid_sizes=[64, 64]
)
# NO h parameter anywhere!
```

**êµ¬í˜„ ê²€ì¦**: âœ… `airl_utils.py:validate_airl_setup()`ì—ì„œ ìë™ ê²€ì‚¬

### ìœ„í—˜ 2: Observation dimension ë¶ˆì¼ì¹˜
**ë¬¸ì œ**: Reward networkê°€ 89-dimì„ ê¸°ëŒ€í•˜ì§€ ì•ŠìŒ
**ê²€ì¦**:
```python
# Test forward pass
obs = env.reset()  # (89,)
action = env.action_space.sample()
next_obs, _, _, _, _ = env.step(action)

reward = reward_net(obs, action, next_obs)
print(f"Reward: {reward}")  # Should work without error
```
**êµ¬í˜„**: âœ… `airl_utils.py:validate_reward_network_forward_pass()`

### ìœ„í—˜ 3: Imitation library ë²„ì „ ì°¨ì´
**ë¬¸ì œ**: Imitation library APIê°€ ë²„ì „ë§ˆë‹¤ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
**ê²€ì¦**:
```python
import imitation
print(f"Imitation version: {imitation.__version__}")
# Expected: >= 0.4.0
```

## âœ… Validation Checkpoints

- [ ] **Checkpoint 1**: Reward networkì— depth ê´€ë ¨ attribute ì—†ìŒ
- [ ] **Checkpoint 2**: Forward passì— h parameter ì—†ìŒ
- [ ] **Checkpoint 3**: 89-dim observation ì²˜ë¦¬ ê°€ëŠ¥
- [ ] **Checkpoint 4**: Discrete(36) action ì²˜ë¦¬ ê°€ëŠ¥
- [ ] **Checkpoint 5**: Output scalar reward

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì˜ˆì •)

```bash
python3 test_reward_net.py
```

---

# Step 3: Generator Policy Setup

## ğŸ“‹ íŒŒì¼
`fourinarow_airl/policy_wrapper.py` (ì‘ì„± ì˜ˆì •)

## âš ï¸ ì ì¬ì  ìœ„í—˜

### ìœ„í—˜ 1: Depthê°€ observationì— ì¶”ê°€ë¨
**ë¬¸ì œ**: PPO policyê°€ observationìœ¼ë¡œ depthë¥¼ ë°›ì„ ê°€ëŠ¥ì„±
**ê¸ˆì§€ íŒ¨í„´**:
```python
# âŒ WRONG: Augmented observation
obs_with_h = np.concatenate([obs, [h]])  # (90,) with depth!
action = policy(obs_with_h)
```

**ì˜¬ë°”ë¥¸ íŒ¨í„´**:
```python
# âœ… CORRECT: DepthëŠ” policy ë‚´ë¶€ì—ì„œë§Œ ì‚¬ìš©
class DepthLimitedPolicyWrapper:
    def __init__(self, h):
        self.h = h  # â† Depth stored internally
        self.depth_policy = DepthLimitedPolicy(h=h)

    def __call__(self, obs):
        # obsëŠ” 89-dim (depth ì •ë³´ ì—†ìŒ)
        # self.hë¥¼ ë‚´ë¶€ì ìœ¼ë¡œë§Œ ì‚¬ìš©
        action = self.depth_policy.select_action(obs, h=self.h)
        return action
```

### ìœ„í—˜ 2: BC (Behavior Cloning) ì‹œ depth ìœ ì¶œ
**ë¬¸ì œ**: BC training ì‹œ depthê°€ featureë¡œ ì¶”ê°€ë  ê°€ëŠ¥ì„±
**ê²€ì¦**:
```python
# BC demonstrationsëŠ” 89-dim observationë§Œ í¬í•¨
for traj in bc_trajectories:
    assert traj.obs.shape[1] == 89
    # NO depth column!
```

### ìœ„í—˜ 3: ì—¬ëŸ¬ hë¥¼ ë™ì‹œì— í•™ìŠµ
**ë¬¸ì œ**: ì‹¤ìˆ˜ë¡œ ì—¬ëŸ¬ hì˜ ë°ì´í„°ë¥¼ ì„ì–´ì„œ í•™ìŠµ
**ì›ì¹™**:
```python
# âœ… CORRECT: ê° hë§ˆë‹¤ ë³„ë„ í•™ìŠµ
for h in [1, 2, 4, 8]:
    # Create h-specific generator
    generator_h = create_generator(h=h)

    # Train AIRL with THIS h only
    trainer = AIRL(gen_algo=generator_h, ...)
    trainer.train()

    # Save separately
    save(f'generator_h{h}.pt')
```

## âœ… Validation Checkpoints

- [ ] **Checkpoint 1**: Policy observationì€ 89-dim (depth ì—†ìŒ)
- [ ] **Checkpoint 2**: DepthëŠ” policy ë‚´ë¶€ ë³€ìˆ˜ë¡œë§Œ ì¡´ì¬
- [ ] **Checkpoint 3**: BC trajectoriesì— depth ì •ë³´ ì—†ìŒ
- [ ] **Checkpoint 4**: ê° hë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµ
- [ ] **Checkpoint 5**: Generator outputsë§Œ h-dependent

---

# Step 4: AIRL Training

## ğŸ“‹ íŒŒì¼
`train_baseline_airl.py` (ì‘ì„± ì˜ˆì •)

## âš ï¸ ì ì¬ì  ìœ„í—˜

### ìœ„í—˜ 1: Discriminatorê°€ depthë¥¼ ê°„ì ‘ì ìœ¼ë¡œ í•™ìŠµ
**ë¬¸ì œ**: Different h generatorsê°€ identifiableí•œ patternì„ ë§Œë“¤ë©´, discriminatorê°€ ì´ë¥¼ í•™ìŠµí•  ê°€ëŠ¥ì„±
**ì˜ˆì‹œ**:
- h=1: ì§§ì€ trajectory
- h=8: ê¸´ trajectory
- Discriminatorê°€ "trajectory length"ë¡œ depthë¥¼ ì¶”ë¡ 

**ëŒ€ì‘**:
```python
# Trajectory length normalization ë˜ëŠ”
# Fixed-length padding ì‚¬ìš©
# í•˜ì§€ë§Œ ì´ëŠ” information lossë¥¼ ìœ ë°œí•  ìˆ˜ ìˆìŒ

# Better: Accept this as valid signal
# DiscriminatorëŠ” "behavior pattern"ì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ ëª©í‘œ
# Patternì´ depthì™€ correlateë˜ëŠ” ê²ƒì€ ìì—°ìŠ¤ëŸ¬ì›€
# ë‹¨, discriminator architecture ìì²´ì— hê°€ ì—†ì–´ì•¼ í•¨
```

**ì›ì¹™**: Discriminatorê°€ trajectory patternìœ¼ë¡œë¶€í„° depthë¥¼ **ì¶”ë¡ **í•˜ëŠ” ê²ƒì€ OK. í•˜ì§€ë§Œ **ëª…ì‹œì  depth input**ì€ ì ˆëŒ€ ì•ˆ ë¨.

### ìœ„í—˜ 2: Reward network ê°€ì¤‘ì¹˜ ê³µìœ 
**ë¬¸ì œ**: ì—¬ëŸ¬ h ì‹¤í—˜ ì‹œ ì‹¤ìˆ˜ë¡œ ê°™ì€ reward networkë¥¼ ì¬ì‚¬ìš©
**ê²€ì¦**:
```python
# âŒ WRONG: Reusing same reward_net
reward_net = BasicRewardNet()
for h in [1, 2, 4, 8]:
    trainer = AIRL(reward_net=reward_net, ...)  # Same object!
    trainer.train()

# âœ… CORRECT: Fresh reward_net for each h
for h in [1, 2, 4, 8]:
    reward_net = BasicRewardNet()  # New instance
    trainer = AIRL(reward_net=reward_net, ...)
    trainer.train()
```

### ìœ„í—˜ 3: Expert data contamination
**ë¬¸ì œ**: Expert dataê°€ h labelì„ í¬í•¨
**ê²€ì¦**:
```python
# Expert trajectories should NOT have depth labels
for traj in expert_trajectories:
    # Only (s, a, s') tuples
    # NO h information
    assert not hasattr(traj, 'depth')
    assert not hasattr(traj, 'h')
```

**êµ¬í˜„**: âœ… Expert dataëŠ” GameTrajectoryì—ì„œ ë³€í™˜ë˜ë©°, h ì •ë³´ ì—†ìŒ

### ìœ„í—˜ 4: Evaluation metric ì˜¤í•´
**ë¬¸ì œ**: Discriminator accuracyë¥¼ ì˜ëª» í•´ì„
**ì˜¬ë°”ë¥¸ í•´ì„**:
```python
# Discriminator accuracy ~ 0.5 = GOOD
# (Expertì™€ Generatedë¥¼ êµ¬ë¶„ ëª»í•¨ = Generatorê°€ ì˜ í•™ìŠµë¨)

# Discriminator accuracy >> 0.5 = BAD
# (Generatorê°€ Expertì™€ ë‹¤ë¦„)

# ìš°ë¦¬ì˜ ëª©í‘œ:
# hë³„ë¡œ í•™ìŠµ í›„, ì–´ë–¤ hê°€ ê°€ì¥ ë¹¨ë¦¬ acc ~ 0.5ì— ë„ë‹¬í•˜ëŠ”ê°€?
```

## âœ… Validation Checkpoints

- [ ] **Checkpoint 1**: Discriminatorì— depth input ì—†ìŒ (ì¬í™•ì¸)
- [ ] **Checkpoint 2**: ê° hë§ˆë‹¤ fresh reward network
- [ ] **Checkpoint 3**: Expert dataì— depth label ì—†ìŒ
- [ ] **Checkpoint 4**: Metrics ì˜¬ë°”ë¥´ê²Œ í•´ì„ (disc_acc ~ 0.5 ëª©í‘œ)
- [ ] **Checkpoint 5**: Training ì•ˆì •ì„± (loss divergence ì—†ìŒ)

---

# Step 5: Results Analysis

## ğŸ“‹ íŒŒì¼
`analyze_airl_results.py` (ì‘ì„± ì˜ˆì •)

## âš ï¸ ì ì¬ì  ìœ„í—˜

### ìœ„í—˜ 1: "h-specific reward" ìš©ì–´ ì‚¬ìš©
**ë¬¸ì œ**: ê²°ê³¼ ë¶„ì„ ì‹œ "h=4 reward"ì²˜ëŸ¼ í‘œí˜„
**ì˜¬ë°”ë¥¸ í‘œí˜„**:
```python
# âŒ WRONG terminology
"h=4 reward network"
"Reward for depth 4"

# âœ… CORRECT terminology
"Reward learned with h=4 generator"
"Reward trained using depth-4 policy"
```

### ìœ„í—˜ 2: Reward network ë¹„êµ ì‹œ h í˜¼ë™
**ë¬¸ì œ**: ì—¬ëŸ¬ hì˜ rewardë¥¼ ì§ì ‘ ë¹„êµ
**ì£¼ì˜**:
```python
# Reward networks are NOT directly comparable
# Each was learned with different generator

# What we compare:
# - Discrimination accuracy (which h â†’ best acc?)
# - Imitation quality (which h â†’ trajectories most similar to expert?)
# - Expertise prediction (which h â†’ best expert/novice classifier?)
```

### ìœ„í—˜ 3: Causal inference ì˜¤ë¥˜
**ë¬¸ì œ**: "h=8ì´ bestì´ë¯€ë¡œ expertëŠ” h=8ë¡œ planningí•œë‹¤"
**ì˜¬ë°”ë¥¸ í•´ì„**:
```python
# âœ… CORRECT interpretation:
# "Expert behavior is MOST CONSISTENT with h=8 planning assumption"
# NOT: "Expert uses h=8 algorithm"
```

## âœ… Validation Checkpoints

- [ ] **Checkpoint 1**: ìš©ì–´ ì‚¬ìš© ì •í™• ("trained with h=X")
- [ ] **Checkpoint 2**: Reward network ë¹„êµ ë°©ë²• íƒ€ë‹¹
- [ ] **Checkpoint 3**: Causal claim ì ì ˆ
- [ ] **Checkpoint 4**: ê²°ê³¼ í•´ì„ PLANNING_DEPTH_PRINCIPLES.md ì¤€ìˆ˜

---

# Overall Validation Protocol

## Pre-Implementation Checklist

ë§¤ ë‹¨ê³„ êµ¬í˜„ **ì „**:

- [ ] PLANNING_DEPTH_PRINCIPLES.md ì¬í™•ì¸
- [ ] í•´ë‹¹ ë‹¨ê³„ ìœ„í—˜ ìš”ì†Œ ê²€í† 
- [ ] Validation checkpoints ì¤€ë¹„

## Implementation Checklist

êµ¬í˜„ **ì¤‘**:

- [ ] Depthê°€ reward networkì— ë“¤ì–´ê°€ì§€ ì•ŠëŠ”ì§€ í™•ì¸
- [ ] Observationì´ 89-dimë§Œ ìœ ì§€í•˜ëŠ”ì§€ í™•ì¸
- [ ] ê° hê°€ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµë˜ëŠ”ì§€ í™•ì¸

## Post-Implementation Checklist

êµ¬í˜„ **í›„**:

- [ ] ëª¨ë“  validation checkpoints í†µê³¼
- [ ] Test ì½”ë“œ ì‹¤í–‰ ë° ê²€ì¦
- [ ] ê²°ê³¼ë¥¼ ë¬¸ì„œí™”í•  ë•Œ ìš©ì–´ ì •í™•íˆ ì‚¬ìš©

---

# Emergency: If Principle Violated

ë§Œì•½ êµ¬í˜„ ê³¼ì •ì—ì„œ ì›ì¹™ ìœ„ë°˜ì´ ë°œê²¬ë˜ë©´:

## 1. STOP immediately
êµ¬í˜„ì„ ë©ˆì¶”ê³  ì½”ë“œ ë¦¬ë·°

## 2. Identify the violation
ì–´ë””ì„œ depthê°€ rewardë¡œ ìœ ì¶œë˜ì—ˆëŠ”ê°€?

## 3. Fix at the source
í•´ë‹¹ ì½”ë“œë¥¼ ì™„ì „íˆ ì œê±° ë˜ëŠ” ìˆ˜ì •

## 4. Re-validate
ëª¨ë“  checkpoints ì¬í™•ì¸

## 5. Document
RESPONSE_TO_FEEDBACK.mdì— ê¸°ë¡

---

# Quick Reference: What Goes Where

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Planning Depth h                        â”‚
â”‚ Location: POLICY (Generator) ONLY      â”‚
â”‚                                         â”‚
â”‚ âœ“ DepthLimitedPolicy(h=h)              â”‚
â”‚ âœ“ BC training with h-specific data     â”‚
â”‚ âœ“ File names: generator_h4.pt          â”‚
â”‚                                         â”‚
â”‚ âœ— NOT in reward network                â”‚
â”‚ âœ— NOT in discriminator                 â”‚
â”‚ âœ— NOT in observations                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Reward Network                          â”‚
â”‚ Location: DISCRIMINATOR                â”‚
â”‚                                         â”‚
â”‚ âœ“ Input: (state, action, next_state)   â”‚
â”‚ âœ“ Output: scalar reward                â”‚
â”‚ âœ“ Same architecture for ALL h          â”‚
â”‚                                         â”‚
â”‚ âœ— NO h parameter                       â”‚
â”‚ âœ— NO h-related attributes              â”‚
â”‚ âœ— NO depth conditioning                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Observations (State)                    â”‚
â”‚ Dimension: 89                           â”‚
â”‚                                         â”‚
â”‚ âœ“ 0-35: Black pieces                   â”‚
â”‚ âœ“ 36-71: White pieces                  â”‚
â”‚ âœ“ 72-88: Van Opheusden features        â”‚
â”‚                                         â”‚
â”‚ âœ— NO depth information                 â”‚
â”‚ âœ— NO h label                           â”‚
â”‚ âœ— NO augmented features                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Document**: PHASE2_VALIDATION_CHECKLIST.md
**Purpose**: Prevent principle violations during Phase 2 implementation
**Status**: Active - Use for every implementation step
**Next**: Begin Step 1 with full validation
