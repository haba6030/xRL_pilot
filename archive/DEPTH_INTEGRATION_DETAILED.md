# Depth Integration ì „ëµ: ìƒì„¸ êµ¬í˜„ ê°€ì´ë“œ

## ëª©í‘œ

**Planning depth hë¥¼ POLICYì—ë§Œ í†µí•©í•˜ë©´ì„œ, AIRL discriminator(reward network)ëŠ” ì™„ì „íˆ depth-agnosticí•˜ê²Œ ìœ ì§€**

ì´ ë¬¸ì„œëŠ” GYMNASIUM_AND_AIRL_GUIDE.md 3.3ì ˆì˜ ìƒì„¸ êµ¬í˜„ ê°€ì´ë“œì…ë‹ˆë‹¤.

---

# ì „ì²´ íë¦„ë„

```
For each h âˆˆ {1, 2, 4, 8}:

[Step A] DepthLimitedPolicy(h) ì‚¬ìš©
    â†“ generates trajectories
[Step B] BC (Behavior Cloning): Neural net learns to mimic
    â†“ produces
[Step C] Neural policy that behaves like depth-h
    â†“ wrapped by
[Step D] PPO (for AIRL fine-tuning)
    â†“ combined with
[Step E] Depth-AGNOSTIC reward network
    â†“
[Step F] AIRL training
    â†“
[Step G] Evaluate & Compare h values
```

---

# Step A: Generate h-specific Training Data

## ëª©ì 

DepthLimitedPolicy(h)ë¥¼ ì‚¬ìš©í•´ trajectories ìƒì„± â†’ BC í•™ìŠµìš© ë°ì´í„°

## ì½”ë“œ

```python
from fourinarow_airl import FourInARowEnv
from fourinarow_airl.depth_limited_policy import DepthLimitedPolicy
from fourinarow_airl.bfs_wrapper import load_all_participant_parameters
import numpy as np

def generate_depth_limited_trajectories(
    h: int,
    num_episodes: int = 100,
    seed: int = 42
):
    """
    Generate trajectories using DepthLimitedPolicy(h)

    Args:
        h: Planning depth
        num_episodes: Number of episodes to generate
        seed: Random seed

    Returns:
        trajectories: List of (observations, actions) tuples
    """
    # Environment
    env = FourInARowEnv()

    # Load expert parameters for heuristic weights
    params_dict = load_all_participant_parameters(
        'opendata/model_fits_main_model.csv'
    )
    expert_params = params_dict[1]  # Use participant 1

    # Create depth-limited policy
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CRITICAL: hëŠ” ì—¬ê¸°ì„œë§Œ ì‚¬ìš©ë¨ (policy internal)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    policy = DepthLimitedPolicy(
        h=h,                                    # â† h is HERE
        params=expert_params,
        beta=1.0,
        lapse_rate=expert_params.lapse_rate
    )

    trajectories = []
    rng = np.random.default_rng(seed + h)  # h-specific seed

    print(f"Generating {num_episodes} episodes with h={h}...")

    for episode in range(num_episodes):
        obs, _ = env.reset(seed=seed + h + episode * 1000)

        episode_obs = [obs.copy()]  # (T+1,)
        episode_acts = []           # (T,)

        done = False
        step_count = 0
        max_steps = 36  # Board size

        while not done and step_count < max_steps:
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # VALIDATION CHECKPOINT 1:
            # obsëŠ” 89-dim (board + features), NO h information
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            assert obs.shape == (89,), f"Observation should be 89-dim, got {obs.shape}"

            # Select action using h-step planning
            action, planning_result = policy.select_action(env, rng)

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # VALIDATION CHECKPOINT 2:
            # actionì€ 0-35 ë²”ìœ„
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            assert 0 <= action <= 35, f"Action out of range: {action}"

            # Execute action
            obs, reward, terminated, truncated, info = env.step(action)

            episode_obs.append(obs.copy())
            episode_acts.append(action)

            done = terminated or truncated
            step_count += 1

        # Store trajectory
        trajectory = {
            'observations': np.array(episode_obs, dtype=np.float32),  # (T+1, 89)
            'actions': np.array(episode_acts, dtype=np.int64),        # (T,)
            'length': len(episode_acts),
            'h': h  # â† Metadata only (NOT used in training!)
        }

        trajectories.append(trajectory)

    avg_length = np.mean([t['length'] for t in trajectories])
    print(f"Generated {len(trajectories)} trajectories")
    print(f"Average length: {avg_length:.1f}")
    print(f"Nodes expanded (approx): {int(avg_length * planning_result.nodes_expanded)}")

    return trajectories
```

## ğŸš¨ ìœ„í—˜ ìš”ì†Œ

### ìœ„í—˜ A1: 'h' metadataê°€ trainingì— ì‚¬ìš©ë¨
**ëŒ€ì‘**:
```python
# 'h'ëŠ” debugging/loggingìš©ìœ¼ë¡œë§Œ ì €ì¥
# BC training ì‹œ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ

# âŒ WRONG
obs_with_h = np.concatenate([obs, [trajectory['h']]])

# âœ… CORRECT
obs = trajectory['observations']  # h ì •ë³´ ì œì™¸
```

### ìœ„í—˜ A2: ê°™ì€ seedë¡œ ì—¬ëŸ¬ h ìƒì„±
**ëŒ€ì‘**:
```python
# ê° hë§ˆë‹¤ ë‹¤ë¥¸ seed ì‚¬ìš©
rng = np.random.default_rng(seed + h)  # h-dependent seed
```

---

# Step B: Behavior Cloning (BC)

## ëª©ì 

Neural networkê°€ DepthLimitedPolicy(h)ì˜ behaviorë¥¼ ëª¨ë°©í•˜ë„ë¡ í•™ìŠµ

## í•µì‹¬ ì›ì¹™

> **BCëŠ” (state â†’ action) mappingë§Œ í•™ìŠµ. hëŠ” trainingì— ì‚¬ìš©ë˜ì§€ ì•ŠìŒ.**

## ì½”ë“œ

```python
from imitation.algorithms import bc
from imitation.data import types as il_types
import torch
import torch.nn as nn

def train_bc_policy(
    trajectories: List[dict],
    env,
    h: int,  # For logging only!
    n_epochs: int = 50,
    batch_size: int = 64,
    learning_rate: float = 3e-4
):
    """
    Train BC policy to mimic DepthLimitedPolicy(h)

    Args:
        trajectories: Generated from DepthLimitedPolicy(h)
        env: FourInARowEnv
        h: Planning depth (for logging/saving ONLY, NOT training!)
        n_epochs: Training epochs
        batch_size: Batch size
        learning_rate: Learning rate

    Returns:
        bc_trainer: Trained BC object
    """
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # VALIDATION CHECKPOINT 3:
    # Convert to imitation format WITHOUT using h
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    imitation_trajectories = []

    for traj in trajectories:
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # CRITICAL: Only use observations and actions
        # DO NOT use traj['h'] anywhere!
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        obs = traj['observations']   # (T+1, 89)
        acts = traj['actions']       # (T,)

        # Verify dimensions
        assert obs.shape[1] == 89, f"Expected 89-dim obs, got {obs.shape[1]}"
        assert acts.min() >= 0 and acts.max() <= 35, f"Actions out of range"

        imitation_traj = il_types.Trajectory(
            obs=obs,
            acts=acts,
            infos=None,
            terminal=True
        )
        imitation_trajectories.append(imitation_traj)

    print(f"\n[BC Training for h={h}]")
    print(f"Trajectories: {len(imitation_trajectories)}")
    print(f"Total transitions: {sum(len(t.acts) for t in imitation_trajectories)}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # VALIDATION CHECKPOINT 4:
    # BC policy architecture has NO h parameter
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # Create BC trainer
    bc_trainer = bc.BC(
        observation_space=env.observation_space,  # Box(89,)
        action_space=env.action_space,            # Discrete(36)
        demonstrations=imitation_trajectories,
        batch_size=batch_size,
        # Policy network configuration
        policy_kwargs=dict(
            net_arch=[64, 64],  # MLP architecture
            activation_fn=nn.Tanh,
        ),
    )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # VALIDATION: Verify policy has no h-related attributes
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    policy = bc_trainer.policy
    suspicious_attrs = [attr for attr in dir(policy) if 'depth' in attr.lower() or attr == 'h']

    if len(suspicious_attrs) > 0:
        print(f"âš ï¸  WARNING: Found suspicious attributes: {suspicious_attrs}")
    else:
        print(f"âœ“ Policy has no depth-related attributes")

    # Train
    print(f"Training for {n_epochs} epochs...")
    bc_trainer.train(n_epochs=n_epochs)

    print(f"âœ“ BC training complete")

    return bc_trainer
```

## ğŸš¨ ìœ„í—˜ ìš”ì†Œ

### ìœ„í—˜ B1: Policy networkì— hë¥¼ inputìœ¼ë¡œ ì¶”ê°€
**ì˜ëª»ëœ ì˜ˆ**:
```python
# âŒ ABSOLUTELY WRONG
class PolicyWithDepth(nn.Module):
    def forward(self, obs, h):  # â† h in forward!
        x = torch.cat([obs, torch.tensor([h])])
        return self.mlp(x)
```

**ì˜¬ë°”ë¥¸ ì˜ˆ**:
```python
# âœ… CORRECT
# BCëŠ” ê¸°ë³¸ policyë¥¼ ì‚¬ìš© (observation â†’ action)
# hëŠ” ì „í˜€ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ
bc_trainer = bc.BC(
    observation_space=env.observation_space,
    action_space=env.action_space,
    demonstrations=trajectories
)
# NO h parameter anywhere!
```

### ìœ„í—˜ B2: Demonstrationsì— h label ì¶”ê°€
**ëŒ€ì‘**:
```python
# TrajectoryëŠ” (obs, acts)ë§Œ í¬í•¨
# hëŠ” metadataë¡œë§Œ ì‚¬ìš© (trainingì— ì‚¬ìš© ì•ˆ ë¨)

for traj in trajectories:
    # âœ“ Use these
    obs = traj['observations']
    acts = traj['actions']

    # âœ— DO NOT use
    # h = traj['h']  # Ignore this!
```

---

# Step C: Wrap BC Policy with PPO

## ëª©ì 

BCë¡œ í•™ìŠµí•œ policyë¥¼ PPOë¡œ ê°ì‹¸ì„œ AIRLì—ì„œ fine-tuning ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦

## ì½”ë“œ

```python
from stable_baselines3 import PPO

def create_ppo_from_bc(
    bc_trainer,
    env,
    h: int,  # For logging/saving only!
    learning_rate: float = 3e-4,
):
    """
    Wrap BC policy with PPO for AIRL training

    Args:
        bc_trainer: Trained BC object
        env: FourInARowEnv
        h: Planning depth (metadata only)
        learning_rate: PPO learning rate

    Returns:
        ppo_algo: PPO algorithm with BC-initialized policy
    """
    print(f"\n[Creating PPO from BC policy (h={h})]")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # VALIDATION CHECKPOINT 5:
    # PPO uses BC policy, which is depth-agnostic
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # Extract BC policy
    bc_policy = bc_trainer.policy

    # Verify policy input dimension
    print(f"BC Policy observation space: {env.observation_space}")
    print(f"BC Policy action space: {env.action_space}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CRITICAL: PPO receives depth-agnostic policy
    # hëŠ” ì—¬ê¸°ì„œë„ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # Create PPO with BC policy
    ppo_algo = PPO(
        policy=bc_policy,           # â† BC-learned policy (no h!)
        env=env,
        learning_rate=learning_rate,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        verbose=0,
    )

    print(f"âœ“ PPO created with BC-initialized policy")
    print(f"âœ“ Policy is depth-agnostic (only sees 89-dim observations)")

    return ppo_algo
```

## ğŸš¨ ìœ„í—˜ ìš”ì†Œ

### ìœ„í—˜ C1: PPO observation augmentation
**ë¬¸ì œ**: PPOê°€ ìë™ìœ¼ë¡œ observationì„ augmentí•  ê°€ëŠ¥ì„±
**ê²€ì¦**:
```python
# PPO policyì˜ observation dimension í™•ì¸
assert ppo_algo.policy.observation_space.shape == (89,)
```

---

# Step D: Create Depth-AGNOSTIC Reward Network

## ëª©ì 

ëª¨ë“  hì— ëŒ€í•´ **ë™ì¼í•œ architecture**ì˜ reward network ìƒì„±

## ì½”ë“œ

```python
from imitation.rewards.reward_nets import BasicRewardNet

def create_reward_network(env):
    """
    Create depth-agnostic reward network

    CRITICAL: This function has NO h parameter!
    Same architecture for ALL h values.

    Args:
        env: FourInARowEnv

    Returns:
        reward_net: BasicRewardNet (depth-agnostic)
    """
    print(f"\n[Creating Depth-AGNOSTIC Reward Network]")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # VALIDATION CHECKPOINT 6:
    # NO h parameter in this function
    # NO h parameter in reward network
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    reward_net = BasicRewardNet(
        observation_space=env.observation_space,  # Box(89,)
        action_space=env.action_space,            # Discrete(36)
        hid_sizes=[64, 64],                       # MLP hidden layers
        activation=nn.Tanh,
    )

    print(f"Reward Network Architecture:")
    print(f"  Input: (state, action, next_state)")
    print(f"  State dim: 89 (board + features, NO depth)")
    print(f"  Action dim: 36 (one-hot encoded)")
    print(f"  Hidden: [64, 64]")
    print(f"  Output: scalar reward")
    print(f"âœ“ NO h parameter")
    print(f"âœ“ Same architecture for ALL h values")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # VALIDATION: Check for depth-related attributes
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    suspicious_attrs = [
        attr for attr in dir(reward_net)
        if 'depth' in attr.lower() or attr == 'h'
    ]

    if len(suspicious_attrs) > 0:
        raise ValueError(
            f"Reward network has depth-related attributes: {suspicious_attrs}\n"
            f"This violates PLANNING_DEPTH_PRINCIPLES.md!"
        )

    print(f"âœ“ Validation passed: No depth-related attributes")

    return reward_net
```

## ğŸš¨ ìœ„í—˜ ìš”ì†Œ

### ìœ„í—˜ D1: ì‹¤ìˆ˜ë¡œ hë¥¼ parameterë¡œ ì „ë‹¬
**ì ˆëŒ€ ê¸ˆì§€**:
```python
# âŒ ABSOLUTELY FORBIDDEN
def create_reward_network(env, h):  # â† NO h parameter!
    reward_net = BasicRewardNet(..., depth=h)
```

**ì˜¬ë°”ë¦„**:
```python
# âœ… CORRECT
def create_reward_network(env):  # No h!
    reward_net = BasicRewardNet(...)  # No h!
```

### ìœ„í—˜ D2: ì—¬ëŸ¬ hì— ëŒ€í•´ reward network ì¬ì‚¬ìš©
**ë¬¸ì œ**: ê°™ì€ reward_net instanceë¥¼ ì—¬ëŸ¬ h trainingì— ì‚¬ìš©
**ì˜¬ë°”ë¥¸ ë°©ë²•**:
```python
for h in [1, 2, 4, 8]:
    # Fresh reward network for each h
    reward_net = create_reward_network(env)  # New instance!
    # Train with h-specific generator
```

---

# Step E: AIRL Training

## ëª©ì 

h-specific generator + depth-agnostic discriminatorë¡œ AIRL í•™ìŠµ

## ì½”ë“œ

```python
from imitation.algorithms.adversarial import airl

def train_airl_for_depth(
    h: int,
    expert_trajectories: List,
    env,
    total_timesteps: int = 100000,
    n_disc_updates_per_round: int = 4,
):
    """
    Train AIRL for specific planning depth h

    Args:
        h: Planning depth (only affects generator!)
        expert_trajectories: Expert demonstrations (depth-agnostic)
        env: Environment
        total_timesteps: Training timesteps
        n_disc_updates_per_round: Discriminator updates

    Returns:
        trainer: Trained AIRL object
        results: Training metrics
    """
    print("=" * 80)
    print(f"AIRL Training for h={h}")
    print("=" * 80)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Step E1: Generate h-specific training data
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\n[Step 1] Generate trajectories with h={h}")
    depth_trajectories = generate_depth_limited_trajectories(
        h=h,
        num_episodes=100
    )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Step E2: BC training (h â†’ neural policy)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\n[Step 2] BC training (mimic h={h} policy)")
    bc_trainer = train_bc_policy(
        trajectories=depth_trajectories,
        env=env,
        h=h,  # Metadata only!
        n_epochs=50
    )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Step E3: Wrap with PPO
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\n[Step 3] Create PPO generator")
    gen_algo = create_ppo_from_bc(
        bc_trainer=bc_trainer,
        env=env,
        h=h  # Metadata only!
    )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Step E4: Create depth-AGNOSTIC reward network
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\n[Step 4] Create depth-AGNOSTIC reward network")
    reward_net = create_reward_network(env)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # VALIDATION CHECKPOINT 7:
    # Final check before AIRL training
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\n[Validation] Pre-training checks:")
    print(f"âœ“ Generator: Learned from h={h} policy")
    print(f"âœ“ Discriminator: NO h parameter")
    print(f"âœ“ Expert data: NO h labels")
    print(f"âœ“ Observations: 89-dim (board + features, no depth)")

    # Verify expert data
    for i, traj in enumerate(expert_trajectories[:3]):
        assert traj.obs.shape[1] == 89, \
            f"Expert traj {i}: Expected 89-dim, got {traj.obs.shape[1]}"
    print(f"âœ“ Expert trajectories validated")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Step E5: Create AIRL trainer
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\n[Step 5] Create AIRL trainer")

    trainer = airl.AIRL(
        demonstrations=expert_trajectories,      # Expert data
        demo_batch_size=256,

        venv=env,                                # Environment
        gen_algo=gen_algo,                       # h-dependent generator
        reward_net=reward_net,                   # h-AGNOSTIC discriminator!

        n_disc_updates_per_round=n_disc_updates_per_round,
        demo_minibatch_size=64,
    )

    print(f"âœ“ AIRL trainer created")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Step E6: Train
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\n[Step 6] AIRL training ({total_timesteps} timesteps)")

    trainer.train(total_timesteps=total_timesteps)

    print(f"âœ“ AIRL training complete")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Step E7: Extract results
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    results = {
        'h': h,
        'reward_net': reward_net,
        'generator': gen_algo,
        'trainer': trainer,
    }

    return trainer, results
```

## ğŸš¨ ìœ„í—˜ ìš”ì†Œ

### ìœ„í—˜ E1: Expert trajectoriesì— h label ì¶”ê°€
**ê²€ì¦**:
```python
# Expert dataëŠ” h ì •ë³´ê°€ ì „í˜€ ì—†ì–´ì•¼ í•¨
for traj in expert_trajectories:
    assert not hasattr(traj, 'depth')
    assert not hasattr(traj, 'h')
    assert traj.obs.shape[1] == 89  # No augmented features
```

### ìœ„í—˜ E2: Discriminatorê°€ ê°„ì ‘ì ìœ¼ë¡œ h í•™ìŠµ
**ì§ˆë¬¸**: "Discriminatorê°€ trajectory patternìœ¼ë¡œ hë¥¼ ì¶”ë¡ í•˜ë©´?"

**ë‹µë³€**: **ì´ê²ƒì€ acceptableí•¨!**

```python
# Pattern recognition is OK:
# - h=1 generator â†’ short trajectories, shallow planning
# - h=8 generator â†’ longer trajectories, deeper planning
# Discriminatorê°€ ì´ patternì„ í•™ìŠµí•˜ëŠ” ê²ƒì€ ìì—°ìŠ¤ëŸ¬ì›€

# What's NOT OK:
# - Discriminatorì— ëª…ì‹œì  h input
# - Observationì— h ì •ë³´ í¬í•¨
```

**ì´ìœ **: Discriminatorì˜ ëª©í‘œëŠ” "expert behavior patternì„ í•™ìŠµ"í•˜ëŠ” ê²ƒ. ë§Œì•½ expertê°€ ì‹¤ì œë¡œ h=8ë¡œ planningí•œë‹¤ë©´, discriminatorëŠ” "h=8 pattern"ì„ í•™ìŠµí•´ì•¼ í•¨.

---

# Step F: Multi-Depth Comparison

## ëª©ì 

ì—¬ëŸ¬ h ê°’ì— ëŒ€í•´ í•™ìŠµ í›„ ë¹„êµ

## ì½”ë“œ

```python
def train_all_depths(
    depths: List[int],
    expert_trajectories: List,
    env,
    total_timesteps: int = 100000,
):
    """
    Train AIRL for multiple depths and compare

    Args:
        depths: List of planning depths (e.g., [1, 2, 4, 8])
        expert_trajectories: Expert demonstrations
        env: Environment
        total_timesteps: Timesteps per depth

    Returns:
        all_results: Dict mapping h â†’ results
    """
    all_results = {}

    for h in depths:
        print(f"\n{'=' * 80}")
        print(f"Training depth h={h}")
        print(f"{'=' * 80}")

        trainer, results = train_airl_for_depth(
            h=h,
            expert_trajectories=expert_trajectories,
            env=env,
            total_timesteps=total_timesteps
        )

        all_results[h] = results

        # Save models
        import torch
        save_dir = f'models/h{h}'
        os.makedirs(save_dir, exist_ok=True)

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # NAMING CONVENTION (CRITICAL):
        # NOT "h4_reward.pt" (implies h-specific reward)
        # BUT "reward_trained_with_h4_generator.pt"
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        torch.save(
            results['reward_net'].state_dict(),
            f'{save_dir}/reward_trained_with_h{h}_generator.pt'
        )

        torch.save(
            results['generator'].policy.state_dict(),
            f'{save_dir}/generator_h{h}.pt'
        )

        print(f"âœ“ Saved models for h={h}")

    return all_results
```

---

# Step G: Evaluation & Comparison

## í‰ê°€ ì§€í‘œ

### 1. Discrimination Accuracy

```python
def evaluate_discrimination(trainer, expert_trajectories):
    """
    Measure how well discriminator distinguishes expert vs generated

    Target: ~0.5 (means generator matches expert well)
    """
    # Expert accuracy
    expert_logits = []
    for traj in expert_trajectories:
        for t in range(len(traj.acts)):
            logit = trainer.reward_net(
                traj.obs[t],
                traj.acts[t],
                traj.obs[t+1]
            )
            expert_logits.append(logit)

    expert_acc = (torch.sigmoid(expert_logits) > 0.5).float().mean()

    # Generated accuracy
    gen_trajectories = generate_from_policy(trainer.gen_algo, num=100)
    gen_logits = []
    for traj in gen_trajectories:
        for t in range(len(traj.acts)):
            logit = trainer.reward_net(...)
            gen_logits.append(logit)

    gen_acc = (torch.sigmoid(gen_logits) < 0.5).float().mean()

    # Overall accuracy
    disc_acc = (expert_acc + gen_acc) / 2

    return {
        'disc_acc': disc_acc,
        'expert_acc': expert_acc,
        'gen_acc': gen_acc
    }
```

**í•´ì„**:
- `disc_acc ~ 0.5`: Generatorê°€ expertë¥¼ ì˜ ëª¨ë°© (good!)
- `disc_acc >> 0.5`: Generatorê°€ expertì™€ ë‹¤ë¦„ (need more training)

### 2. Imitation Quality

```python
def evaluate_imitation_quality(trainer, expert_trajectories):
    """
    Compare generated vs expert trajectories
    """
    gen_trajs = generate_from_policy(trainer.gen_algo, num=100)

    # Trajectory length
    expert_lengths = [len(t.acts) for t in expert_trajectories]
    gen_lengths = [len(t.acts) for t in gen_trajs]

    # Action distribution
    expert_actions = np.concatenate([t.acts for t in expert_trajectories])
    gen_actions = np.concatenate([t.acts for t in gen_trajs])

    # KL divergence
    expert_dist = np.bincount(expert_actions, minlength=36) / len(expert_actions)
    gen_dist = np.bincount(gen_actions, minlength=36) / len(gen_actions)

    kl_div = np.sum(expert_dist * np.log((expert_dist + 1e-10) / (gen_dist + 1e-10)))

    return {
        'length_diff': abs(np.mean(expert_lengths) - np.mean(gen_lengths)),
        'kl_divergence': kl_div
    }
```

### 3. Best h Selection

```python
def select_best_depth(all_results):
    """
    Select which h best explains expert behavior
    """
    metrics = {}

    for h, results in all_results.items():
        disc_metrics = evaluate_discrimination(results['trainer'], expert_trajs)
        imit_metrics = evaluate_imitation_quality(results['trainer'], expert_trajs)

        metrics[h] = {
            'disc_acc': disc_metrics['disc_acc'],
            'kl_div': imit_metrics['kl_divergence'],
            # Lower is better for both (disc_acc closer to 0.5, kl_div closer to 0)
            'score': abs(disc_metrics['disc_acc'] - 0.5) + imit_metrics['kl_divergence']
        }

    # Best h = lowest score
    best_h = min(metrics.keys(), key=lambda h: metrics[h]['score'])

    print(f"\nBest depth: h={best_h}")
    print(f"Metrics: {metrics[best_h]}")

    return best_h, metrics
```

---

# ìµœì¢… Validation ì²´í¬ë¦¬ìŠ¤íŠ¸

## ì „ì²´ Pipeline ê²€ì¦

- [ ] **Step A**: DepthLimitedPolicy(h) generates 89-dim observations
- [ ] **Step B**: BC trains on (obs, action) only, NO h
- [ ] **Step C**: PPO policy has 89-dim observation space
- [ ] **Step D**: Reward network has NO h parameter
- [ ] **Step E**: AIRL expert data has NO h labels
- [ ] **Step F**: Each h uses fresh reward network instance
- [ ] **Step G**: Terminology: "reward trained with h=X generator"

## ì½”ë“œ Audit

```python
# Search for forbidden patterns:
grep -r "depth.*reward" *.py  # Should find nothing
grep -r "reward.*depth" *.py  # Should find nothing
grep -r "h.*discriminator" *.py  # Should find nothing

# Allowed patterns:
grep -r "h.*generator" *.py  # OK
grep -r "h.*policy" *.py  # OK
```

---

# ìš”ì•½: ì–´ë””ì— hê°€ ìˆê³  ì—†ëŠ”ê°€

## hê°€ ìˆëŠ” ê³³ âœ…

1. `DepthLimitedPolicy(h=h)` - Policy definition
2. `generate_depth_limited_trajectories(h=h)` - Data generation
3. File names: `generator_h4.pt`, `reward_trained_with_h4_generator.pt`
4. Metadata/logging: `{'h': h, ...}` (trainingì— ì‚¬ìš© ì•ˆ ë¨)

## hê°€ ì ˆëŒ€ ì—†ì–´ì•¼ í•˜ëŠ” ê³³ âŒ

1. `BasicRewardNet(...)` - NO h parameter
2. `reward_net.forward(state, action, next_state)` - NO h input
3. Observations: `(batch, 89)` - NO h in state
4. Expert trajectories: `Trajectory(obs, acts)` - NO h label
5. BC training: `bc.BC(demonstrations=...)` - NO h in data

---

**ë¬¸ì„œ**: DEPTH_INTEGRATION_DETAILED.md
**ìƒíƒœ**: ì™„ì „ êµ¬ì²´í™” ì™„ë£Œ
**ë‹¤ìŒ**: Step-by-step êµ¬í˜„ ì‹œì‘
**í™˜ê²½**: pedestrian_analysis (Python 3.9.7, imitation installed)
