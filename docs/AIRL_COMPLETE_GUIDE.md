# AIRL ì™„ì „ ê°€ì´ë“œ: ì „ì²´ êµ¬ì¡°ì™€ ì‹¤í–‰ ë°©ë²•

**Last Updated**: 2025-12-26
**Main Approach**: **Option A (Pure NN)** â­
**Status**: Option B baseline ì™„ë£Œ (71%), Option A ì§„í–‰ ì˜ˆì •

---

## âš ï¸ ì¤‘ìš”: ì—°êµ¬ ë°©í–¥

**ì´ í”„ë¡œì íŠ¸ëŠ” Option A (Pure Neural Network)ë¥¼ main approachë¡œ ì§„í–‰í•©ë‹ˆë‹¤.**

### ì™œ Option Aë¥¼ ì„ íƒí•˜ëŠ”ê°€?

| ì´ìœ  | ì„¤ëª… |
|------|------|
| **ìˆœìˆ˜í•œ IRL ê²€ì¦** | Domain knowledge ì—†ì´ ìˆœìˆ˜ í•™ìŠµìœ¼ë¡œ planning depth íš¨ê³¼ ì¸¡ì • |
| **ì´ë¡ ì  ì •í•©ì„±** | Planning depthê°€ í–‰ë™ì— ë¯¸ì¹˜ëŠ” ìˆœìˆ˜í•œ ì˜í–¥ ë¶„ë¦¬ |
| **Pedestrian í”„ë¡œì íŠ¸ ì¼ê´€ì„±** | ê¸°ì¡´ ì—°êµ¬ì™€ ë™ì¼í•œ ì ‘ê·¼ë²• |
| **ì—°êµ¬ ì§ˆë¬¸ì— ì§ì ‘ ë‹µë³€** | "Planning depthë§Œìœ¼ë¡œ í–‰ë™ì´ ë‹¬ë¼ì§€ëŠ”ê°€?" |

### ë‘ ê°€ì§€ ì˜µì…˜

- **Option A (Pure NN)**: Random ì´ˆê¸°í™” â†’ ìˆœìˆ˜ AIRL í•™ìŠµ - **Main Approach** â­
- **Option B (BC Distillation)**: BFS â†’ BC â†’ AIRL fine-tuning - **Baseline/ë¹„êµêµ°**

### í˜„ì¬ ìƒíƒœ

- âœ… **Option B (Baseline)**: Steps A-E ì™„ë£Œ (71%) - ë¹„êµ ê¸°ì¤€ í™•ë³´
- ğŸ”„ **Option A (Main)**: ì½”ë“œ ì¤€ë¹„ ì™„ë£Œ, ì‹¤í—˜ ì§„í–‰ ì˜ˆì •

---

## ğŸ“‹ ëª©ì°¨
1. [AIRLì´ë€ ë¬´ì—‡ì¸ê°€?](#1-airlì´ë€-ë¬´ì—‡ì¸ê°€)
2. [ì „ì²´ íŒŒì´í”„ë¼ì¸ êµ¬ì¡°](#2-ì „ì²´-íŒŒì´í”„ë¼ì¸-êµ¬ì¡°)
3. [ê° ë‹¨ê³„ ìƒì„¸ ì„¤ëª…](#3-ê°-ë‹¨ê³„-ìƒì„¸-ì„¤ëª…)
4. [ì˜µì…˜ë³„ ì°¨ì´ì ](#4-ì˜µì…˜ë³„-ì°¨ì´ì )
5. [í˜„ì¬ êµ¬í˜„ í˜„í™©](#5-í˜„ì¬-êµ¬í˜„-í˜„í™©)
6. [ì‹¤í–‰ ë°©ë²•](#6-ì‹¤í–‰-ë°©ë²•)

---

## 1. AIRLì´ë€ ë¬´ì—‡ì¸ê°€?

### í•µì‹¬ ì•„ì´ë””ì–´

**AIRL (Adversarial Inverse Reinforcement Learning)**ëŠ”:
- **ì…ë ¥**: Expertì˜ í–‰ë™ ë°ì´í„° (trajectories)
- **ì¶œë ¥**: Expertë¥¼ ë§Œë“¤ì–´ë‚¸ "Reward function"
- **ë°©ë²•**: GANì²˜ëŸ¼ Discriminator(reward)ì™€ Generator(policy)ë¥¼ ë²ˆê°ˆì•„ í•™ìŠµ

### ì™œ í•„ìš”í•œê°€?

```
ê¸°ì¡´ ë°©ì‹:
  ì‚¬ëŒì´ reward ì„¤ê³„ â†’ RLë¡œ í•™ìŠµ â†’ ì •ì±…
  âš ï¸ ë¬¸ì œ: reward ì„¤ê³„ê°€ ì–´ë µê³  ì‹¤ì œ ì˜ë„ì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ

AIRL ë°©ì‹:
  Expert í–‰ë™ ê´€ì°° â†’ AIRL â†’ Reward ë³µì› â†’ ì •ì±…
  âœ… ì¥ì : Expertê°€ ì‹¤ì œë¡œ ìµœì í™”í•œ rewardë¥¼ ì—­ì¶”ë¡ 
```

### ìš°ë¦¬ í”„ë¡œì íŠ¸ì˜ ëª©í‘œ

**"Planning depth hê°€ í–‰ë™ì— ì–´ë–»ê²Œ ì˜í–¥ì„ ì£¼ëŠ”ê°€?"**
- ExpertëŠ” depth h=4ë¡œ ê³„íší•¨
- ë‹¤ë¥¸ h (1, 2, 8)ë¡œ í•™ìŠµí•˜ë©´ ì–´ë–»ê²Œ ë˜ëŠ”ê°€?
- RewardëŠ” depthì™€ ë…ë¦½ì ì´ì–´ì•¼ í•¨!

---

## 2. ì „ì²´ íŒŒì´í”„ë¼ì¸ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AIRL PIPELINE                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  [Step 0] Expert Data                                           â”‚
â”‚      â†“                                                           â”‚
â”‚      â””â”€â†’ Expert trajectories (state, action, next_state)        â”‚
â”‚          (NO depth information!)                                 â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  [Step 1] Environment Setup                             â”‚  â”‚
â”‚  â”‚  - FourInARowEnv (6x6 board)                            â”‚  â”‚
â”‚  â”‚  - Observation: 89-dim (board + features, NO h)         â”‚  â”‚
â”‚  â”‚  - Action: 36 positions                                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  [Step 2] Generator (Policy) - ì˜µì…˜ ì„ íƒ!               â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  â­ Option A: Pure NN (Main Approach)                   â”‚  â”‚
â”‚  â”‚  â”œâ”€ Random initialization                                â”‚  â”‚
â”‚  â”‚  â””â”€ ìˆœìˆ˜ AIRL í•™ìŠµ (50K-100K steps)                    â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  Option B: BFS Distillation (Baseline)                   â”‚  â”‚
â”‚  â”‚  â”œâ”€ Step 2a: BFS(h) ë°ì´í„° ìƒì„±                         â”‚  â”‚
â”‚  â”‚  â”œâ”€ Step 2b: BCë¡œ BFS ëª¨ë°©                              â”‚  â”‚
â”‚  â”‚  â”œâ”€ Step 2c: PPOë¡œ ë˜í•‘                                  â”‚  â”‚
â”‚  â”‚  â””â”€ AIRL fine-tuning (10K steps)                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  [Step 3] Discriminator (Reward Network)                â”‚  â”‚
â”‚  â”‚  - BasicRewardNet                                        â”‚  â”‚
â”‚  â”‚  - Input: (state, action, next_state)                    â”‚  â”‚
â”‚  â”‚  - Output: reward (scalar)                               â”‚  â”‚
â”‚  â”‚  - âœ… NO depth parameter!                                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  [Step 4] AIRL Training Loop                            â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  for iteration in range(num_iterations):                â”‚  â”‚
â”‚  â”‚    1. Generator rollout                                  â”‚  â”‚
â”‚  â”‚       â””â”€ Generate trajectories using current policy      â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚    2. Discriminator update                               â”‚  â”‚
â”‚  â”‚       â”œâ”€ Sample expert trajectories                      â”‚  â”‚
â”‚  â”‚       â”œâ”€ Sample generated trajectories                   â”‚  â”‚
â”‚  â”‚       â””â”€ Train to distinguish expert vs generated        â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚    3. Generator update                                   â”‚  â”‚
â”‚  â”‚       â””â”€ Improve policy using discriminator feedback     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                  â”‚
â”‚  [Step 5] Evaluation                                            â”‚
â”‚      â”œâ”€ Compare with expert behavior                            â”‚
â”‚      â”œâ”€ Measure KL divergence                                   â”‚
â”‚      â””â”€ Evaluate win rate                                       â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. ê° ë‹¨ê³„ ìƒì„¸ ì„¤ëª…

### Step 0: Expert Data ì¤€ë¹„

**ëª©ì **: AIRLì´ ëª¨ë°©í•  expert í–‰ë™ ë°ì´í„° ì¤€ë¹„

**ì˜µì…˜ 1: ì‹¤ì œ human data ì‚¬ìš©**
```python
# opendata/raw_data.csv ë¡œë“œ
expert_trajectories = load_expert_trajectories(
    csv_path='opendata/raw_data.csv',
    player_filter=0,  # Black player
    max_trajectories=100
)
```

**ì˜µì…˜ 2: Synthetic data (BFS ìƒì„±)**
```python
# BFS(h=4)ë¡œ ë°ì´í„° ìƒì„± (expert ëŒ€ìš©)
from generate_training_data import generate_depth_limited_trajectories

expert_trajs = generate_depth_limited_trajectories(
    h=4,  # "Expert"ëŠ” h=4ë¡œ ê³„íší•œë‹¤ê³  ê°€ì •
    num_episodes=100
)
```

**ë°ì´í„° í˜•ì‹**:
```python
GameTrajectory:
  - observations: (T+1, 89)  # ìƒíƒœ ì‹œí€€ìŠ¤ (ìµœì¢… ìƒíƒœ í¬í•¨)
  - actions: (T,)             # í–‰ë™ ì‹œí€€ìŠ¤
  - rewards: (T,)             # ë³´ìƒ ì‹œí€€ìŠ¤ (terminalë§Œ Â±1)
  - âœ… NO depth label!        # depth ì •ë³´ ì—†ìŒ!
```

**ì¤‘ìš”**: Expert ë°ì´í„°ì—ëŠ” **depth ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤**! ë‹¨ì§€ (state, action) ìŒë§Œ ìˆìŠµë‹ˆë‹¤.

---

### Step 1: Environment Setup

**ëª©ì **: 4-in-a-row í™˜ê²½ êµ¬ì¶•

**íŒŒì¼**: `fourinarow_airl/env.py`

**ì£¼ìš” íŠ¹ì§•**:
```python
env = FourInARowEnv()

# Observation space
env.observation_space
# Box(89,)
# - 0-35: Black pieces (6x6 board)
# - 36-71: White pieces (6x6 board)
# - 72-88: Van Opheusden features (17 features)
# âœ… NO depth encoding!

# Action space
env.action_space
# Discrete(36) - 6x6 board positions
```

**ì™œ 89-dim?**
- Board state (72) + Van Opheusden heuristic features (17)
- Depth hëŠ” **í¬í•¨ë˜ì§€ ì•ŠìŒ** (ê´€ì°° ë¶ˆê°€ëŠ¥)

---

### Step 2: Generator (Policy) ìƒì„±

**ëª©ì **: Expertë¥¼ ëª¨ë°©í•  ì •ì±… í•™ìŠµ

ì´ ë‹¨ê³„ì—ì„œ **Option A vs Option B ì„ íƒ!**

#### **Option A: Pure Neural Network**

**ì² í•™**: Domain knowledge ì—†ì´ ìˆœìˆ˜ í•™ìŠµ

**ë‹¨ê³„**:
```python
# 1. Random ì´ˆê¸°í™”
from create_ppo_generator_pure_nn import create_pure_ppo_generator

gen_algo, venv = create_pure_ppo_generator(
    env=env,
    h=4,  # Naming only (NOT in network!)
    learning_rate=3e-4
)

# 2. AIRLì—ì„œ ë°”ë¡œ ì‚¬ìš©
# (no pretraining)
```

**íŠ¹ì§•**:
- âœ… ìˆœìˆ˜ neural network
- âœ… Random weights
- âš ï¸ ëŠë¦° í•™ìŠµ (50K-100K steps)
- âš ï¸ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ

**íŒŒì¼**: `fourinarow_airl/create_ppo_generator_pure_nn.py`

---

#### **Option B: BFS Distillation**

**ì² í•™**: Van Opheusden BFSë¥¼ warm startë¡œ í™œìš©

**ë‹¨ê³„**:

**2a. BFS ë°ì´í„° ìƒì„±**
```python
from generate_training_data import generate_depth_limited_trajectories

# BFS(h=4)ë¡œ ê²Œì„ í”Œë ˆì´
training_trajs = generate_depth_limited_trajectories(
    h=4,
    num_episodes=100,
    seed=42
)
```

**íŒŒì¼**: `fourinarow_airl/generate_training_data.py`

**2b. Behavior Cloning (BC) í•™ìŠµ**
```python
from train_bc import train_bc_policy

# BFS í–‰ë™ì„ ì‹ ê²½ë§ì´ ëª¨ë°©
bc_trainer = train_bc_policy(
    trajectories=training_trajs,
    env=env,
    h=4,
    n_epochs=50
)
```

**ëª©ì **: BFSì˜ (state â†’ action) ë§¤í•‘ì„ ì‹ ê²½ë§ìœ¼ë¡œ í•™ìŠµ

**íŒŒì¼**: `fourinarow_airl/train_bc.py`

**2c. PPOë¡œ ë˜í•‘**
```python
from create_ppo_generator import create_ppo_from_bc

# BC policyë¥¼ PPOë¡œ ë˜í•‘ (AIRLì—ì„œ fine-tune ê°€ëŠ¥)
gen_algo, venv = create_ppo_from_bc(
    bc_trainer=bc_trainer,
    env=env,
    h=4
)
```

**íŒŒì¼**: `fourinarow_airl/create_ppo_generator.py`

**íŠ¹ì§•**:
- âœ… BFS ì§€ì‹ í™œìš©
- âœ… ë¹ ë¥¸ í•™ìŠµ (10K steps)
- âœ… ì•ˆì •ì 
- âš ï¸ BCì™€ IRL íš¨ê³¼ êµ¬ë¶„ ì–´ë ¤ì›€

---

### Step 3: Discriminator (Reward Network) ìƒì„±

**ëª©ì **: Expert vs Generatedë¥¼ êµ¬ë¶„í•˜ëŠ” reward function í•™ìŠµ

```python
from create_reward_net import create_reward_network

reward_net = create_reward_network(env)
# âœ… NO h parameter!
```

**ì•„í‚¤í…ì²˜**:
```python
BasicRewardNet:
  Input: (state, action, next_state, done)
    - state: (batch, 89)
    - action: (batch,) - discrete indices
    - next_state: (batch, 89)
    - done: (batch,) - boolean

  Internal:
    - Preprocessing (one-hot encoding for actions)
    - MLP [64, 64]
    - Tanh activation

  Output: (batch, 1) - reward
```

**ì¤‘ìš” ì›ì¹™**:
- âœ… **NO depth parameter** - rewardëŠ” ê´€ì°° ê°€ëŠ¥í•œ ì •ë³´ë§Œ ì‚¬ìš©
- âœ… ëª¨ë“  h ì‹¤í—˜ì—ì„œ **ë™ì¼í•œ ì•„í‚¤í…ì²˜** ì‚¬ìš©
- âœ… ê° hë§ˆë‹¤ **ë³„ë„ë¡œ í•™ìŠµ** (fresh instance)

**íŒŒì¼**: `fourinarow_airl/create_reward_net.py`

---

### Step 4: AIRL Training Loop

**ëª©ì **: Discriminatorì™€ Generatorë¥¼ ë²ˆê°ˆì•„ í•™ìŠµ

```python
from train_airl import train_airl_single_depth  # Option B
# OR
from train_airl_pure_nn import train_airl_pure_nn  # Option A

trainer = train_airl_single_depth(
    h=4,
    expert_trajectories=expert_trajectories,
    env=env,
    total_timesteps=10000  # Option B
    # total_timesteps=50000  # Option A
)
```

**ë‚´ë¶€ ë™ì‘** (imitation libraryê°€ ìë™ ì²˜ë¦¬):

```python
# Pseudocode
for iteration in range(total_timesteps // gen_train_timesteps):

    # 1. Generator rollout
    gen_trajectories = []
    for _ in range(gen_train_timesteps):
        trajectory = gen_algo.rollout(env)
        gen_trajectories.append(trajectory)

    # 2. Discriminator update (n_disc_updates_per_round ë²ˆ)
    for _ in range(n_disc_updates_per_round):
        # Sample expert batch
        expert_batch = sample(expert_trajectories, demo_batch_size)

        # Sample generated batch
        gen_batch = sample(gen_trajectories, demo_batch_size)

        # Binary classification loss
        # Expert = 1, Generated = 0
        loss = BCE(
            reward_net(expert_batch), ones
        ) + BCE(
            reward_net(gen_batch), zeros
        )

        # Update discriminator
        optimizer.step()

    # 3. Generator update (PPO)
    # Use discriminator output as reward
    gen_algo.learn(
        rollouts=gen_trajectories,
        reward_function=reward_net
    )
```

**í•˜ì´í¼íŒŒë¼ë¯¸í„°**:
- `demo_batch_size`: Discriminator í•™ìŠµ ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 64)
- `n_disc_updates_per_round`: Discriminator ì—…ë°ì´íŠ¸ íšŸìˆ˜ (ê¸°ë³¸: 4)
- `gen_train_timesteps`: Generator ë¡¤ì•„ì›ƒ ê¸¸ì´ (ê¸°ë³¸: 2048)

**íŒŒì¼**:
- `fourinarow_airl/train_airl.py` (Option B)
- `fourinarow_airl/train_airl_pure_nn.py` (Option A)

---

### Step 5: Evaluation

**ëª©ì **: í•™ìŠµëœ policyê°€ expertë¥¼ ì–¼ë§ˆë‚˜ ì˜ ëª¨ë°©í•˜ëŠ”ì§€ í‰ê°€

```python
# 1. Trajectory ìƒì„±
from compare_option_a_vs_b import generate_trajectories

test_trajs = generate_trajectories(
    gen_algo=trained_gen,
    env=env,
    num_episodes=50
)

# 2. Metrics ê³„ì‚°
# - Action distribution similarity (KL divergence)
kl_div = compute_kl_divergence(expert_dist, test_dist)

# - Win rate
win_rate = compute_win_rate(test_trajs)

# - Trajectory length distribution
lengths = [len(t.actions) for t in test_trajs]
```

**íŒŒì¼**: `compare_option_a_vs_b.py`

---

## 4. ì˜µì…˜ë³„ ì°¨ì´ì 

### ì „ì²´ ë¹„êµí‘œ

| ì»´í¬ë„ŒíŠ¸ | **Option A** | **Option B** | **ì°¨ì´ì ** |
|---------|-------------|-------------|-----------|
| **Expert Data** | Human or BFS(h=4) | Human or BFS(h=4) | âœ… ë™ì¼ |
| **Environment** | FourInARowEnv (89-dim) | FourInARowEnv (89-dim) | âœ… ë™ì¼ |
| **Generator ì´ˆê¸°í™”** | Random | BC(BFS) | âš ï¸ **í•µì‹¬ ì°¨ì´!** |
| **Generator í•™ìŠµ** | ìˆœìˆ˜ AIRL | BC â†’ AIRL fine-tune | âš ï¸ ë‹¤ë¦„ |
| **Timesteps** | 50K-100K | 10K | âš ï¸ ë‹¤ë¦„ |
| **Reward Network** | BasicRewardNet (NO h) | BasicRewardNet (NO h) | âœ… ë™ì¼ |
| **AIRL Algorithm** | imitation.airl.AIRL | imitation.airl.AIRL | âœ… ë™ì¼ |
| **Output** | Learned policy + reward | Learned policy + reward | âœ… ë™ì¼ |

### í•µì‹¬ í¬ì¸íŠ¸

1. **Reward networkëŠ” í•­ìƒ ë™ì¼**
   - ë‘˜ ë‹¤ depth-agnostic
   - ë‘˜ ë‹¤ (state, action, next_state)ë§Œ ì‚¬ìš©
   - ê° hë§ˆë‹¤ ë³„ë„ í•™ìŠµí•˜ì§€ë§Œ ì•„í‚¤í…ì²˜ëŠ” ê°™ìŒ

2. **ì°¨ì´ëŠ” Generator ì´ˆê¸°í™”ë¿**
   - Option A: ë°±ì§€ ìƒíƒœ
   - Option B: BFS ì§€ì‹ ì‚¬ì „ í•™ìŠµ

3. **hì˜ ì—­í• **
   - Option A: Naming only (íŒŒì¼ ì €ì¥ìš©)
   - Option B: BC ë°ì´í„° ìƒì„± ì‹œ BFS(h) ì‚¬ìš©, ì´í›„ naming
   - ë‘˜ ë‹¤: Networkì—ëŠ” h parameter ì—†ìŒ!

---

## 5. í˜„ì¬ êµ¬í˜„ í˜„í™©

### âœ… ì™„ë£Œëœ íŒŒì¼ë“¤

#### **í™˜ê²½ & ë°ì´í„°**
- âœ… `fourinarow_airl/env.py` - 4-in-a-row í™˜ê²½
- âœ… `fourinarow_airl/features.py` - Van Opheusden features
- âœ… `fourinarow_airl/data_loader.py` - Expert ë°ì´í„° ë¡œë”
- âœ… `fourinarow_airl/bfs_wrapper.py` - BFS C++ wrapper

#### **Option B êµ¬í˜„** (BFS Distillation)
- âœ… `fourinarow_airl/generate_training_data.py` - BFS ë°ì´í„° ìƒì„±
- âœ… `fourinarow_airl/depth_limited_policy.py` - BFS policy wrapper
- âœ… `fourinarow_airl/train_bc.py` - Behavior Cloning
- âœ… `fourinarow_airl/create_ppo_generator.py` - BC â†’ PPO
- âœ… `fourinarow_airl/create_reward_net.py` - Reward network
- âœ… `fourinarow_airl/airl_utils.py` - Utility functions
- âœ… `fourinarow_airl/train_airl.py` - AIRL í•™ìŠµ (Option B)

#### **Option A êµ¬í˜„** (Pure NN)
- âœ… `fourinarow_airl/create_ppo_generator_pure_nn.py` - Pure NN generator
- âœ… `fourinarow_airl/train_airl_pure_nn.py` - AIRL í•™ìŠµ (Option A)

#### **ë¹„êµ & ë¶„ì„**
- âœ… `compare_option_a_vs_b.py` - Option A vs B ë¹„êµ
- âœ… `visualize_option_difference.py` - ì‹œê°í™” ìƒì„±

#### **í…ŒìŠ¤íŠ¸ & ê²€ì¦**
- âœ… `verify_depth_utility.py` - Depth utility ê²€ì¦
- âœ… `test_phase1_integration.py` - Phase 1 í†µí•© í…ŒìŠ¤íŠ¸

#### **ë¬¸ì„œ**
- âœ… `AIRL_DESIGN.md` - ì´ˆê¸° ì„¤ê³„ ë¬¸ì„œ
- âœ… `OPTION_A_VS_B.md` - ì˜µì…˜ ë¹„êµ ìƒì„¸ ë¬¸ì„œ
- âœ… `OPTION_DIFFERENCE_SIMPLE.md` - ê°„ë‹¨ ì„¤ëª…
- âœ… `AIRL_COMPLETE_GUIDE.md` - ì´ ë¬¸ì„œ!

### ğŸ”§ êµ¬í˜„ ìƒíƒœ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### Phase 1: ê¸°ë³¸ Infrastructure âœ…
- [x] Environment êµ¬í˜„
- [x] Feature extraction
- [x] Data loading
- [x] BFS wrapper

#### Phase 2: Option B Pipeline (Baseline) âœ…
- [x] BFS data generation (Step A)
- [x] BC training (Step B)
- [x] PPO wrapping (Step C)
- [x] Reward network (Step D)
- [x] AIRL training (Step E)
- **Status**: 5/7 steps complete (71%)

#### Phase 3: Option A Pipeline (Main) âœ…
- [x] Pure NN generator ì½”ë“œ ì¤€ë¹„
- [x] AIRL training ì½”ë“œ ì¤€ë¹„
- **Status**: ì½”ë“œ ì™„ë£Œ, ì‹¤í—˜ ì§„í–‰ ì˜ˆì •

#### Phase 4: Evaluation & Comparison âœ…
- [x] Comparison script
- [x] Visualization
- [x] Documentation

#### Phase 5: Experiments (ğŸ”„ ì§„í–‰ ì˜ˆì •)

**Main Experiments (Option A - Priority 1)** â­:
- [ ] Option A í•™ìŠµ: h=1 (50K-100K steps)
- [ ] Option A í•™ìŠµ: h=2 (50K-100K steps)
- [ ] Option A í•™ìŠµ: h=4 (50K-100K steps)
- [ ] Option A í•™ìŠµ: h=8 (50K-100K steps)
- [ ] Option A í‰ê°€ ë° ë¶„ì„

**Baseline Experiments (Option B - Priority 2)**:
- [x] Option B í•™ìŠµ: Steps A-E ì™„ë£Œ
- [ ] Option B ì¶”ê°€ í•™ìŠµ (ë” ë§ì€ timesteps)
- [ ] Option B í‰ê°€ ë° ë¶„ì„

**Comparison Analysis**:
- [ ] Option A vs B ì„±ëŠ¥ ë¹„êµ
- [ ] Depth discrimination í…ŒìŠ¤íŠ¸
- [ ] Best h ì‹ë³„

---

## 6. ì‹¤í–‰ ë°©ë²•

### 6.1 í™˜ê²½ ì„¤ì •

```bash
# Conda environment í™œì„±í™”
conda activate pedestrian_analysis

# í•„ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸
pip list | grep -E "imitation|stable-baselines3|torch|gymnasium"
```

### 6.2 Option A ì‹¤í–‰ (Pure NN) â­ Main Approach

**ê¶Œì¥**: Option Aë¥¼ main approachë¡œ ì‚¬ìš©í•˜ì„¸ìš”.

#### ë‹¨ê³„ë³„ ì‹¤í–‰

```bash
# Step 1: í…ŒìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
python3 fourinarow_airl/create_ppo_generator_pure_nn.py --test

# Step 2: AIRL í•™ìŠµ
python3 fourinarow_airl/train_airl_pure_nn.py --h 4 --total_timesteps 50000

# ì¶œë ¥:
# - models/airl_pure_nn_results/airl_pure_generator_h4.zip
# - models/airl_pure_nn_results/airl_pure_reward_h4.pt
# - models/airl_pure_nn_results/airl_pure_metadata_h4.pkl
```

#### ëª¨ë“  depth í•™ìŠµ

```bash
for h in 1 2 4 8; do
    python3 fourinarow_airl/train_airl_pure_nn.py \
        --h $h \
        --total_timesteps 50000 \
        --output_dir models/airl_pure_nn_results
done
```

### 6.3 Option B ì‹¤í–‰ (BFS Distillation) - Baseline

**ìš©ë„**: ë¹ ë¥¸ baseline êµ¬ì¶• ë˜ëŠ” ë¹„êµêµ°

#### ë‹¨ê³„ë³„ ì‹¤í–‰

```bash
# Step 1: BFS ë°ì´í„° ìƒì„±
python3 fourinarow_airl/generate_training_data.py \
    --h 4 \
    --num_episodes 100 \
    --output training_data/depth_h4.pkl

# Step 2: BC í•™ìŠµ
python3 fourinarow_airl/train_bc.py \
    --h 4 \
    --training_data training_data/depth_h4.pkl \
    --n_epochs 50

# Step 3: PPO generator ìƒì„±
python3 fourinarow_airl/create_ppo_generator.py --h 4

# Step 4: AIRL í•™ìŠµ
python3 fourinarow_airl/train_airl.py \
    --h 4 \
    --total_timesteps 10000 \
    --output_dir models/airl_results

# ì¶œë ¥:
# - models/airl_results/airl_generator_h4.zip
# - models/airl_results/airl_reward_h4.pt
# - models/airl_results/airl_metadata_h4.pkl
```

#### ëª¨ë“  ë‹¨ê³„ ìë™í™”

```bash
# ëª¨ë“  depthì— ëŒ€í•´ Option B íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
for h in 1 2 4 8; do
    echo "Processing h=$h..."

    # BFS ë°ì´í„°
    python3 fourinarow_airl/generate_training_data.py \
        --h $h --num_episodes 100

    # BC í•™ìŠµ
    python3 fourinarow_airl/train_bc.py --h $h --n_epochs 50

    # PPO ìƒì„±
    python3 fourinarow_airl/create_ppo_generator.py --h $h

    # AIRL í•™ìŠµ
    python3 fourinarow_airl/train_airl.py \
        --h $h --total_timesteps 10000
done
```

### 6.4 ë¹„êµ ë° í‰ê°€

```bash
# Option A vs B ë¹„êµ (h=4)
python3 compare_option_a_vs_b.py --h 4 --num_episodes 50

# ì¶œë ¥:
# - figures/option_a_vs_b_h4.png
# - ì½˜ì†”ì— metrics ì¶œë ¥ (KL divergence, win rate, etc.)

# ì‹œê°í™” ìƒì„±
python3 visualize_option_difference.py

# ì¶œë ¥:
# - figures/option_a_vs_b_diagram.png
# - figures/option_a_vs_b_training_curves.png
# - figures/reward_network_same.png
```

### 6.5 Quick Start (í…ŒìŠ¤íŠ¸ìš©)

```bash
# Option A ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
python3 fourinarow_airl/train_airl_pure_nn.py --test

# Option B ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
python3 fourinarow_airl/train_airl.py --test

# ë‘ í…ŒìŠ¤íŠ¸ ëª¨ë‘ minimal timestepsë¡œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ê²€ì¦
```

---

## 7. í•µì‹¬ ê°œë… ì •ë¦¬

### Q1: depth hëŠ” ì–´ë””ì— ì‚¬ìš©ë˜ë‚˜?

**A: hëŠ” ì„¸ ê°€ì§€ ë‹¤ë¥¸ ì˜ë¯¸ë¡œ ì‚¬ìš©ë¨**

1. **Experiment design**: ê° h={1,2,4,8}ë§ˆë‹¤ ë³„ë„ AIRL í•™ìŠµ
2. **Option B - BFS ë°ì´í„° ìƒì„±**: BFS(h=4)ë¡œ trajectory ìƒì„±
3. **Naming/Metadata**: íŒŒì¼ ì €ì¥ ì‹œ `h4` íƒœê·¸

**hê°€ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ê³³**:
- âŒ Reward network architecture
- âŒ Observation space
- âŒ AIRL algorithm

### Q2: Reward networkëŠ” hë§ˆë‹¤ ë‹¤ë¥¸ê°€?

**A: ì•„í‚¤í…ì²˜ëŠ” ê°™ê³ , weightsëŠ” ë‹¤ë¦„**

```python
# ëª¨ë“  hì— ëŒ€í•´ ë™ì¼í•œ ì•„í‚¤í…ì²˜
for h in [1, 2, 4, 8]:
    reward_net = create_reward_network(env)  # Same architecture

    # í•˜ì§€ë§Œ ê°ê° ë³„ë„ë¡œ í•™ìŠµ (different weights)
    train_airl(h=h, reward_net=reward_net, ...)
```

### Q3: Option Aì™€ B ì¤‘ ì–´ëŠ ê²ƒì„ ì„ íƒ?

**A: ì´ í”„ë¡œì íŠ¸ëŠ” Option Aë¥¼ main approachë¡œ ì§„í–‰í•©ë‹ˆë‹¤** â­

| ìƒí™© | ì„ íƒ | ì´ìœ  |
|------|------|------|
| **Main Experiments** | **Option A** â­ | ìˆœìˆ˜í•œ IRL ëŠ¥ë ¥ ê²€ì¦, Planning depth ìˆœìˆ˜ íš¨ê³¼ |
| **Baseline êµ¬ì¶•** | **Option B** | ë¹ ë¥¸ ë¹„êµ ê¸°ì¤€, ì•ˆì •ì  ê²°ê³¼ |
| **ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘** | Option B | BFS ì§€ì‹ í™œìš©ìœ¼ë¡œ ë¹ ë¥¸ ìˆ˜ë ´ |
| **Pedestrian í”„ë¡œì íŠ¸ ì¼ê´€ì„±** | Option A | ê¸°ì¡´ ì—°êµ¬ì™€ ë™ì¼ ì ‘ê·¼ë²• |
| **ë…¼ë¬¸ ì‘ì„±** | ë‘˜ ë‹¤ | Option A (main) + Option B (baseline) ë¹„êµ |

**í˜„ì¬ ìƒíƒœ**:
- Option B: ì´ë¯¸ êµ¬í˜„ë¨ (Steps A-E ì™„ë£Œ, 71%) â†’ Baseline í™•ë³´ âœ…
- Option A: ì½”ë“œ ì¤€ë¹„ ì™„ë£Œ â†’ Main experiments ì§„í–‰ ì˜ˆì • ğŸ”„

### Q4: ì™œ Rewardì— hë¥¼ ë„£ì§€ ì•Šë‚˜?

**A: ì´ë¡ ì /ì‹¤í—˜ì  ì´ìœ **

1. **AIRL ì´ë¡ **: RewardëŠ” ê´€ì°° ê°€ëŠ¥í•œ ì •ë³´ë§Œ ì‚¬ìš©í•´ì•¼ í•¨
2. **Identifiability**: hë¥¼ ë„£ìœ¼ë©´ rewardì™€ planningì´ confoundë¨
3. **ì—°êµ¬ ì§ˆë¬¸**: "ê°™ì€ rewardì—ì„œ ë‹¤ë¥¸ hê°€ ë‹¤ë¥¸ í–‰ë™ì„ ë§Œë“œëŠ”ê°€?"
   - ì´ë¥¼ ë‹µí•˜ë ¤ë©´ rewardëŠ” h-agnosticí•´ì•¼ í•¨

---

## 8. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: "imitation library not found"

```bash
# í•´ê²°
conda activate pedestrian_analysis
pip install imitation stable-baselines3 torch
```

### ë¬¸ì œ 2: "PPO generator not found"

```bash
# Option Bì˜ ê²½ìš° ìˆœì„œëŒ€ë¡œ ì‹¤í–‰ í•„ìš”
python3 fourinarow_airl/generate_training_data.py --h 4
python3 fourinarow_airl/train_bc.py --h 4
python3 fourinarow_airl/create_ppo_generator.py --h 4
python3 fourinarow_airl/train_airl.py --h 4
```

### ë¬¸ì œ 3: "Expert trajectories too few"

```python
# generate_training_data.pyì—ì„œ num_episodes ì¦ê°€
python3 fourinarow_airl/generate_training_data.py \
    --h 4 \
    --num_episodes 200  # ê¸°ë³¸ 100 â†’ 200
```

### ë¬¸ì œ 4: "Training too slow"

```bash
# Option AëŠ” ì›ë˜ ëŠë¦¼ â†’ timesteps ì¤„ì´ê¸° (í…ŒìŠ¤íŠ¸ìš©)
python3 fourinarow_airl/train_airl_pure_nn.py \
    --h 4 \
    --total_timesteps 10000  # 50000 â†’ 10000

# ë˜ëŠ” Option B ì‚¬ìš©
```

---

## 9. ë‹¤ìŒ ë‹¨ê³„

### ì‹¤í—˜ ê³„íš

1. **Baseline êµ¬ì¶•**
   ```bash
   # Option Bë¡œ ëª¨ë“  h í•™ìŠµ (ë¹ ë¥´ê³  ì•ˆì •ì )
   for h in 1 2 4 8; do
       # Option B pipeline
   done
   ```

2. **Pure Learning ê²€ì¦**
   ```bash
   # Option Aë¡œ í•™ìŠµ (ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼)
   for h in 1 2 4 8; do
       python3 fourinarow_airl/train_airl_pure_nn.py \
           --h $h --total_timesteps 100000
   done
   ```

3. **ë¹„êµ ë¶„ì„**
   ```bash
   # ê° hì— ëŒ€í•´ Option A vs B ë¹„êµ
   for h in 1 2 4 8; do
       python3 compare_option_a_vs_b.py --h $h
   done
   ```

4. **Depth Discrimination**
   - í•™ìŠµëœ hë³„ policyê°€ ì‹¤ì œë¡œ ë‹¤ë¥¸ í–‰ë™ì„ í•˜ëŠ”ê°€?
   - Expert depthë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ”ê°€?

---

## ìš”ì•½

### ì „ì²´ íŒŒì´í”„ë¼ì¸ (í•œëˆˆì—)

```
Expert Data â†’ Environment Setup â†’ Generator Choice
                                      â†“
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚                   â”‚
                       Option A              Option B
                       (Pure NN)         (BFS Distillation)
                            â”‚                   â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â†“
                              Reward Network
                              (depth-agnostic)
                                      â†“
                              AIRL Training
                            (Discriminator â†” Generator)
                                      â†“
                         Learned Policy + Reward
                                      â†“
                                 Evaluation
```

### í•µì‹¬ ê¸°ì–µí•  ì 

1. **RewardëŠ” í•­ìƒ depth-agnostic** (h parameter ì—†ìŒ)
2. **ì°¨ì´ëŠ” Generator ì´ˆê¸°í™” ë°©ë²•** (Random vs BC)
3. **ê° hë§ˆë‹¤ ë³„ë„ í•™ìŠµ** (í•˜ì§€ë§Œ ê°™ì€ ì•„í‚¤í…ì²˜)
4. **Expert ë°ì´í„°ì— depth ì •ë³´ ì—†ìŒ** (ê´€ì°° ë¶ˆê°€ëŠ¥)
5. **ë‘˜ ë‹¤ êµ¬í˜„ë¨** (Option Aì™€ B ëª¨ë‘ ì¤€ë¹„ ì™„ë£Œ!)

---

**ì´ì œ ì‹¤í—˜ì„ ì‹œì‘í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!** ğŸš€
