# AIRL ì ìš© ì„¤ê³„: 4-in-a-row Planning-Aware IRL

**ìž‘ì„±ì¼:** 2024-12-23
**ëª©ì :** Pedestrian í”„ë¡œì íŠ¸ AIRL êµ¬ì¡°ë¥¼ ì°¸ê³ í•˜ì—¬ 4-in-a-rowì— ì ìš© ê°€ëŠ¥í•œ ì„¤ê³„ ë„ì¶œ

---

## 1. Pedestrian í”„ë¡œì íŠ¸ AIRL êµ¬ì¡° ë¶„ì„

### A. í•µì‹¬ ì»´í¬ë„ŒíŠ¸

```python
# 1. Environment (Gymnasium)
env = PedestrianEnv()  # Custom environment
env = InfiniteHorizonEnvWrapper(env)  # or FixedHorizonEnvWrapper

# 2. Expert Demonstrations
demonstrations = load_traj(subjId)  # List[TrajectoryWithRew]
# TrajectoryWithRew = (obs, acts, rews, infos, terminal)

# 3. Reward Network (Discriminator)
reward_net = BasicRewardNet(
    observation_space=env.observation_space,  # Box(113,)
    action_space=env.action_space,            # Discrete(9)
    hid_sizes=[32, 32],                       # [32, 32] or [256, 256, 256]
    activation=nn.Tanh                        # or nn.LeakyReLU
)
# Input: (state, action) â†’ Output: reward

# 4. Generator (Policy)
gen_algo = PPO(
    "MlpPolicy",
    env,
    n_steps=512,
    batch_size=32,
    ...
)
# Neural network policy

# 5. AIRL Trainer
trainer = airl.AIRL(
    demonstrations=demonstrations,
    demo_batch_size=256,
    venv=env,
    gen_algo=gen_algo,
    reward_net=reward_net,
    n_disc_updates_per_round=2,
    gen_train_timesteps=512,
    ...
)

# 6. Training
trainer.train(total_timesteps=4000 * 512)
```

### B. ë°ì´í„° í˜•ì‹

**Expert trajectories:**
```python
TrajectoryWithRew(
    obs=np.array([[obs_0], [obs_1], ..., [obs_T]]),     # shape: (T+1, obs_dim)
    acts=np.array([act_0, act_1, ..., act_{T-1}]),      # shape: (T,)
    rews=np.array([rew_0, rew_1, ..., rew_{T-1}]),      # shape: (T,)
    infos=[info_0, info_1, ..., info_{T-1}],            # shape: (T,)
    terminal=True
)
```

### C. í•µì‹¬ í†µì°°

1. **Imitation library ì‚¬ìš©**: `from imitation.algorithms.adversarial import airl`
2. **Discriminator = BasicRewardNet**: Neural network (state, action) â†’ reward
3. **Generator = PPO**: Neural network policy (í•™ìŠµ ê°€ëŠ¥)
4. **Environment í•„ìˆ˜**: Gymnasium interface í•„ìš”
5. **Variable horizon ì§€ì›**: `allow_variable_horizon=True`

---

## 2. 4-in-a-row AIRL ì ìš© ê°€ëŠ¥ì„± íŒë‹¨

### âœ… ì ìš© ê°€ëŠ¥! (ì¡°ê±´ë¶€)

**ê·¼ê±°:**
1. **State ì •ì˜ ê°€ëŠ¥**: Board state (6Ã—6 bitboard)
2. **Action ì •ì˜ ê°€ëŠ¥**: Move (0-35)
3. **Expert data ìžˆìŒ**: opendata/raw_data.csv (40ëª…, 67K trials)
4. **Trajectory êµ¬ì„± ê°€ëŠ¥**: ê²Œìž„ë³„ (state, action) ì‹œí€€ìŠ¤

### âš ï¸ ì£¼ìš” ë„ì „ ê³¼ì œ

#### **Challenge 1: BFS Policyì˜ ë¹„ë¯¸ë¶„ì„±**

**ë¬¸ì œ:**
```python
# Pedestrian: PPO (neural network) â†’ Gradient-based update
policy_net = MLP(obs) â†’ action_logits
# Back-propagation ê°€ëŠ¥

# 4-in-a-row: BFS (symbolic algorithm) â†’ No gradients
policy_bfs = BFS(board, h, beta, lapse) â†’ action
# Back-propagation ë¶ˆê°€ëŠ¥!
```

**í•´ê²° ë°©ì•ˆ:**
1. **Option A**: BFS â†’ Neural network distillation
2. **Option B**: Hybrid (neural + BFS)
3. **Option C**: Direct parameter optimization (CEM, BADS)

#### **Challenge 2: Van Opheusden Heuristic í™œìš©**

**ë¬¸ì œ:**
```python
# Van Opheusden: 17 hand-crafted features
heuristic_value = sum([w_i * feature_i for i in range(17)])

# AIRL: Learned reward (neural network)
reward = reward_net(state, action)

# ì¶©ëŒ: ì–´ë–»ê²Œ í†µí•©?
```

**í•´ê²° ë°©ì•ˆ:**
1. **Option A**: Heuristicìœ¼ë¡œ reward_net initialization
2. **Option B**: Heuristicë¥¼ auxiliary lossë¡œ
3. **Option C**: Pure AIRL (heuristic ë²„ë¦¼)

#### **Challenge 3: Environment êµ¬í˜„**

**ë¬¸ì œ:**
- 4-in-a-rowëŠ” 2-player game
- í˜„ìž¬ C++ êµ¬í˜„ë§Œ ìžˆìŒ
- Gymnasium interface í•„ìš”

**í•´ê²° ë°©ì•ˆ:**
- Python Gymnasium environment êµ¬í˜„ í•„ìš”

---

## 3. ì œì•ˆ ì„¤ê³„: Planning-Aware AIRL

### A. ì „ì²´ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Planning-Aware AIRL                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  [Expert Data]                                       â”‚
â”‚    â†“                                                 â”‚
â”‚  Board states + Actions (from opendata)              â”‚
â”‚    â†“                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚   Discriminator (Reward Network)      â”‚           â”‚
â”‚  â”‚   Input: (board_state, action, h)     â”‚           â”‚
â”‚  â”‚   Output: reward estimate             â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚    â†“                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚   Generator (h-constrained policy)    â”‚           â”‚
â”‚  â”‚   Option A: PPO (neural)              â”‚           â”‚
â”‚  â”‚   Option B: BFS (fixed-h) + distill   â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚    â†“                                                 â”‚
â”‚  Adversarial Training                                â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### B. State Representation

**Option 1: Board Encoding (ê°„ë‹¨)**
```python
# 72-dimensional vector
state = np.concatenate([
    black_pieces,  # 36-dim (6Ã—6 binary)
    white_pieces,  # 36-dim (6Ã—6 binary)
])
# Total: 72-dim
```

**Option 2: Feature-based (Van Opheusden)**
```python
# 17 + 72 = 89-dimensional
state = np.concatenate([
    board_encoding,       # 72-dim (raw board)
    heuristic_features,   # 17-dim (center, 2/3/4-in-a-row, ...)
])
# Total: 89-dim

# Features:
# - center_control: count pieces in center
# - connected_2_in_a_row: count connected pairs
# - unconnected_2_in_a_row: count unconnected pairs
# - 3_in_a_row: count triplets
# - 4_in_a_row: count winning states
# ... (17 features total)
```

**Option 3: CNN Embedding (í•™ìŠµ)**
```python
# Raw board as 2-channel image
state = np.stack([
    black_pieces.reshape(6, 6),  # Channel 0
    white_pieces.reshape(6, 6),  # Channel 1
], axis=0)
# Shape: (2, 6, 6)

# CNN encoder
cnn_encoder = nn.Sequential(
    nn.Conv2d(2, 32, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.Conv2d(32, 64, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.Flatten(),
    nn.Linear(64 * 6 * 6, 128)
)
state_embedding = cnn_encoder(state)  # 128-dim
```

**ì¶”ì²œ: Option 2 (Feature-based)**
- Van Opheusdenì˜ domain knowledge í™œìš©
- Reward network initialization ê°€ëŠ¥
- í•´ì„ ê°€ëŠ¥ì„± ìœ ì§€

### C. Action Space

```python
action_space = gym.spaces.Discrete(36)
# 0-35: board positions (6Ã—6)
```

**Legal action masking:**
```python
def get_legal_actions(board_state):
    """Return mask of legal actions"""
    black_pieces, white_pieces = board_state[:36], board_state[36:]
    occupied = (black_pieces | white_pieces)
    legal_mask = 1 - occupied  # 1=legal, 0=illegal
    return legal_mask  # shape: (36,)
```

### D. Environment êµ¬í˜„

```python
import gymnasium as gym
import numpy as np

class FourInARowEnv(gym.Env):
    """4-in-a-row Gymnasium Environment"""

    def __init__(self):
        super().__init__()

        # State: board encoding (72-dim) + features (17-dim)
        self.observation_space = gym.spaces.Box(
            low=0.0,
            high=1.0,
            shape=(89,),  # 72 + 17
            dtype=np.float32
        )

        # Action: move position (0-35)
        self.action_space = gym.spaces.Discrete(36)

        # Internal state
        self.black_pieces = np.zeros(36, dtype=np.float32)
        self.white_pieces = np.zeros(36, dtype=np.float32)
        self.current_player = 0  # 0=black, 1=white

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.black_pieces = np.zeros(36, dtype=np.float32)
        self.white_pieces = np.zeros(36, dtype=np.float32)
        self.current_player = 0
        obs = self._get_obs()
        info = {}
        return obs, info

    def step(self, action):
        # 1. Apply action
        if self.current_player == 0:  # Black
            self.black_pieces[action] = 1.0
        else:  # White
            self.white_pieces[action] = 1.0

        # 2. Check win/draw
        terminated = self._check_win() or self._check_draw()

        # 3. Compute reward (placeholder)
        reward = 1.0 if self._check_win() else 0.0

        # 4. Switch player
        self.current_player = 1 - self.current_player

        obs = self._get_obs()
        info = {}

        return obs, reward, terminated, False, info

    def _get_obs(self):
        """Construct observation: board + features"""
        # Board encoding
        board = np.concatenate([self.black_pieces, self.white_pieces])

        # Heuristic features (17-dim)
        features = self._extract_features()

        obs = np.concatenate([board, features])
        return obs.astype(np.float32)

    def _extract_features(self):
        """Extract 17 Van Opheusden features"""
        # TODO: Implement feature extraction
        # - center_control
        # - connected_2/3/4_in_a_row
        # - ...
        return np.zeros(17, dtype=np.float32)

    def _check_win(self):
        """Check 4-in-a-row for current player"""
        # TODO: Implement win check
        return False

    def _check_draw(self):
        """Check if board is full"""
        return np.sum(self.black_pieces + self.white_pieces) >= 36

    def get_legal_actions(self):
        """Return mask of legal actions"""
        occupied = self.black_pieces + self.white_pieces
        return 1 - occupied  # 1=legal, 0=illegal
```

### E. Expert Demonstrations ë³€í™˜

```python
def load_expert_trajectories(participant_id):
    """
    Load expert data from opendata/raw_data.csv
    Convert to imitation library format
    """
    import pandas as pd
    from imitation.data.types import TrajectoryWithRew

    # Load raw data
    raw_data = pd.read_csv('opendata/raw_data.csv')
    participant_data = raw_data[raw_data['participant'] == participant_id]

    # Group by game (need to reconstruct game sequences)
    # TODO: ê²Œìž„ ë‹¨ìœ„ë¡œ grouping í•„ìš” (í˜„ìž¬ raw_dataì—ëŠ” ê²Œìž„ ID ì—†ìŒ)

    trajectories = []

    # For each game:
    for game in games:
        observations = []
        actions = []
        rewards = []
        infos = []

        # Initial state
        black_pieces = np.zeros(36, dtype=np.float32)
        white_pieces = np.zeros(36, dtype=np.float32)
        current_player = 0

        for trial in game:
            # Current observation
            obs = construct_observation(black_pieces, white_pieces)
            observations.append(obs)

            # Action
            action = trial['move']
            actions.append(action)

            # Update board
            if current_player == 0:
                black_pieces[action] = 1.0
            else:
                white_pieces[action] = 1.0

            # Reward (placeholder)
            reward = 0.0
            rewards.append(reward)

            # Info
            info = {'response_time': trial['response_time']}
            infos.append(info)

            # Switch player
            current_player = 1 - current_player

        # Final observation
        final_obs = construct_observation(black_pieces, white_pieces)
        observations.append(final_obs)

        # Create trajectory
        traj = TrajectoryWithRew(
            obs=np.array(observations),
            acts=np.array(actions),
            rews=np.array(rewards),
            infos=infos,
            terminal=True
        )
        trajectories.append(traj)

    return trajectories

def construct_observation(black_pieces, white_pieces):
    """Construct observation from board state"""
    board = np.concatenate([black_pieces, white_pieces])
    features = extract_features(black_pieces, white_pieces)  # 17-dim
    obs = np.concatenate([board, features])
    return obs.astype(np.float32)
```

### F. Discriminator (Reward Network)

**Option 1: Pure Neural Network**
```python
reward_net = BasicRewardNet(
    observation_space=env.observation_space,  # Box(89,)
    action_space=env.action_space,            # Discrete(36)
    hid_sizes=[128, 128, 128],
    activation=nn.Tanh
)
# Input: (state, action) â†’ Output: reward
```

**Option 2: Van Opheusden Initialization**
```python
class HeuristicInitializedRewardNet(nn.Module):
    """
    Reward network initialized with Van Opheusden heuristic
    """
    def __init__(self, obs_dim, action_dim, hid_sizes, heuristic_weights):
        super().__init__()

        # Feature extractor (board â†’ 17 features)
        self.feature_extractor = nn.Linear(72, 17, bias=False)
        # Initialize with Van Opheusden feature weights
        with torch.no_grad():
            self.feature_extractor.weight.data = torch.tensor(
                heuristic_weights, dtype=torch.float32
            )

        # MLP (features + action â†’ reward)
        self.mlp = nn.Sequential(
            nn.Linear(17 + action_dim, hid_sizes[0]),
            nn.Tanh(),
            nn.Linear(hid_sizes[0], hid_sizes[1]),
            nn.Tanh(),
            nn.Linear(hid_sizes[1], 1)
        )

    def forward(self, state, action):
        # Extract board
        board = state[:, :72]  # (batch, 72)

        # Extract features (initialized with heuristic)
        features = self.feature_extractor(board)  # (batch, 17)

        # One-hot action
        action_onehot = F.one_hot(action, num_classes=36)  # (batch, 36)

        # Concatenate
        x = torch.cat([features, action_onehot], dim=-1)

        # MLP
        reward = self.mlp(x)
        return reward
```

**ì¶”ì²œ: Option 2 (Heuristic Initialization)**
- Van Opheusdenì˜ domain knowledge í™œìš©
- Warm start (í•™ìŠµ ì†ë„ í–¥ìƒ)
- Interpretability

### G. Generator (Policy)

**í•µì‹¬ ë¬¸ì œ: BFSëŠ” ë¯¸ë¶„ ë¶ˆê°€ëŠ¥!**

**Option A: Pure Neural Network (Pedestrian ë°©ì‹)**
```python
gen_algo = PPO(
    "MlpPolicy",
    env,
    learning_rate=1e-4,
    n_steps=512,
    batch_size=32,
    ...
)
# ì™„ì „ížˆ í•™ìŠµ ê°€ëŠ¥
# BFS ë²„ë¦¼ (Van Opheusden ê¸°ì—¬ ë¬´ì‹œ)
```

**Option B: BFS Distillation (ì¶”ì²œ) â­**
```python
class BFSDistilledPolicy(nn.Module):
    """
    Neural network trained to mimic BFS with fixed h
    """
    def __init__(self, obs_dim, action_dim, h, hid_sizes):
        super().__init__()
        self.h = h  # Fixed planning depth

        # Policy network
        self.policy_net = nn.Sequential(
            nn.Linear(obs_dim, hid_sizes[0]),
            nn.Tanh(),
            nn.Linear(hid_sizes[0], hid_sizes[1]),
            nn.Tanh(),
            nn.Linear(hid_sizes[1], action_dim)
        )

    def forward(self, obs):
        logits = self.policy_net(obs)
        return logits

    @staticmethod
    def pretrain_from_bfs(env, h, beta, lapse, heuristic_params):
        """
        Pre-train policy by imitating BFS outputs
        """
        # 1. Generate BFS rollouts
        bfs_policy = BFS_fixed_h(h=h, beta=beta, lapse=lapse,
                                  heuristic=heuristic_params)

        # 2. Collect (state, action) pairs
        dataset = []
        for _ in range(10000):  # 10K episodes
            obs = env.reset()
            done = False
            while not done:
                # BFS action
                action = bfs_policy.predict(obs)
                dataset.append((obs, action))

                obs, _, done, _, _ = env.step(action)

        # 3. Train neural network via behavior cloning
        policy = BFSDistilledPolicy(obs_dim, action_dim, h, hid_sizes)
        optimizer = torch.optim.Adam(policy.parameters(), lr=1e-3)

        for epoch in range(100):
            for obs, action in dataset:
                logits = policy(obs)
                loss = F.cross_entropy(logits, action)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

        return policy

# Usage in AIRL
policy = BFSDistilledPolicy.pretrain_from_bfs(env, h=6, ...)
gen_algo = CustomPPO(policy, env, ...)  # Fine-tune with PPO
```

**Option C: Hybrid (Neural + BFS) - ì‹¤í—˜ì **
```python
class HybridPolicy(nn.Module):
    """
    Combine neural network and BFS
    """
    def __init__(self, obs_dim, action_dim, h):
        super().__init__()
        self.h = h

        # Neural component
        self.neural_net = nn.Sequential(...)

        # BFS component (non-differentiable)
        self.bfs_policy = BFS_fixed_h(h=h)

        # Mixing parameter
        self.alpha = nn.Parameter(torch.tensor(0.5))

    def forward(self, obs):
        # Neural logits
        neural_logits = self.neural_net(obs)

        # BFS action (detached, no gradients)
        with torch.no_grad():
            bfs_action = self.bfs_policy.predict(obs)
            bfs_logits = torch.zeros_like(neural_logits)
            bfs_logits[bfs_action] = 10.0  # High logit for BFS action

        # Mix
        logits = self.alpha * neural_logits + (1 - self.alpha) * bfs_logits
        return logits
```

**ì¶”ì²œ: Option B (BFS Distillation)**
- Van Opheusden BFS í™œìš©
- Gradient-based í•™ìŠµ ê°€ëŠ¥
- Fixed h ëª…ì‹œì ìœ¼ë¡œ í‘œí˜„

---

## 4. Planning-Aware AIRL: hë¥¼ Latent Variableë¡œ

### A. í•µì‹¬ ì•„ì´ë””ì–´

**Standard AIRL:**
```python
# h ë¬´ì‹œ
reward_net = RewardNet(state, action) â†’ reward
```

**Planning-Aware AIRL:**
```python
# hë¥¼ conditionìœ¼ë¡œ
reward_net = RewardNet(state, action, h) â†’ reward

# hë³„ë¡œ ë³„ë„ í•™ìŠµ
for h in [2, 4, 6, 8, 10]:
    reward_net_h = RewardNet_h(state, action, h=h)
    policy_h = BFSDistilledPolicy(h=h)

    trainer = AIRL(demonstrations, policy_h, reward_net_h)
    trainer.train()
```

### B. Reward Network with h

```python
class PlanningAwareRewardNet(nn.Module):
    """
    Reward network conditioned on planning depth h
    """
    def __init__(self, obs_dim, action_dim, hid_sizes, num_h_values=5):
        super().__init__()

        # h embedding
        self.h_embedding = nn.Embedding(
            num_embeddings=num_h_values,  # h âˆˆ {2,4,6,8,10}
            embedding_dim=8
        )

        # MLP (state + action + h_embedding â†’ reward)
        input_dim = obs_dim + action_dim + 8
        self.mlp = nn.Sequential(
            nn.Linear(input_dim, hid_sizes[0]),
            nn.Tanh(),
            nn.Linear(hid_sizes[0], hid_sizes[1]),
            nn.Tanh(),
            nn.Linear(hid_sizes[1], 1)
        )

    def forward(self, state, action, h_idx):
        """
        Args:
            state: (batch, obs_dim)
            action: (batch,) or (batch, action_dim)
            h_idx: (batch,) - index of h value (0-4 for {2,4,6,8,10})
        """
        # Embed h
        h_emb = self.h_embedding(h_idx)  # (batch, 8)

        # One-hot action (if discrete)
        if action.dim() == 1:
            action_onehot = F.one_hot(action, num_classes=36)
        else:
            action_onehot = action

        # Concatenate
        x = torch.cat([state, action_onehot, h_emb], dim=-1)

        # Predict reward
        reward = self.mlp(x)
        return reward
```

### C. í•™ìŠµ ì ˆì°¨ (Actual Implementation)

**Implementation**: Steps A-E (see IMPLEMENTATION_NOTES.md for details)

```python
def train_planning_aware_airl():
    """
    Planning-Aware AIRL for 4-in-a-row (Implemented)
    
    Uses BC (Behavior Cloning) approach instead of BFS distillation.
    See fourinarow_airl/train_airl.py for complete implementation.
    """
    # 1. Load or generate expert data
    expert_trajectories = load_expert_trajectories(participant_id)
    # OR generate synthetic expert data
    expert_game_trajs = generate_depth_limited_trajectories(h=4, num_episodes=100)
    expert_trajectories = convert_to_imitation_format(expert_game_trajs)
    
    # 2. Setup environment
    env = FourInARowEnv()
    
    results = {}
    
    # 3. Train for each h
    for h in [1, 2, 4, 8]:
        print(f"\n=== Training with h={h} ===")
        
        # 3.1. Load BC-initialized PPO generator (Steps A-C)
        # Step A: generate_depth_limited_trajectories(h) 
        # Step B: train_bc_policy()
        # Step C: create_ppo_from_bc()
        gen_algo = load_ppo_generator(h=h)
        
        # 3.2. Create depth-AGNOSTIC reward network (Step D)
        # CRITICAL: NO h parameter!
        reward_net = create_reward_network(env)
        
        # 3.3. AIRL trainer (Step E)
        venv = DummyVecEnv([lambda: env])
        trainer = airl.AIRL(
            demonstrations=expert_trajectories,  # NO h labels!
            demo_batch_size=64,
            venv=venv,
            gen_algo=gen_algo,                   # h-specific (BC-initialized)
            reward_net=reward_net,               # depth-agnostic!
            n_disc_updates_per_round=4,
            gen_train_timesteps=512,
            allow_variable_horizon=True,         # 4-in-a-row games vary in length
        )
        
        # 3.4. Train
        trainer.train(total_timesteps=50000)
        
        # 3.5. Save results
        results[h] = {
            'reward_net': reward_net,
            'policy': gen_algo,
            'trainer': trainer
        }
    
    # 4. Model selection (best h)
    # Compare discriminator metrics across h values
    for h in [1, 2, 4, 8]:
        print(f"h={h}: disc_acc={results[h]['trainer'].disc_acc:.3f}")
    
    return results
```

**Key Differences from Initial Design**:

| Aspect | Initial Design | Actual Implementation |
|--------|----------------|----------------------|
| **Generator** | BFS Distillation | BC (Behavior Cloning) |
| **Reward Network** | h-conditioned | Completely depth-agnostic |
| **Depths** | [2, 4, 6, 8, 10] | [1, 2, 4, 8] |
| **Approach** | Direct BFS â†’ NN | Policy â†’ Trajectories â†’ BC â†’ PPO |

**Why BC was chosen**:
1. Simpler implementation using imitation library
2. No need for C++ BFS wrapper
3. Same goal achieved: neural policy mimics h-specific behavior
4. Faster to implement and test

### D. Discriminator í™œìš©

**Discriminatorì˜ ì—­í• :**
```python
# Discriminator = Reward Network
D(s, a, h) = sigmoid(f(s, a, h))

# f(s, a, h) = r(s, a, h) + Î³V(s') - V(s)
# where r(s, a, h) = learned reward conditioned on h
```

**Training:**
```python
# 1. Expert data: (s, a, s')_expert
# 2. Generated data: (s, a, s')_generated from policy_h

# Discriminator loss (binary classification)
loss_D = -E_expert[log D(s,a,h)] - E_gen[log(1 - D(s,a,h))]

# Policy loss (fool discriminator)
loss_G = -E_gen[log D(s,a,h)]
```

**Discriminator metrics (ì¤‘ìš”!):**
```python
# Training callbackì—ì„œ ëª¨ë‹ˆí„°ë§
metrics = {
    'disc_acc': overall accuracy,
    'disc_acc_expert': accuracy on expert data,
    'disc_acc_gen': accuracy on generated data
}

# Good training:
# - disc_acc_expert â‰ˆ disc_acc_gen â‰ˆ 0.5 (ê· í˜•)
# - ë„ˆë¬´ ë†’ìœ¼ë©´ (>0.9): discriminator ìŠ¹ë¦¬ â†’ policy í•™ìŠµ ì–´ë ¤ì›€
# - ë„ˆë¬´ ë‚®ìœ¼ë©´ (<0.2): policy ìŠ¹ë¦¬ â†’ reward ì˜ë¯¸ ì—†ìŒ
```

---

## 5. êµ¬í˜„ ë¡œë“œë§µ

### Phase 1: Environment êµ¬í˜„ âœ…
```python
# TODO:
# 1. FourInARowEnv í´ëž˜ìŠ¤ ìž‘ì„±
# 2. Board state â†’ observation ë³€í™˜
# 3. Van Opheusden 17 features êµ¬í˜„
# 4. Win/draw ì²´í¬
# 5. Legal action masking

# ì˜ˆìƒ ì†Œìš”: 1-2ì£¼
```

### Phase 2: Expert Data ë³€í™˜ âœ…
```python
# TODO:
# 1. raw_data.csv íŒŒì‹± (ê²Œìž„ ë‹¨ìœ„ reconstruction)
# 2. TrajectoryWithRew í˜•ì‹ìœ¼ë¡œ ë³€í™˜
# 3. Data validation (ì™„ì „í•œ ê²Œìž„ë§Œ)
# 4. Train/test split

# ì˜ˆìƒ ì†Œìš”: 1ì£¼
```

### Phase 3: BFS Distillation âœ…
```python
# TODO:
# 1. C++ BFS wrapper (Python interface)
# 2. BFS rollout ìƒì„± (ê° hë³„)
# 3. Behavior cloning (neural â†’ BFS)
# 4. Validation (neural policy â‰ˆ BFS?)

# ì˜ˆìƒ ì†Œìš”: 2-3ì£¼
```

### Phase 4: AIRL Training âœ…
```python
# TODO:
# 1. Reward network ì„¤ê³„ (with h conditioning)
# 2. AIRL trainer setup
# 3. Training loop (ê° hë³„)
# 4. Discriminator metrics ëª¨ë‹ˆí„°ë§
# 5. Model selection (best h)

# ì˜ˆìƒ ì†Œìš”: 2-3ì£¼
```

### Phase 5: Evaluation âœ…
```python
# TODO:
# 1. OOD generalization test
# 2. Reward visualization
# 3. Counterfactual analysis (h ë³€ê²½)
# 4. Expert vs Novice ë¹„êµ

# ì˜ˆìƒ ì†Œìš”: 2ì£¼
```

**ì´ ì˜ˆìƒ ì†Œìš”: 2-3ê°œì›”**

---

## 6. ì˜ˆìƒ ê²°ê³¼ ë° ê¸°ì—¬

### A. ê¸°ëŒ€ ê²°ê³¼

**1. hë³„ Reward ë¶„ë¦¬**
```python
reward_2 = reward_net(state, action, h=2)  # Shallow planning reward
reward_10 = reward_net(state, action, h=10) # Deep planning reward

# Analysis:
# "h=2ì—ì„œ center controlì´ ë” ì¤‘ìš”"
# "h=10ì—ì„œ 3-in-a-rowê°€ ë” ì¤‘ìš”"
```

**2. ìµœì  h ë¶„í¬**
```python
# Expert vs Novice
expert_best_h = [2, 4, 2, 4, 6, ...]  # ì£¼ë¡œ 2, 4
novice_best_h = [6, 8, 10, 6, 8, ...]  # ì£¼ë¡œ 6, 8, 10

# Chi-square test: p < 0.001
# ExpertëŠ” shallow planning ì„ í˜¸
```

**3. Discriminator Accuracy**
```python
# Well-trained:
# disc_acc_expert â‰ˆ 0.52
# disc_acc_gen â‰ˆ 0.48
# â†’ Generatorê°€ expertì²˜ëŸ¼ í–‰ë™

# h=2: disc_acc = 0.50 (best match for expert)
# h=10: disc_acc = 0.70 (discriminatorê°€ êµ¬ë¶„ ê°€ëŠ¥)
```

### B. ì—°êµ¬ ê¸°ì—¬

**1. Reward + Planning ë¶„ë¦¬**
- Van Opheusden: Heuristic (reward) + BFS (planning) í˜¼ìž¬
- ì œì•ˆ: h-conditioned rewardë¡œ ë¶„ë¦¬

**2. Yao (2024) ì£¼ìž¥ ê²€ì¦**
- "Planning horizonì€ latent confounder"
- hë¥¼ ëª…ì‹œí•˜ë©´ reward ì¶”ë¡  ì •í™•

**3. Counterfactual Analysis**
```python
# "ë§Œì•½ Expertê°€ h=10ìœ¼ë¡œ planí–ˆë‹¤ë©´?"
behavior_counterfactual = policy_h10.generate(expert_board)
# â†’ ì„±ëŠ¥ ì €í•˜ ì˜ˆìƒ
```

**4. OOD Generalization**
- ìƒˆë¡œìš´ board ìƒí™©ì— ì ìš©
- Transfer learning (ë‹¤ë¥¸ ê²Œìž„?)

---

## 7. ë¯¸í•´ê²° ì´ìŠˆ ë° ëŒ€ì•ˆ

### Issue 1: ê²Œìž„ ë‹¨ìœ„ Reconstruction

**ë¬¸ì œ:**
- raw_data.csvì— ê²Œìž„ ID ì—†ìŒ
- Trialë³„ board stateëŠ” ìžˆì§€ë§Œ ê²Œìž„ ê²½ê³„ ë¶ˆëª…

**í•´ê²°:**
```python
# Option A: Session ë‹¨ìœ„ë¡œ ê°€ì •
# - ê°™ì€ participant, ê°™ì€ session â†’ í•˜ë‚˜ì˜ ê²Œìž„?

# Option B: Board state ì¶”ì 
# - Empty board â†’ ìƒˆ ê²Œìž„ ì‹œìž‘
# - Win/draw ë°œìƒ â†’ ê²Œìž„ ì¢…ë£Œ

# Option C: data_hvh.txt í™œìš©
# - C++ data_struct.hì˜ board history
```

### Issue 2: Reward Ground Truth ë¶€ìž¬

**ë¬¸ì œ:**
- AIRLì€ rewardë¥¼ í•™ìŠµí•˜ì§€ë§Œ ground truth ì—†ìŒ
- Van Opheusden heuristicë„ learned (fitted)

**í•´ê²°:**
```python
# Validation:
# 1. Win/loss outcomeìœ¼ë¡œ ê²€ì¦
# 2. Human preferenceë¡œ ê²€ì¦
# 3. Cross-validation (held-out games)
```

### Issue 3: 2-Player Game ì²˜ë¦¬

**ë¬¸ì œ:**
- ExpertëŠ” Blackë§Œ í”Œë ˆì´ (WhiteëŠ” AI)
- AIRLì€ single-agent ê°€ì •

**í•´ê²°:**
```python
# Option A: Blackë§Œ ëª¨ë¸ë§
# - Expertì˜ Black movesë§Œ í•™ìŠµ
# - WhiteëŠ” í™˜ê²½ì˜ ì¼ë¶€ë¡œ

# Option B: Both players
# - Black + White ëª¨ë‘ í•™ìŠµ
# - Two-player AIRL (ë³µìž¡)
```

---

## 8. ìµœì¢… íŒë‹¨

### âœ… AIRL ì ìš© ê°€ëŠ¥!

**ì¡°ê±´:**
1. **Environment êµ¬í˜„**: Gymnasium interface (2ì£¼)
2. **BFS Distillation**: Neural policy pre-training (3ì£¼)
3. **Data Reconstruction**: ê²Œìž„ ë‹¨ìœ„ trajectory (1ì£¼)

### í•µì‹¬ ì„¤ê³„ ê²°ì •

**1. State: Feature-based (72 + 17 = 89-dim)**
- Board encoding + Van Opheusden features

**2. Generator: BFS Distillation**
- Pre-train neural policy to mimic BFS
- Fine-tune with PPO in AIRL

**3. Discriminator: h-conditioned Reward Network**
- Input: (state, action, h)
- Output: reward
- Initialize with Van Opheusden heuristic

**4. Training: hë³„ë¡œ ë³„ë„ í•™ìŠµ**
- h âˆˆ {2, 4, 6, 8, 10}
- Model selection (best h per participant)

### ë‹¤ìŒ ë‹¨ê³„

1. **Prototype**: FourInARowEnv êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸
2. **Data Pipeline**: raw_data.csv â†’ TrajectoryWithRew
3. **BFS Wrapper**: Python interface for C++ BFS
4. **Pilot Study**: 1ëª… ì°¸ê°€ìžë¡œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ê²€ì¦
5. **Full Training**: 40ëª… ì°¸ê°€ìž AIRL

---


## 9. Implementation Technical Notes

**See**: [IMPLEMENTATION_NOTES.md](./IMPLEMENTATION_NOTES.md) for detailed technical documentation including:

- Environment setup and library versions
- BasicRewardNet usage patterns
- imitation 1.0.1 API details
- Architecture matching (BC â†’ PPO)
- Data formats and dimensions
- AIRL training metrics interpretation
- Common issues and solutions
- Training recommendations

**Implementation Status** (2025-12-25):
- âœ… Steps A-E Complete (generate data â†’ BC â†’ PPO â†’ reward network â†’ AIRL training)
- ðŸ”„ Steps F-G Pending (multi-depth comparison, evaluation)
- âœ… Core principle maintained: h only in POLICY, never in reward network

---

**Last Updated**: 2025-12-25
**Status**: Design document (initial approach in Sections 1-8), actual implementation in Steps A-E
